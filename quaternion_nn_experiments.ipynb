{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quaternion_nn_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8BnKrCgT_BLJ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccardo-Vecchi/Pytorch-Quaternion-Neural-Networks/blob/master/quaternion_nn_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BnKrCgT_BLJ",
        "colab_type": "text"
      },
      "source": [
        "#Presentation \n",
        "\n",
        "**Goal:** Test several compression techniques for Quaternion Neural Networks (from new forms of regularization to Deep compression).\n",
        "\n",
        "**Datasets:**\n",
        " - MNIST    http://yann.lecun.com/exdb/mnist/\n",
        " - CIFAR10  https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "\n",
        " **Inspiring papers:**   \n",
        "  - Group Sparse Regularization for Deep Neural Networks \n",
        "     - https://arxiv.org/pdf/1607.00485.pdf\n",
        "  - Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding\n",
        "     - https://arxiv.org/pdf/1510.00149.pdf\n",
        "  - MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks\n",
        "     - https://arxiv.org/pdf/1711.06798.pdf\n",
        "\n",
        "**Author:** Riccardo Vecchi\n",
        "\n",
        "**Version:** 1.0\n",
        "\n",
        "---\n",
        "\n",
        "#Guide\n",
        "\n",
        "1.   Clone the repository in the Colab's runtime with the \"Clone repository\" cell\n",
        "2.   Use forms for setting parameters and hyper-parameters and run experiments\n",
        "3.   Good testing :-)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDIqYU5eu-na",
        "colab_type": "text"
      },
      "source": [
        "# Manage repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwdqbV6NxZmV",
        "colab_type": "text"
      },
      "source": [
        "## Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1798o2EQra4V",
        "colab_type": "code",
        "outputId": "d92fc65f-cb7d-4847-8137-2359dfd70c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "!git clone https://github.com/eretis/Pytorch-Quaternion-Neural-Networks.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Pytorch-Quaternion-Neural-Networks'...\n",
            "remote: Enumerating objects: 341, done.\u001b[K\n",
            "remote: Total 341 (delta 0), reused 0 (delta 0), pack-reused 341\u001b[K\n",
            "Receiving objects: 100% (341/341), 2.64 MiB | 15.63 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0ehzOLmzQGs",
        "colab_type": "text"
      },
      "source": [
        "## Refresh repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvEl0wovxeB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd /content/Pytorch-Quaternion-Neural-Networks && git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUlfNXd3StGv",
        "colab_type": "text"
      },
      "source": [
        "## Delete repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ZRWSG9SwgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf Pytorch-Quaternion-Neural-Networks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7UrFzeNs4Yq",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bT__elUyW2",
        "colab_type": "text"
      },
      "source": [
        "##Import all needed library and set the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7peI9aXpOxhz",
        "colab_type": "code",
        "outputId": "8bca35ce-598e-4ab7-9264-fdedf104a41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%cd /content/Pytorch-Quaternion-Neural-Networks \n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST, CIFAR10 \n",
        "\n",
        "from models.convolutional_models import *\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.interpolate import splrep, splev\n",
        "\n",
        "from skimage.measure import compare_ssim as ssim\n",
        "from utils.misc import *\n",
        "import imageio\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re, time, gzip\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "CIFAR10_num_to_classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "colors = ('green', 'blue', 'red', 'black', 'darkviolet', 'brown', 'darkorange', 'deepskyblue', 'darkcyan', 'pink')\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "\n",
        "def get_dataset():\n",
        "  \n",
        "    print('Retrieve ' + dataset.__name__ + ' dataset...\\n')\n",
        "\n",
        "    if dataset == CIFAR10:\n",
        "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                                    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    else:\n",
        "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), \n",
        "                                                    torchvision.transforms.Normalize((0.1307,), (0.3081,))])  # global mean and standard deviation for MNIST\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset('/files/', train=True, download=True, transform=transform), batch_size=batch_size_train, shuffle=True, num_workers=4)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset('/files/', train=False, download=True, transform=transform), batch_size=batch_size_test, shuffle=True, num_workers=4)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def count_trainable_parameters():\n",
        "    return sum(p.numel() for p in network.parameters() if p.requires_grad) \n",
        "  \n",
        "  \n",
        "def regularization(reg_type=None):\n",
        "\n",
        "    reg = 0\n",
        "    \n",
        "    if reg_type == 'L1':\n",
        "      for param in network.parameters():\n",
        "        reg = reg + torch.sum(torch.abs(param))    \n",
        "        \n",
        "    elif reg_type == 'L1_gamma':\n",
        "      for module in network.modules():\n",
        "        if isinstance(module, QuaternionBatchNorm2d):\n",
        "          reg = reg + torch.sum(torch.abs(module.gamma))\n",
        "      \n",
        "    elif reg_type == 'L1_without_gamma':\n",
        "      for module in network.modules():\n",
        "        if not isinstance(module, QuaternionBatchNorm2d):\n",
        "          for param in module.parameters(recurse=False):\n",
        "            reg = reg + torch.sum(torch.abs(param))\n",
        "                 \n",
        "    elif reg_type == 'L2':\n",
        "      for param in network.parameters():\n",
        "        reg = reg + torch.sum(param**2)\n",
        "\n",
        "    elif reg_type == 'RQ': # 'Group L1'\n",
        "        for module in network.modules():\n",
        "          if isinstance(module, (QuaternionConv, QuaternionLinear, QuaternionTransposeConv)):\n",
        "            reg = reg + torch.sum(torch.sqrt(module.r_weight**2 + module.i_weight**2 + module.j_weight**2 + module.k_weight**2))\n",
        "            \n",
        "    elif reg_type == 'RQL': # 'Sparse GL1'\n",
        "      reg = reg + regularization('RQ') + regularization('L1')\n",
        "        \n",
        "    elif reg_type == 'RQL_without_gamma':\n",
        "        reg = reg + regularization('RQ') + regularization('L1_without_gamma')\n",
        "        \n",
        "    return reg\n",
        "\n",
        "\n",
        "def compute_and_print_sparsity(decimals=3, experiment_name=None, writer=None):\n",
        "\n",
        "    network.eval()\n",
        "  \n",
        "    nonzero_weights, nonzero_quaternions, number_of_quaternions, gamma_ratio = 0, 0, 0, 1.\n",
        "    number_of_neurons = 0\n",
        "    nonzero_neurons = 0\n",
        "    \n",
        "    for module in network.modules():\n",
        "      \n",
        "        if isinstance(module, QuaternionBatchNorm2d):\n",
        "          zero_gamma = torch.le(torch.abs(module.gamma), 10**(-decimals))  # number of gamma that are 0\n",
        "          gamma_ratio *= (zero_gamma.numel() - torch.sum(zero_gamma).item())/zero_gamma.numel() if zero_gamma is not None else 1.\n",
        "          zero_gamma = None\n",
        "          \n",
        "        for param in module.parameters(recurse=False):       \n",
        "          nonzero_weights += torch.sum(torch.ge(torch.abs(param.detach()), 10**(-decimals))).item() * gamma_ratio\n",
        "                       \n",
        "        if isinstance(module, (QuaternionConv, QuaternionLinear, QuaternionTransposeConv)):\n",
        "          number_of_quaternions += module.r_weight.numel()\n",
        "          number_of_neurons += module.r_weight.shape[1]\n",
        "          quat_norm = torch.sqrt(module.r_weight**2 + module.i_weight**2 + module.j_weight**2 + module.k_weight**2)          \n",
        "          nonzero_quaternions += torch.sum(torch.ge(torch.abs(quat_norm), 10**(-decimals))).item() * gamma_ratio\n",
        "\n",
        "          #aaa = torch.abs(module.r_weight + module.i_weight + module.j_weight + module.k_weight)\n",
        "          #bbb = torch.le(torch.sum(aaa), 10**(-decimals))\n",
        "          #print(bbb)\n",
        "          #nonzero_neurons += torch.sum(bbb) * gamma_ratio\n",
        "          \n",
        "    weight_sparsity = (1 - nonzero_weights / count_trainable_parameters()) * 100\n",
        "    quaternion_sparsity = (1 - nonzero_quaternions / number_of_quaternions) * 100 if use_quaternion_variant else 0\n",
        "    #neuron_sparsity = (1 - nonzero_neurons / number_of_neurons) * 100 \n",
        "    \n",
        "    if experiment_name is not None:\n",
        "      experiments_results[experiment_name]['weight'].append(weight_sparsity)\n",
        "      experiments_results[experiment_name]['quaternion'].append(quaternion_sparsity)\n",
        "      #experiments_results[experiment_name]['neuron'].append(neuron_sparsity)\n",
        "\n",
        "    print('Checking sparsity...\\nWeight sparsity: {:.2f}%'.format(weight_sparsity))\n",
        "    print('Quaternion sparsity: {:.2f}%\\n'.format(quaternion_sparsity)) if use_quaternion_variant else print()\n",
        "    #print('Neuron sparsity: {:.2f}%\\n'.format(neuron_sparsity))\n",
        "    \n",
        "    if use_tensorboard and writer is not None:\n",
        "      writer.add_scalars('Sparsity', { 'Weight sparsity [%]' : weight_sparsity,\n",
        "                                       'Quaternion sparsity [%]' : quaternion_sparsity }, globaliter)\n",
        "    \n",
        "\n",
        "def expand_input(data, expansion='rgb_vector'):  # [BATCH X CHANNELS X WIDTH X HEIGHT]\n",
        "\n",
        "    if expansion == 'repeat':  # Copy the original input also for vector components (i, j, k)\n",
        "        new_input = np.repeat(data, 4, axis=1)\n",
        "\n",
        "    elif expansion == 'zero_vector':  # Zero-fill for vector components (i, j, k)\n",
        "        new_input = torch.zeros(data.shape[0], 4, data.shape[2], data.shape[3], dtype=torch.float, device=device)\n",
        "        new_input[:, :1, :, :] = data\n",
        "\n",
        "    elif expansion == 'rgb_vector':  # Real part to 0 and vector part (i, j, k) <- RGB\n",
        "        new_input = torch.zeros((data.shape[0], 4, data.shape[2], data.shape[3]), dtype=torch.float, device=device)\n",
        "        new_input[:, 1:, :, :] = data\n",
        "\n",
        "    return new_input\n",
        "  \n",
        "  \n",
        "#def adjust_learning_rate(lr, optimizer, epoch):\n",
        "#    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "#    learning_rt = lr * (0.1 ** (epoch // 30))\n",
        "#    for param_group in optimizer.param_groups:\n",
        "#        param_group['lr'] = learning_rt\n",
        "        \n",
        "def adjust_learning_rate(lr, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    return lr * (0.1 ** (epoch // 30))\n",
        "    #for param_group in optimizer.param_groups:\n",
        "    #    param_group['lr'] = learning_rt\n",
        "  \n",
        "\n",
        "def prune(decimals=3):\n",
        "\n",
        "  print('Pruning the network...\\n')\n",
        "  \n",
        "  for param in network.parameters(): # with network.parameters(recurse=False) non mi entra nel ciclo :O\n",
        "\n",
        "    weight_mask = torch.ge(torch.abs(param), 10**(-decimals)).type_as(param)\n",
        "    param.detach().mul_(weight_mask)\n",
        "    \n",
        "    codebook['masks'].append(weight_mask)\n",
        "    # param.data = param.to_sparse().coalesce() # feature requested in https://github.com/pytorch/pytorch/issues/16667\n",
        "    \n",
        "  # compute_and_print_sparsity() sono riuscito a rendere weight sparsity = anche dopo il pruning..c'è ancora qualche differenza con la quaternion, lascialo per testare\n",
        "  \n",
        "\n",
        "def apply_weight_sharing(bits=2):  # 2^bits possible weights per layer\n",
        "  \n",
        "  print('Applying weight sharing [{} bits]...'.format(bits))\n",
        "\n",
        "  for param in network.parameters():\n",
        "\n",
        "    weights = param.to('cpu').detach().numpy()\n",
        "    nonzero_indices = np.nonzero(weights)\n",
        "    k_means_input = weights[nonzero_indices]\n",
        "    \n",
        "    space = np.linspace(np.min(k_means_input), np.max(k_means_input), num=2**bits)\n",
        "    k_means = KMeans(n_clusters=len(space), init=space.reshape(-1, 1), n_init=1)\n",
        "    k_means.fit(k_means_input.reshape(-1, 1))\n",
        "    weights[nonzero_indices] = k_means.cluster_centers_[k_means.labels_].reshape(-1)\n",
        "\n",
        "    param.data = torch.from_numpy(weights).to(device)\n",
        "    \n",
        "    codebook['centroids'].append(torch.from_numpy(k_means.cluster_centers_).reshape(-1))\n",
        "    codebook['labels'].append(torch.from_numpy(k_means.labels_).type(dtype=torch.LongTensor))\n",
        "\n",
        "    \n",
        "def apply_quaternion_weight_sharing(bits=3):  # 2^bits possible weights per quaternion layer\n",
        "  \n",
        "  print('Applying quaternion weight sharing [{} bits]...'.format(bits))\n",
        "\n",
        "  for module in network.modules():\n",
        "    \n",
        "    if isinstance(module, (QuaternionConv, QuaternionLinear, QuaternionTransposeConv)):\n",
        "      \n",
        "      weights = np.array([], dtype=np.float32)   \n",
        "      for param in module.parameters():\n",
        "        if param.dim() == 1: # avoid biases if exist (one-dimensional arrays)\n",
        "          continue\n",
        "        weights = np.append(weights, param.reshape(-1).detach().to('cpu').numpy())\n",
        "      \n",
        "      nonzero_indices = np.nonzero(weights)\n",
        "      k_means_input = weights[nonzero_indices]\n",
        "    \n",
        "      space = np.linspace(np.min(k_means_input), np.max(k_means_input), num=2**bits)\n",
        "      k_means = KMeans(n_clusters=len(space), init=space.reshape(-1, 1), n_init=1)\n",
        "      k_means.fit(k_means_input.reshape(-1, 1))\n",
        "      weights[nonzero_indices] = k_means.cluster_centers_[k_means.labels_].reshape(-1)\n",
        "    \n",
        "      new_weights = np.split(weights, 4)\n",
        "      \n",
        "      for index, param in enumerate(module.parameters()):\n",
        "        if param.dim() == 1: # avoid biases if exist (one-dimensional arrays)\n",
        "          continue\n",
        "        param.data = torch.from_numpy(new_weights[index]).reshape_as(param.data).to(device)\n",
        "    \n",
        "      codebook['centroids'].append(torch.from_numpy(k_means.cluster_centers_).reshape(-1))\n",
        "      codebook['labels'].append(torch.from_numpy(k_means.labels_).type(dtype=torch.LongTensor))    \n",
        "    \n",
        "    \n",
        "def fine_tuning_centroids(n_iter=30, learning_rate_tuning=0.0001):\n",
        "  \n",
        "  print('Fine tuning centroids...\\n')\n",
        "  \n",
        "  # compute_and_print_sparsity() poi sale la sparsity....\n",
        "  \n",
        "  for iteration in range(n_iter):\n",
        "    \n",
        "    print('iteration [{}/{}]'.format(iteration+1, n_iter))\n",
        "    \n",
        "    for batch_index, (data, target) in enumerate(train_set):\n",
        "      \n",
        "      if use_quaternion_variant:\n",
        "        data = expand_input(data, input_expansion)\n",
        "        \n",
        "      data, target = data.to(device), target.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      output = network(data)  # Forward pass\n",
        "      loss = loss_criterion(output, target)\n",
        "      loss.backward()  # Backward pass\n",
        "      \n",
        "    for index, param in enumerate(network.parameters()):\n",
        "\n",
        "      gradient = param.grad.reshape(-1)\n",
        "\n",
        "      for centroid in range(codebook['centroids'][index].numel()):\n",
        "        sum_of_gradients = 0\n",
        "\n",
        "        for cluster in range(codebook['labels'][index].numel()):\n",
        "          if codebook['labels'][index][cluster] == centroid:\n",
        "            sum_of_gradients = sum_of_gradients + gradient[cluster]\n",
        "\n",
        "        codebook['centroids'][index][centroid] -= learning_rate_tuning * sum_of_gradients\n",
        "        \n",
        "      new_weights = codebook['masks'][index].to('cpu').numpy()\n",
        "      \n",
        "      np.place(new_weights, new_weights, codebook['centroids'][index][codebook['labels'][index]])\n",
        "\n",
        "      param.data = torch.from_numpy(new_weights).to(device)\n",
        "    \n",
        "    #learning_rate_tuning = adjust_learning_rate(learning_rate_tuning, optimizer, n_iter)\n",
        "    #for param_group in optimizer.param_groups:\n",
        "    #  print(param_group['lr'])\n",
        "      \n",
        "    test(n_epochs + n_iter)\n",
        "  \n",
        "\n",
        "def save_codebook():\n",
        "  \n",
        "  torch.set_printoptions(profile='full')\n",
        "  \n",
        "  codified_codebook = str(codebook).replace(' ', '').replace('tensor([', '$').replace(']),', '$').replace('$$', '$').replace('\\n', '').replace(\"])],'labels':[\", \"$$\").replace(\"{'centroids':[\", \"\").replace('])]}', '')\n",
        "  \n",
        "  # print(codified_codebook)\n",
        "  \n",
        "  codebook_filename = '../' + network.network_type() + '_codebook'\n",
        "  \n",
        "  with open(codebook_filename, 'w') as file:\n",
        "    file.write(codified_codebook)\n",
        "    \n",
        "  with gzip.open(codebook_filename + '.gz', 'wb') as file:\n",
        "    file.write(codified_codebook.encode())\n",
        "\n",
        "    \n",
        "def load_codebook():\n",
        "  \n",
        "  codebook_filename = '../' + network.network_type() + '_codebook'\n",
        "    \n",
        "  with gzip.open(codebook_filename + '.gz', 'r') as file:\n",
        "    stored_codebook = file.read().decode().split('$$')\n",
        "\n",
        "    centroids = re.findall(r'\\$([0-9+\\-.,e]+)', stored_codebook[0])\n",
        "    for centroid in centroids:\n",
        "      centroid_ = [float(i) for i in centroid.split(',')]\n",
        "      codebook['centroids'].append(torch.Tensor(centroid_))\n",
        "      \n",
        "    labels = re.findall(r'\\$([0-9+\\-.,e]+)', stored_codebook[1])\n",
        "    for label in labels:\n",
        "      label_ = [int(i) for i in label.split(',')]\n",
        "      codebook['labels'].append(torch.LongTensor(label_))\n",
        "      \n",
        "  #print(codebook)\n",
        "      \n",
        "     \n",
        "def train(experiment_name=None, writer=None):\n",
        "  \n",
        "    global globaliter\n",
        "    \n",
        "    print('\\nStart training from ' + dataset.__name__ + ' training set to generate the model...')\n",
        "    print('Epochs: ' + str(n_epochs) + '\\nLearning rate: ' + str(learning_rate) + '\\n')\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    # TRAIN LOOP #\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        test(epoch, experiment_name, writer)\n",
        "        compute_and_print_sparsity(experiment_name=experiment_name, writer=writer)\n",
        "        network.train()\n",
        "\n",
        "        for batch_index, (data, target) in enumerate(train_set):\n",
        "\n",
        "            if use_quaternion_variant:\n",
        "                data = expand_input(data, input_expansion)\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = network(data)  # Forward pass\n",
        "            loss = loss_criterion(output, target)\n",
        "            loss_with_reg = loss + regularization_factor * regularization(regularizer)\n",
        "            loss_with_reg.backward()  # Backward pass\n",
        "            \n",
        "            optimizer.step()  # Optimize\n",
        "\n",
        "            if batch_index % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch + 1, batch_index * data.shape[0], len(train_set.dataset),\n",
        "                    100. * batch_index / len(train_set), loss.item()))\n",
        "                train_losses.append(loss_with_reg.item())\n",
        "                train_counter.append((batch_index * batch_size_train) + (epoch * len(train_set.dataset)))\n",
        "                \n",
        "            if use_tensorboard and writer is not None:\n",
        "              writer.add_scalar('Performance/Loss', loss.item(), globaliter)\n",
        "                  \n",
        "              for name, param in network.named_parameters():\n",
        "                writer.add_histogram(name, param.clone().to('cpu').detach().numpy(), globaliter)\n",
        "\n",
        "              for name, module in network.named_modules():\n",
        "                if isinstance(module, (QuaternionConv, QuaternionLinear, QuaternionTransposeConv)):\n",
        "                  quat_norm = torch.sqrt(module.r_weight**2 + module.i_weight**2 + module.j_weight**2 + module.k_weight**2)\n",
        "                  writer.add_histogram(name +'_norm_'+regularizer, quat_norm, globaliter)\n",
        "            \n",
        "            globaliter += 1\n",
        "    \n",
        "    test(n_epochs, experiment_name, writer)\n",
        "    compute_and_print_sparsity(experiment_name=experiment_name, writer=writer)\n",
        "    print('Elapsed time: {:.2f} seconds\\n'.format(time.time()-start_time))\n",
        "    \n",
        "        \n",
        "def test(epoch, experiment_name=None, writer=None):\n",
        "  \n",
        "    network.eval()\n",
        "  \n",
        "    test_loss, correct = 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for data, target in test_set:\n",
        "\n",
        "        if use_quaternion_variant:\n",
        "          data = expand_input(data, input_expansion)\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        output = network(data)\n",
        "        test_loss += loss_criterion(output, target, reduction='sum').item()\n",
        "        pred = output.detach().max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.detach().view_as(pred)).sum().item()\n",
        "        \n",
        "    test_loss /= len(test_set.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "    \n",
        "    accuracy = 100. * correct / len(test_set.dataset)\n",
        "    \n",
        "    if experiment_name is not None:\n",
        "      experiments_results[experiment_name]['accuracy'].append(accuracy)\n",
        "    test_counter.append(epoch * len(train_set.dataset))\n",
        "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_set.dataset), accuracy))\n",
        "    \n",
        "    if use_tensorboard and writer is not None:\n",
        "      writer.add_scalar('Performance/Accuracy', accuracy, epoch)\n",
        "\n",
        "        \n",
        "def inference(raw_image):\n",
        "  \n",
        "    image_tensor = raw_image.unsqueeze_(0).to(device)\n",
        "    if use_quaternion_variant:\n",
        "        image_tensor = expand_input(image_tensor, input_expansion)\n",
        "    network.eval()\n",
        "    output = network(image_tensor)\n",
        "    index = torch.argmax(output).item()\n",
        "    index = CIFAR10_num_to_classes[index] if dataset == CIFAR10 else index\n",
        "    return index\n",
        "\n",
        "\n",
        "def show_image(image, text_ground_truth):\n",
        "    plt.tight_layout()\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    image = np.transpose(image / 2 + 0.5, (1, 2, 0)) if dataset == CIFAR10 else image[0]\n",
        "    plt.imshow(image, cmap='gray', interpolation='nearest')\n",
        "\n",
        "    text_ground_truth = CIFAR10_num_to_classes[text_ground_truth] if dataset == CIFAR10 else text_ground_truth\n",
        "    plt.title('Ground Truth: {}'.format(text_ground_truth))\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def plot_training_curve():\n",
        "    plt.plot(train_counter, train_losses, color='blue')\n",
        "    plt.scatter(test_counter, test_losses, color='red')\n",
        "    plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "    plt.xlabel('Number of training examples seen')\n",
        "    plt.ylabel(loss_criterion.__name__.capitalize().replace('_', ' '))\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def run_tensorboard_server():\n",
        "  print('\\nStarting Tensorboard server...')\n",
        "  !pip install -qqq tensorboardX tensorboardcolab\n",
        "  from tensorboardX import SummaryWriter\n",
        "  from tensorboardcolab import TensorBoardColab\n",
        "  TensorBoardColab()\n",
        "  return SummaryWriter('Graph')\n",
        "\n",
        "\n",
        "#def set_deterministic_environment(seed=1234):\n",
        "#  torch.manual_seed(seed)\n",
        "#  torch.cuda.manual_seed(seed)\n",
        "#  torch.backends.cudnn.deterministic = True\n",
        "  \n",
        "    \n",
        "def smooth_data(list_x, list_y, smooth=125):\n",
        "  \n",
        "  #bspl = splrep(list_x, list_y, s=smooth)\n",
        "  #bspl_y = splev(list_x, bspl)\n",
        "  \n",
        "  #df = pd.DataFrame(list_y, columns=['a'])\n",
        "  #df.ewm(halflife=6.).mean()\n",
        "    \n",
        "  return list_y #bspl_y\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Pytorch-Quaternion-Neural-Networks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM-Dlt5Vmjz0",
        "colab_type": "text"
      },
      "source": [
        "## Test Quaternion vs Real-valued NNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeDB_rdkmjSw",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "dbc5f8c1-b71c-47d7-99fc-ccf79f163186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class CIFARConvNetA(nn.Module):  # Standard CNN for CIFAR-10\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CIFARConvNetA, self).__init__()\n",
        "\n",
        "        self.act_fn = F.relu\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2_drop1 = nn.Dropout2d()\n",
        "        self.conv3 = nn.Conv2d(16, 24, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(24, 76, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4_drop2 = nn.Dropout2d()\n",
        "        self.conv5 = nn.Conv2d(76, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(2048, 40)\n",
        "        self.fc2 = nn.Linear(40, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act_fn(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = self.act_fn(F.max_pool2d(self.conv2_drop1(self.conv2(x)), 2))\n",
        "        x = self.act_fn(self.conv3(x))\n",
        "        x = self.act_fn(F.max_pool2d(self.conv4_drop2(self.conv4(x)), 2))\n",
        "        x = self.act_fn(self.conv5(x))\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1, 2048)\n",
        "        x = self.act_fn(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def network_type(self):\n",
        "        return type(self).__name__\n",
        "      \n",
        "      \n",
        "class CIFARQConvNetA(nn.Module):  # Quaternion CNN for CIFAR-10\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CIFARQConvNetA, self).__init__()\n",
        "\n",
        "        self.act_fn = F.relu\n",
        "\n",
        "        self.conv1 = QuaternionConv(4, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = QuaternionConv(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2_drop1 = nn.Dropout2d()\n",
        "        self.conv3 = QuaternionConv(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = QuaternionConv(64, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4_drop2 = nn.Dropout2d()\n",
        "        self.conv5 = QuaternionConv(96, 128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.fc1 = QuaternionLinear(2048, 40)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act_fn(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = self.act_fn(F.max_pool2d(self.conv2_drop1(self.conv2(x)), 2))\n",
        "        x = self.act_fn(self.conv3(x))\n",
        "        x = self.act_fn(F.max_pool2d(self.conv4_drop2(self.conv4(x)), 2))\n",
        "        x = self.act_fn(self.conv5(x))\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1, 2048)\n",
        "        x = self.act_fn(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = torch.reshape(x, (-1, 10, 4))\n",
        "        x = torch.sum(torch.abs(x), dim=2)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def network_type(self):\n",
        "        return type(self).__name__\n",
        "\n",
        "\n",
        "#@markdown ##Parameters and Hyper-parameters\n",
        "n_epochs = 50 #@param {type: 'number'}\n",
        "dataset = CIFAR10 #@param [\"CIFAR10\", \"MNIST\"] {type:\"raw\"}\n",
        "learning_rate = 0.002 #@param {type: 'number'}\n",
        "loss_criterion = F.cross_entropy #@param [\"F.cross_entropy\", \"F.nll_loss\"] {type:\"raw\"}\n",
        "batch_size_train = 400 #@param {type: 'number'}\n",
        "batch_size_test = 1000 #@param {type: 'number'}\n",
        "use_tensorboard = False #@param {type:\"boolean\"}\n",
        "\n",
        "log_interval = 10\n",
        "globaliter = 0\n",
        "\n",
        "#set_deterministic_environment()\n",
        "\n",
        "input_expansion = 'zero_vector' if dataset == MNIST else 'rgb_vector'\n",
        "regularizer = 'None'\n",
        "regularization_factor = 0.\n",
        "\n",
        "train_counter, train_losses, test_counter, test_losses = [], [], [], []\n",
        "\n",
        "writer = run_tensorboard_server() if use_tensorboard else None\n",
        "\n",
        "experiments = {\n",
        "    'R-CNN' : { 'network': MNISTConvNet() if dataset == MNIST else CIFARConvNetA() },\n",
        "    'Q-CNN' : { 'network': MNISTQConvNet() if dataset == MNIST else CIFARQConvNetA() }\n",
        "}\n",
        "\n",
        "experiments_results = {\n",
        "  'R-CNN' : { 'accuracy' : [], 'weight' : [], 'quaternion' : [] },\n",
        "  'Q-CNN' : { 'accuracy' : [], 'weight' : [], 'quaternion' : [] },\n",
        "}\n",
        "\n",
        "plot1, ax1 = plt.subplots()\n",
        "plot2, ax2 = plt.subplots()\n",
        "plot3, ax3 = plt.subplots()\n",
        "epoch_counter = list(range(n_epochs+1))\n",
        "for index, (experiment_name, experiment_params) in enumerate(experiments.items()):\n",
        "  \n",
        "  use_quaternion_variant = True if experiment_name == 'Q-CNN' else False\n",
        "  \n",
        "  network = experiment_params['network']\n",
        "  network = network.to(device)\n",
        "  optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "  \n",
        "  print('Device used: ' + device.type)\n",
        "  print('Network variant: ' + network.network_type())\n",
        "  print('Number of trainable parameters: {}\\n'.format(count_trainable_parameters()))\n",
        "  \n",
        "  train_set, test_set = get_dataset()\n",
        "  \n",
        "  train(experiment_name, writer)\n",
        "  \n",
        "  print(experiment_name + ' weight')\n",
        "  print(experiments_results[experiment_name]['weight'])\n",
        "  print(experiment_name + ' quaternion')\n",
        "  print(experiments_results[experiment_name]['quaternion'])\n",
        "  print(experiment_name + ' accuracy')\n",
        "  print(experiments_results[experiment_name]['accuracy'])\n",
        "  \n",
        "  ax1.plot(epoch_counter, smooth_data(test_counter, experiments_results[experiment_name]['weight']), color=colors[index])\n",
        "  ax2.plot(epoch_counter, smooth_data(test_counter, experiments_results[experiment_name]['quaternion']), color=colors[index])\n",
        "  ax3.plot(epoch_counter, smooth_data(test_counter, experiments_results[experiment_name]['accuracy']), color=colors[index]) \n",
        "\n",
        "  \n",
        "ax1.legend(experiments.keys(), loc='best')\n",
        "ax2.legend(experiments.keys(), loc='best')\n",
        "ax3.legend(experiments.keys(), loc='best')\n",
        "ax1.set(xlabel='Epochs', ylabel='Weight Sparsity [%]')\n",
        "ax2.set(xlabel='Epochs', ylabel='Quaternion Sparsity [%]')\n",
        "ax3.set(xlabel='Epochs', ylabel='Test Accuracy [%]')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used: cuda\n",
            "Network variant: CIFARConvNetA\n",
            "Number of trainable parameters: 191414\n",
            "\n",
            "Retrieve CIFAR10 dataset...\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Start training from CIFAR10 training set to generate the model...\n",
            "Epochs: 50\n",
            "Learning rate: 0.002\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 2.3072, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 3.33%\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.317948\n",
            "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 2.231289\n",
            "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2.236071\n",
            "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 2.173315\n",
            "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.140582\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 2.149676\n",
            "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 2.071027\n",
            "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 2.049323\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.011396\n",
            "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 2.052885\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1.992829\n",
            "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 2.062087\n",
            "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 2.017730\n",
            "\n",
            "Test set: Avg. loss: 1.8815, Accuracy: 2985/10000 (29.85%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 3.00%\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.949268\n",
            "Train Epoch: 2 [4000/50000 (8%)]\tLoss: 1.967744\n",
            "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 1.965709\n",
            "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 1.893054\n",
            "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.897859\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 1.942322\n",
            "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1.915005\n",
            "Train Epoch: 2 [28000/50000 (56%)]\tLoss: 1.894631\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.869746\n",
            "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 1.969401\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1.904340\n",
            "Train Epoch: 2 [44000/50000 (88%)]\tLoss: 1.893558\n",
            "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.925686\n",
            "\n",
            "Test set: Avg. loss: 1.8194, Accuracy: 3198/10000 (31.98%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.73%\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.933832\n",
            "Train Epoch: 3 [4000/50000 (8%)]\tLoss: 1.866512\n",
            "Train Epoch: 3 [8000/50000 (16%)]\tLoss: 1.869679\n",
            "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 1.929708\n",
            "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.820198\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1.871499\n",
            "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.815542\n",
            "Train Epoch: 3 [28000/50000 (56%)]\tLoss: 1.904343\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.818194\n",
            "Train Epoch: 3 [36000/50000 (72%)]\tLoss: 1.809425\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1.841319\n",
            "Train Epoch: 3 [44000/50000 (88%)]\tLoss: 1.823760\n",
            "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.882811\n",
            "\n",
            "Test set: Avg. loss: 1.8033, Accuracy: 3466/10000 (34.66%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.56%\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.827881\n",
            "Train Epoch: 4 [4000/50000 (8%)]\tLoss: 1.813022\n",
            "Train Epoch: 4 [8000/50000 (16%)]\tLoss: 1.783597\n",
            "Train Epoch: 4 [12000/50000 (24%)]\tLoss: 1.804170\n",
            "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1.801701\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1.832705\n",
            "Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1.778798\n",
            "Train Epoch: 4 [28000/50000 (56%)]\tLoss: 1.790872\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.805841\n",
            "Train Epoch: 4 [36000/50000 (72%)]\tLoss: 1.765901\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1.772204\n",
            "Train Epoch: 4 [44000/50000 (88%)]\tLoss: 1.812631\n",
            "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.792447\n",
            "\n",
            "Test set: Avg. loss: 1.6765, Accuracy: 3773/10000 (37.73%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.41%\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.751192\n",
            "Train Epoch: 5 [4000/50000 (8%)]\tLoss: 1.812579\n",
            "Train Epoch: 5 [8000/50000 (16%)]\tLoss: 1.714453\n",
            "Train Epoch: 5 [12000/50000 (24%)]\tLoss: 1.762963\n",
            "Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1.811869\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 1.851050\n",
            "Train Epoch: 5 [24000/50000 (48%)]\tLoss: 1.804722\n",
            "Train Epoch: 5 [28000/50000 (56%)]\tLoss: 1.779690\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.762049\n",
            "Train Epoch: 5 [36000/50000 (72%)]\tLoss: 1.781400\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1.735280\n",
            "Train Epoch: 5 [44000/50000 (88%)]\tLoss: 1.809280\n",
            "Train Epoch: 5 [48000/50000 (96%)]\tLoss: 1.747725\n",
            "\n",
            "Test set: Avg. loss: 1.6090, Accuracy: 4128/10000 (41.28%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.35%\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.781248\n",
            "Train Epoch: 6 [4000/50000 (8%)]\tLoss: 1.690762\n",
            "Train Epoch: 6 [8000/50000 (16%)]\tLoss: 1.683374\n",
            "Train Epoch: 6 [12000/50000 (24%)]\tLoss: 1.766430\n",
            "Train Epoch: 6 [16000/50000 (32%)]\tLoss: 1.703148\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tLoss: 1.723791\n",
            "Train Epoch: 6 [24000/50000 (48%)]\tLoss: 1.744905\n",
            "Train Epoch: 6 [28000/50000 (56%)]\tLoss: 1.817739\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.649790\n",
            "Train Epoch: 6 [36000/50000 (72%)]\tLoss: 1.751350\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1.728273\n",
            "Train Epoch: 6 [44000/50000 (88%)]\tLoss: 1.672772\n",
            "Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.707731\n",
            "\n",
            "Test set: Avg. loss: 1.6085, Accuracy: 4181/10000 (41.81%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.29%\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.726610\n",
            "Train Epoch: 7 [4000/50000 (8%)]\tLoss: 1.673230\n",
            "Train Epoch: 7 [8000/50000 (16%)]\tLoss: 1.715169\n",
            "Train Epoch: 7 [12000/50000 (24%)]\tLoss: 1.610250\n",
            "Train Epoch: 7 [16000/50000 (32%)]\tLoss: 1.811842\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tLoss: 1.700171\n",
            "Train Epoch: 7 [24000/50000 (48%)]\tLoss: 1.701898\n",
            "Train Epoch: 7 [28000/50000 (56%)]\tLoss: 1.715686\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.638957\n",
            "Train Epoch: 7 [36000/50000 (72%)]\tLoss: 1.666737\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tLoss: 1.659648\n",
            "Train Epoch: 7 [44000/50000 (88%)]\tLoss: 1.712152\n",
            "Train Epoch: 7 [48000/50000 (96%)]\tLoss: 1.788301\n",
            "\n",
            "Test set: Avg. loss: 1.5762, Accuracy: 4283/10000 (42.83%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.19%\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.710726\n",
            "Train Epoch: 8 [4000/50000 (8%)]\tLoss: 1.642332\n",
            "Train Epoch: 8 [8000/50000 (16%)]\tLoss: 1.725750\n",
            "Train Epoch: 8 [12000/50000 (24%)]\tLoss: 1.721244\n",
            "Train Epoch: 8 [16000/50000 (32%)]\tLoss: 1.676966\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tLoss: 1.747982\n",
            "Train Epoch: 8 [24000/50000 (48%)]\tLoss: 1.654332\n",
            "Train Epoch: 8 [28000/50000 (56%)]\tLoss: 1.724885\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.664388\n",
            "Train Epoch: 8 [36000/50000 (72%)]\tLoss: 1.639816\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tLoss: 1.723584\n",
            "Train Epoch: 8 [44000/50000 (88%)]\tLoss: 1.608360\n",
            "Train Epoch: 8 [48000/50000 (96%)]\tLoss: 1.654946\n",
            "\n",
            "Test set: Avg. loss: 1.5312, Accuracy: 4497/10000 (44.97%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.13%\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.665345\n",
            "Train Epoch: 9 [4000/50000 (8%)]\tLoss: 1.698800\n",
            "Train Epoch: 9 [8000/50000 (16%)]\tLoss: 1.609973\n",
            "Train Epoch: 9 [12000/50000 (24%)]\tLoss: 1.650449\n",
            "Train Epoch: 9 [16000/50000 (32%)]\tLoss: 1.679539\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tLoss: 1.728771\n",
            "Train Epoch: 9 [24000/50000 (48%)]\tLoss: 1.654571\n",
            "Train Epoch: 9 [28000/50000 (56%)]\tLoss: 1.617769\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.675561\n",
            "Train Epoch: 9 [36000/50000 (72%)]\tLoss: 1.675353\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1.697401\n",
            "Train Epoch: 9 [44000/50000 (88%)]\tLoss: 1.658277\n",
            "Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1.726187\n",
            "\n",
            "Test set: Avg. loss: 1.5433, Accuracy: 4544/10000 (45.44%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.09%\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.684728\n",
            "Train Epoch: 10 [4000/50000 (8%)]\tLoss: 1.667146\n",
            "Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.703049\n",
            "Train Epoch: 10 [12000/50000 (24%)]\tLoss: 1.619016\n",
            "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.728984\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tLoss: 1.643450\n",
            "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1.671253\n",
            "Train Epoch: 10 [28000/50000 (56%)]\tLoss: 1.647606\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.640045\n",
            "Train Epoch: 10 [36000/50000 (72%)]\tLoss: 1.635735\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.593117\n",
            "Train Epoch: 10 [44000/50000 (88%)]\tLoss: 1.616886\n",
            "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.660964\n",
            "\n",
            "Test set: Avg. loss: 1.5028, Accuracy: 4714/10000 (47.14%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.00%\n",
            "\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.603430\n",
            "Train Epoch: 11 [4000/50000 (8%)]\tLoss: 1.647437\n",
            "Train Epoch: 11 [8000/50000 (16%)]\tLoss: 1.644849\n",
            "Train Epoch: 11 [12000/50000 (24%)]\tLoss: 1.560653\n",
            "Train Epoch: 11 [16000/50000 (32%)]\tLoss: 1.585688\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tLoss: 1.572836\n",
            "Train Epoch: 11 [24000/50000 (48%)]\tLoss: 1.639188\n",
            "Train Epoch: 11 [28000/50000 (56%)]\tLoss: 1.581702\n",
            "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 1.613326\n",
            "Train Epoch: 11 [36000/50000 (72%)]\tLoss: 1.544608\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tLoss: 1.641073\n",
            "Train Epoch: 11 [44000/50000 (88%)]\tLoss: 1.589941\n",
            "Train Epoch: 11 [48000/50000 (96%)]\tLoss: 1.694837\n",
            "\n",
            "Test set: Avg. loss: 1.5014, Accuracy: 4662/10000 (46.62%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.92%\n",
            "\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.655213\n",
            "Train Epoch: 12 [4000/50000 (8%)]\tLoss: 1.633946\n",
            "Train Epoch: 12 [8000/50000 (16%)]\tLoss: 1.516842\n",
            "Train Epoch: 12 [12000/50000 (24%)]\tLoss: 1.547174\n",
            "Train Epoch: 12 [16000/50000 (32%)]\tLoss: 1.527496\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tLoss: 1.664669\n",
            "Train Epoch: 12 [24000/50000 (48%)]\tLoss: 1.659089\n",
            "Train Epoch: 12 [28000/50000 (56%)]\tLoss: 1.592623\n",
            "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1.638640\n",
            "Train Epoch: 12 [36000/50000 (72%)]\tLoss: 1.548717\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tLoss: 1.580753\n",
            "Train Epoch: 12 [44000/50000 (88%)]\tLoss: 1.592304\n",
            "Train Epoch: 12 [48000/50000 (96%)]\tLoss: 1.575977\n",
            "\n",
            "Test set: Avg. loss: 1.4681, Accuracy: 4791/10000 (47.91%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.91%\n",
            "\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.550424\n",
            "Train Epoch: 13 [4000/50000 (8%)]\tLoss: 1.675399\n",
            "Train Epoch: 13 [8000/50000 (16%)]\tLoss: 1.620026\n",
            "Train Epoch: 13 [12000/50000 (24%)]\tLoss: 1.636815\n",
            "Train Epoch: 13 [16000/50000 (32%)]\tLoss: 1.602534\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tLoss: 1.553735\n",
            "Train Epoch: 13 [24000/50000 (48%)]\tLoss: 1.561806\n",
            "Train Epoch: 13 [28000/50000 (56%)]\tLoss: 1.549393\n",
            "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 1.564658\n",
            "Train Epoch: 13 [36000/50000 (72%)]\tLoss: 1.735262\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tLoss: 1.715605\n",
            "Train Epoch: 13 [44000/50000 (88%)]\tLoss: 1.527546\n",
            "Train Epoch: 13 [48000/50000 (96%)]\tLoss: 1.578838\n",
            "\n",
            "Test set: Avg. loss: 1.4570, Accuracy: 4846/10000 (48.46%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.83%\n",
            "\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.628518\n",
            "Train Epoch: 14 [4000/50000 (8%)]\tLoss: 1.560527\n",
            "Train Epoch: 14 [8000/50000 (16%)]\tLoss: 1.623049\n",
            "Train Epoch: 14 [12000/50000 (24%)]\tLoss: 1.506482\n",
            "Train Epoch: 14 [16000/50000 (32%)]\tLoss: 1.614140\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tLoss: 1.586584\n",
            "Train Epoch: 14 [24000/50000 (48%)]\tLoss: 1.612059\n",
            "Train Epoch: 14 [28000/50000 (56%)]\tLoss: 1.513819\n",
            "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 1.589568\n",
            "Train Epoch: 14 [36000/50000 (72%)]\tLoss: 1.526880\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1.499562\n",
            "Train Epoch: 14 [44000/50000 (88%)]\tLoss: 1.627485\n",
            "Train Epoch: 14 [48000/50000 (96%)]\tLoss: 1.609275\n",
            "\n",
            "Test set: Avg. loss: 1.3929, Accuracy: 5043/10000 (50.43%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.78%\n",
            "\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.543197\n",
            "Train Epoch: 15 [4000/50000 (8%)]\tLoss: 1.518327\n",
            "Train Epoch: 15 [8000/50000 (16%)]\tLoss: 1.542316\n",
            "Train Epoch: 15 [12000/50000 (24%)]\tLoss: 1.468176\n",
            "Train Epoch: 15 [16000/50000 (32%)]\tLoss: 1.555228\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tLoss: 1.533169\n",
            "Train Epoch: 15 [24000/50000 (48%)]\tLoss: 1.466210\n",
            "Train Epoch: 15 [28000/50000 (56%)]\tLoss: 1.520095\n",
            "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 1.549033\n",
            "Train Epoch: 15 [36000/50000 (72%)]\tLoss: 1.508064\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1.571210\n",
            "Train Epoch: 15 [44000/50000 (88%)]\tLoss: 1.566316\n",
            "Train Epoch: 15 [48000/50000 (96%)]\tLoss: 1.546785\n",
            "\n",
            "Test set: Avg. loss: 1.4162, Accuracy: 4992/10000 (49.92%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.77%\n",
            "\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.471257\n",
            "Train Epoch: 16 [4000/50000 (8%)]\tLoss: 1.555204\n",
            "Train Epoch: 16 [8000/50000 (16%)]\tLoss: 1.573207\n",
            "Train Epoch: 16 [12000/50000 (24%)]\tLoss: 1.599762\n",
            "Train Epoch: 16 [16000/50000 (32%)]\tLoss: 1.617484\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tLoss: 1.494239\n",
            "Train Epoch: 16 [24000/50000 (48%)]\tLoss: 1.489451\n",
            "Train Epoch: 16 [28000/50000 (56%)]\tLoss: 1.612172\n",
            "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 1.521700\n",
            "Train Epoch: 16 [36000/50000 (72%)]\tLoss: 1.626178\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tLoss: 1.473722\n",
            "Train Epoch: 16 [44000/50000 (88%)]\tLoss: 1.554818\n",
            "Train Epoch: 16 [48000/50000 (96%)]\tLoss: 1.574890\n",
            "\n",
            "Test set: Avg. loss: 1.3689, Accuracy: 5126/10000 (51.26%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.75%\n",
            "\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.559217\n",
            "Train Epoch: 17 [4000/50000 (8%)]\tLoss: 1.549054\n",
            "Train Epoch: 17 [8000/50000 (16%)]\tLoss: 1.433289\n",
            "Train Epoch: 17 [12000/50000 (24%)]\tLoss: 1.543825\n",
            "Train Epoch: 17 [16000/50000 (32%)]\tLoss: 1.564789\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tLoss: 1.599146\n",
            "Train Epoch: 17 [24000/50000 (48%)]\tLoss: 1.527162\n",
            "Train Epoch: 17 [28000/50000 (56%)]\tLoss: 1.522519\n",
            "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 1.515631\n",
            "Train Epoch: 17 [36000/50000 (72%)]\tLoss: 1.496268\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tLoss: 1.484033\n",
            "Train Epoch: 17 [44000/50000 (88%)]\tLoss: 1.534023\n",
            "Train Epoch: 17 [48000/50000 (96%)]\tLoss: 1.558401\n",
            "\n",
            "Test set: Avg. loss: 1.3842, Accuracy: 5153/10000 (51.53%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.68%\n",
            "\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.498091\n",
            "Train Epoch: 18 [4000/50000 (8%)]\tLoss: 1.451786\n",
            "Train Epoch: 18 [8000/50000 (16%)]\tLoss: 1.563669\n",
            "Train Epoch: 18 [12000/50000 (24%)]\tLoss: 1.377315\n",
            "Train Epoch: 18 [16000/50000 (32%)]\tLoss: 1.547863\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tLoss: 1.461740\n",
            "Train Epoch: 18 [24000/50000 (48%)]\tLoss: 1.618622\n",
            "Train Epoch: 18 [28000/50000 (56%)]\tLoss: 1.585878\n",
            "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 1.491253\n",
            "Train Epoch: 18 [36000/50000 (72%)]\tLoss: 1.531888\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tLoss: 1.543754\n",
            "Train Epoch: 18 [44000/50000 (88%)]\tLoss: 1.435930\n",
            "Train Epoch: 18 [48000/50000 (96%)]\tLoss: 1.495780\n",
            "\n",
            "Test set: Avg. loss: 1.3390, Accuracy: 5296/10000 (52.96%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.68%\n",
            "\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.512283\n",
            "Train Epoch: 19 [4000/50000 (8%)]\tLoss: 1.506918\n",
            "Train Epoch: 19 [8000/50000 (16%)]\tLoss: 1.531895\n",
            "Train Epoch: 19 [12000/50000 (24%)]\tLoss: 1.465279\n",
            "Train Epoch: 19 [16000/50000 (32%)]\tLoss: 1.502983\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tLoss: 1.560890\n",
            "Train Epoch: 19 [24000/50000 (48%)]\tLoss: 1.448866\n",
            "Train Epoch: 19 [28000/50000 (56%)]\tLoss: 1.468519\n",
            "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 1.477014\n",
            "Train Epoch: 19 [36000/50000 (72%)]\tLoss: 1.490230\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tLoss: 1.509950\n",
            "Train Epoch: 19 [44000/50000 (88%)]\tLoss: 1.375337\n",
            "Train Epoch: 19 [48000/50000 (96%)]\tLoss: 1.503275\n",
            "\n",
            "Test set: Avg. loss: 1.3631, Accuracy: 5228/10000 (52.28%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.67%\n",
            "\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.473307\n",
            "Train Epoch: 20 [4000/50000 (8%)]\tLoss: 1.491618\n",
            "Train Epoch: 20 [8000/50000 (16%)]\tLoss: 1.561756\n",
            "Train Epoch: 20 [12000/50000 (24%)]\tLoss: 1.554616\n",
            "Train Epoch: 20 [16000/50000 (32%)]\tLoss: 1.485191\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tLoss: 1.589006\n",
            "Train Epoch: 20 [24000/50000 (48%)]\tLoss: 1.528654\n",
            "Train Epoch: 20 [28000/50000 (56%)]\tLoss: 1.409828\n",
            "Train Epoch: 20 [32000/50000 (64%)]\tLoss: 1.444776\n",
            "Train Epoch: 20 [36000/50000 (72%)]\tLoss: 1.477999\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tLoss: 1.455130\n",
            "Train Epoch: 20 [44000/50000 (88%)]\tLoss: 1.396323\n",
            "Train Epoch: 20 [48000/50000 (96%)]\tLoss: 1.530115\n",
            "\n",
            "Test set: Avg. loss: 1.3599, Accuracy: 5220/10000 (52.20%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.67%\n",
            "\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.485441\n",
            "Train Epoch: 21 [4000/50000 (8%)]\tLoss: 1.486985\n",
            "Train Epoch: 21 [8000/50000 (16%)]\tLoss: 1.501687\n",
            "Train Epoch: 21 [12000/50000 (24%)]\tLoss: 1.415064\n",
            "Train Epoch: 21 [16000/50000 (32%)]\tLoss: 1.460735\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tLoss: 1.490368\n",
            "Train Epoch: 21 [24000/50000 (48%)]\tLoss: 1.478763\n",
            "Train Epoch: 21 [28000/50000 (56%)]\tLoss: 1.512848\n",
            "Train Epoch: 21 [32000/50000 (64%)]\tLoss: 1.566108\n",
            "Train Epoch: 21 [36000/50000 (72%)]\tLoss: 1.473948\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tLoss: 1.363781\n",
            "Train Epoch: 21 [44000/50000 (88%)]\tLoss: 1.499379\n",
            "Train Epoch: 21 [48000/50000 (96%)]\tLoss: 1.496432\n",
            "\n",
            "Test set: Avg. loss: 1.3308, Accuracy: 5311/10000 (53.11%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.63%\n",
            "\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLoss: 1.504954\n",
            "Train Epoch: 22 [4000/50000 (8%)]\tLoss: 1.496694\n",
            "Train Epoch: 22 [8000/50000 (16%)]\tLoss: 1.490621\n",
            "Train Epoch: 22 [12000/50000 (24%)]\tLoss: 1.429698\n",
            "Train Epoch: 22 [16000/50000 (32%)]\tLoss: 1.441765\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tLoss: 1.417853\n",
            "Train Epoch: 22 [24000/50000 (48%)]\tLoss: 1.429505\n",
            "Train Epoch: 22 [28000/50000 (56%)]\tLoss: 1.556251\n",
            "Train Epoch: 22 [32000/50000 (64%)]\tLoss: 1.513880\n",
            "Train Epoch: 22 [36000/50000 (72%)]\tLoss: 1.462527\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tLoss: 1.457877\n",
            "Train Epoch: 22 [44000/50000 (88%)]\tLoss: 1.471895\n",
            "Train Epoch: 22 [48000/50000 (96%)]\tLoss: 1.460892\n",
            "\n",
            "Test set: Avg. loss: 1.2969, Accuracy: 5432/10000 (54.32%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.62%\n",
            "\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.464415\n",
            "Train Epoch: 23 [4000/50000 (8%)]\tLoss: 1.423887\n",
            "Train Epoch: 23 [8000/50000 (16%)]\tLoss: 1.455235\n",
            "Train Epoch: 23 [12000/50000 (24%)]\tLoss: 1.410709\n",
            "Train Epoch: 23 [16000/50000 (32%)]\tLoss: 1.477752\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tLoss: 1.532298\n",
            "Train Epoch: 23 [24000/50000 (48%)]\tLoss: 1.509521\n",
            "Train Epoch: 23 [28000/50000 (56%)]\tLoss: 1.467092\n",
            "Train Epoch: 23 [32000/50000 (64%)]\tLoss: 1.448706\n",
            "Train Epoch: 23 [36000/50000 (72%)]\tLoss: 1.473614\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tLoss: 1.421809\n",
            "Train Epoch: 23 [44000/50000 (88%)]\tLoss: 1.420266\n",
            "Train Epoch: 23 [48000/50000 (96%)]\tLoss: 1.472179\n",
            "\n",
            "Test set: Avg. loss: 1.2967, Accuracy: 5443/10000 (54.43%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.61%\n",
            "\n",
            "Train Epoch: 24 [0/50000 (0%)]\tLoss: 1.430734\n",
            "Train Epoch: 24 [4000/50000 (8%)]\tLoss: 1.409416\n",
            "Train Epoch: 24 [8000/50000 (16%)]\tLoss: 1.417768\n",
            "Train Epoch: 24 [12000/50000 (24%)]\tLoss: 1.469558\n",
            "Train Epoch: 24 [16000/50000 (32%)]\tLoss: 1.389540\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tLoss: 1.371369\n",
            "Train Epoch: 24 [24000/50000 (48%)]\tLoss: 1.440619\n",
            "Train Epoch: 24 [28000/50000 (56%)]\tLoss: 1.505800\n",
            "Train Epoch: 24 [32000/50000 (64%)]\tLoss: 1.389501\n",
            "Train Epoch: 24 [36000/50000 (72%)]\tLoss: 1.471483\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tLoss: 1.486972\n",
            "Train Epoch: 24 [44000/50000 (88%)]\tLoss: 1.530757\n",
            "Train Epoch: 24 [48000/50000 (96%)]\tLoss: 1.485478\n",
            "\n",
            "Test set: Avg. loss: 1.2677, Accuracy: 5574/10000 (55.74%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.61%\n",
            "\n",
            "Train Epoch: 25 [0/50000 (0%)]\tLoss: 1.431813\n",
            "Train Epoch: 25 [4000/50000 (8%)]\tLoss: 1.392997\n",
            "Train Epoch: 25 [8000/50000 (16%)]\tLoss: 1.483608\n",
            "Train Epoch: 25 [12000/50000 (24%)]\tLoss: 1.443255\n",
            "Train Epoch: 25 [16000/50000 (32%)]\tLoss: 1.448949\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tLoss: 1.484780\n",
            "Train Epoch: 25 [24000/50000 (48%)]\tLoss: 1.441999\n",
            "Train Epoch: 25 [28000/50000 (56%)]\tLoss: 1.464637\n",
            "Train Epoch: 25 [32000/50000 (64%)]\tLoss: 1.457081\n",
            "Train Epoch: 25 [36000/50000 (72%)]\tLoss: 1.429459\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tLoss: 1.427290\n",
            "Train Epoch: 25 [44000/50000 (88%)]\tLoss: 1.388662\n",
            "Train Epoch: 25 [48000/50000 (96%)]\tLoss: 1.428375\n",
            "\n",
            "Test set: Avg. loss: 1.2968, Accuracy: 5428/10000 (54.28%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.58%\n",
            "\n",
            "Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.533098\n",
            "Train Epoch: 26 [4000/50000 (8%)]\tLoss: 1.503008\n",
            "Train Epoch: 26 [8000/50000 (16%)]\tLoss: 1.530617\n",
            "Train Epoch: 26 [12000/50000 (24%)]\tLoss: 1.395699\n",
            "Train Epoch: 26 [16000/50000 (32%)]\tLoss: 1.427064\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tLoss: 1.361269\n",
            "Train Epoch: 26 [24000/50000 (48%)]\tLoss: 1.404247\n",
            "Train Epoch: 26 [28000/50000 (56%)]\tLoss: 1.465289\n",
            "Train Epoch: 26 [32000/50000 (64%)]\tLoss: 1.454941\n",
            "Train Epoch: 26 [36000/50000 (72%)]\tLoss: 1.350347\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tLoss: 1.397212\n",
            "Train Epoch: 26 [44000/50000 (88%)]\tLoss: 1.488857\n",
            "Train Epoch: 26 [48000/50000 (96%)]\tLoss: 1.470238\n",
            "\n",
            "Test set: Avg. loss: 1.2626, Accuracy: 5531/10000 (55.31%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.54%\n",
            "\n",
            "Train Epoch: 27 [0/50000 (0%)]\tLoss: 1.403612\n",
            "Train Epoch: 27 [4000/50000 (8%)]\tLoss: 1.373423\n",
            "Train Epoch: 27 [8000/50000 (16%)]\tLoss: 1.392165\n",
            "Train Epoch: 27 [12000/50000 (24%)]\tLoss: 1.409930\n",
            "Train Epoch: 27 [16000/50000 (32%)]\tLoss: 1.403085\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tLoss: 1.482723\n",
            "Train Epoch: 27 [24000/50000 (48%)]\tLoss: 1.395598\n",
            "Train Epoch: 27 [28000/50000 (56%)]\tLoss: 1.470979\n",
            "Train Epoch: 27 [32000/50000 (64%)]\tLoss: 1.448964\n",
            "Train Epoch: 27 [36000/50000 (72%)]\tLoss: 1.501217\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tLoss: 1.413589\n",
            "Train Epoch: 27 [44000/50000 (88%)]\tLoss: 1.441484\n",
            "Train Epoch: 27 [48000/50000 (96%)]\tLoss: 1.464389\n",
            "\n",
            "Test set: Avg. loss: 1.2530, Accuracy: 5635/10000 (56.35%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.54%\n",
            "\n",
            "Train Epoch: 28 [0/50000 (0%)]\tLoss: 1.496882\n",
            "Train Epoch: 28 [4000/50000 (8%)]\tLoss: 1.381564\n",
            "Train Epoch: 28 [8000/50000 (16%)]\tLoss: 1.373242\n",
            "Train Epoch: 28 [12000/50000 (24%)]\tLoss: 1.490056\n",
            "Train Epoch: 28 [16000/50000 (32%)]\tLoss: 1.406534\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tLoss: 1.452009\n",
            "Train Epoch: 28 [24000/50000 (48%)]\tLoss: 1.396437\n",
            "Train Epoch: 28 [28000/50000 (56%)]\tLoss: 1.453563\n",
            "Train Epoch: 28 [32000/50000 (64%)]\tLoss: 1.338340\n",
            "Train Epoch: 28 [36000/50000 (72%)]\tLoss: 1.485163\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tLoss: 1.363814\n",
            "Train Epoch: 28 [44000/50000 (88%)]\tLoss: 1.592337\n",
            "Train Epoch: 28 [48000/50000 (96%)]\tLoss: 1.425672\n",
            "\n",
            "Test set: Avg. loss: 1.2711, Accuracy: 5509/10000 (55.09%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.51%\n",
            "\n",
            "Train Epoch: 29 [0/50000 (0%)]\tLoss: 1.483230\n",
            "Train Epoch: 29 [4000/50000 (8%)]\tLoss: 1.369509\n",
            "Train Epoch: 29 [8000/50000 (16%)]\tLoss: 1.403909\n",
            "Train Epoch: 29 [12000/50000 (24%)]\tLoss: 1.546729\n",
            "Train Epoch: 29 [16000/50000 (32%)]\tLoss: 1.552716\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tLoss: 1.464369\n",
            "Train Epoch: 29 [24000/50000 (48%)]\tLoss: 1.482058\n",
            "Train Epoch: 29 [28000/50000 (56%)]\tLoss: 1.400294\n",
            "Train Epoch: 29 [32000/50000 (64%)]\tLoss: 1.394390\n",
            "Train Epoch: 29 [36000/50000 (72%)]\tLoss: 1.667700\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tLoss: 1.404193\n",
            "Train Epoch: 29 [44000/50000 (88%)]\tLoss: 1.463426\n",
            "Train Epoch: 29 [48000/50000 (96%)]\tLoss: 1.460495\n",
            "\n",
            "Test set: Avg. loss: 1.2498, Accuracy: 5736/10000 (57.36%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.52%\n",
            "\n",
            "Train Epoch: 30 [0/50000 (0%)]\tLoss: 1.351662\n",
            "Train Epoch: 30 [4000/50000 (8%)]\tLoss: 1.378229\n",
            "Train Epoch: 30 [8000/50000 (16%)]\tLoss: 1.536699\n",
            "Train Epoch: 30 [12000/50000 (24%)]\tLoss: 1.421229\n",
            "Train Epoch: 30 [16000/50000 (32%)]\tLoss: 1.452345\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tLoss: 1.491216\n",
            "Train Epoch: 30 [24000/50000 (48%)]\tLoss: 1.507642\n",
            "Train Epoch: 30 [28000/50000 (56%)]\tLoss: 1.441740\n",
            "Train Epoch: 30 [32000/50000 (64%)]\tLoss: 1.411033\n",
            "Train Epoch: 30 [36000/50000 (72%)]\tLoss: 1.372561\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tLoss: 1.398436\n",
            "Train Epoch: 30 [44000/50000 (88%)]\tLoss: 1.378748\n",
            "Train Epoch: 30 [48000/50000 (96%)]\tLoss: 1.469261\n",
            "\n",
            "Test set: Avg. loss: 1.2340, Accuracy: 5665/10000 (56.65%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.52%\n",
            "\n",
            "Train Epoch: 31 [0/50000 (0%)]\tLoss: 1.369818\n",
            "Train Epoch: 31 [4000/50000 (8%)]\tLoss: 1.427347\n",
            "Train Epoch: 31 [8000/50000 (16%)]\tLoss: 1.386193\n",
            "Train Epoch: 31 [12000/50000 (24%)]\tLoss: 1.350377\n",
            "Train Epoch: 31 [16000/50000 (32%)]\tLoss: 1.467998\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tLoss: 1.390163\n",
            "Train Epoch: 31 [24000/50000 (48%)]\tLoss: 1.487046\n",
            "Train Epoch: 31 [28000/50000 (56%)]\tLoss: 1.443432\n",
            "Train Epoch: 31 [32000/50000 (64%)]\tLoss: 1.481819\n",
            "Train Epoch: 31 [36000/50000 (72%)]\tLoss: 1.402181\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tLoss: 1.425985\n",
            "Train Epoch: 31 [44000/50000 (88%)]\tLoss: 1.403861\n",
            "Train Epoch: 31 [48000/50000 (96%)]\tLoss: 1.363416\n",
            "\n",
            "Test set: Avg. loss: 1.2237, Accuracy: 5686/10000 (56.86%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.50%\n",
            "\n",
            "Train Epoch: 32 [0/50000 (0%)]\tLoss: 1.362076\n",
            "Train Epoch: 32 [4000/50000 (8%)]\tLoss: 1.354333\n",
            "Train Epoch: 32 [8000/50000 (16%)]\tLoss: 1.330162\n",
            "Train Epoch: 32 [12000/50000 (24%)]\tLoss: 1.464106\n",
            "Train Epoch: 32 [16000/50000 (32%)]\tLoss: 1.340247\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tLoss: 1.459876\n",
            "Train Epoch: 32 [24000/50000 (48%)]\tLoss: 1.398103\n",
            "Train Epoch: 32 [28000/50000 (56%)]\tLoss: 1.373375\n",
            "Train Epoch: 32 [32000/50000 (64%)]\tLoss: 1.470981\n",
            "Train Epoch: 32 [36000/50000 (72%)]\tLoss: 1.355762\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tLoss: 1.285726\n",
            "Train Epoch: 32 [44000/50000 (88%)]\tLoss: 1.431598\n",
            "Train Epoch: 32 [48000/50000 (96%)]\tLoss: 1.364642\n",
            "\n",
            "Test set: Avg. loss: 1.2787, Accuracy: 5547/10000 (55.47%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.49%\n",
            "\n",
            "Train Epoch: 33 [0/50000 (0%)]\tLoss: 1.404174\n",
            "Train Epoch: 33 [4000/50000 (8%)]\tLoss: 1.415889\n",
            "Train Epoch: 33 [8000/50000 (16%)]\tLoss: 1.439425\n",
            "Train Epoch: 33 [12000/50000 (24%)]\tLoss: 1.420980\n",
            "Train Epoch: 33 [16000/50000 (32%)]\tLoss: 1.343260\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tLoss: 1.417778\n",
            "Train Epoch: 33 [24000/50000 (48%)]\tLoss: 1.274773\n",
            "Train Epoch: 33 [28000/50000 (56%)]\tLoss: 1.309914\n",
            "Train Epoch: 33 [32000/50000 (64%)]\tLoss: 1.376266\n",
            "Train Epoch: 33 [36000/50000 (72%)]\tLoss: 1.378070\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tLoss: 1.329326\n",
            "Train Epoch: 33 [44000/50000 (88%)]\tLoss: 1.336622\n",
            "Train Epoch: 33 [48000/50000 (96%)]\tLoss: 1.395103\n",
            "\n",
            "Test set: Avg. loss: 1.2281, Accuracy: 5631/10000 (56.31%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.46%\n",
            "\n",
            "Train Epoch: 34 [0/50000 (0%)]\tLoss: 1.406967\n",
            "Train Epoch: 34 [4000/50000 (8%)]\tLoss: 1.452853\n",
            "Train Epoch: 34 [8000/50000 (16%)]\tLoss: 1.334546\n",
            "Train Epoch: 34 [12000/50000 (24%)]\tLoss: 1.341772\n",
            "Train Epoch: 34 [16000/50000 (32%)]\tLoss: 1.380156\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tLoss: 1.331077\n",
            "Train Epoch: 34 [24000/50000 (48%)]\tLoss: 1.355381\n",
            "Train Epoch: 34 [28000/50000 (56%)]\tLoss: 1.379923\n",
            "Train Epoch: 34 [32000/50000 (64%)]\tLoss: 1.500578\n",
            "Train Epoch: 34 [36000/50000 (72%)]\tLoss: 1.465498\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tLoss: 1.371859\n",
            "Train Epoch: 34 [44000/50000 (88%)]\tLoss: 1.364219\n",
            "Train Epoch: 34 [48000/50000 (96%)]\tLoss: 1.440596\n",
            "\n",
            "Test set: Avg. loss: 1.2170, Accuracy: 5750/10000 (57.50%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.44%\n",
            "\n",
            "Train Epoch: 35 [0/50000 (0%)]\tLoss: 1.360957\n",
            "Train Epoch: 35 [4000/50000 (8%)]\tLoss: 1.392691\n",
            "Train Epoch: 35 [8000/50000 (16%)]\tLoss: 1.362100\n",
            "Train Epoch: 35 [12000/50000 (24%)]\tLoss: 1.420288\n",
            "Train Epoch: 35 [16000/50000 (32%)]\tLoss: 1.473942\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tLoss: 1.427088\n",
            "Train Epoch: 35 [24000/50000 (48%)]\tLoss: 1.331243\n",
            "Train Epoch: 35 [28000/50000 (56%)]\tLoss: 1.428648\n",
            "Train Epoch: 35 [32000/50000 (64%)]\tLoss: 1.329789\n",
            "Train Epoch: 35 [36000/50000 (72%)]\tLoss: 1.552272\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tLoss: 1.376805\n",
            "Train Epoch: 35 [44000/50000 (88%)]\tLoss: 1.358945\n",
            "Train Epoch: 35 [48000/50000 (96%)]\tLoss: 1.341981\n",
            "\n",
            "Test set: Avg. loss: 1.2038, Accuracy: 5792/10000 (57.92%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.41%\n",
            "\n",
            "Train Epoch: 36 [0/50000 (0%)]\tLoss: 1.298276\n",
            "Train Epoch: 36 [4000/50000 (8%)]\tLoss: 1.404891\n",
            "Train Epoch: 36 [8000/50000 (16%)]\tLoss: 1.337899\n",
            "Train Epoch: 36 [12000/50000 (24%)]\tLoss: 1.301595\n",
            "Train Epoch: 36 [16000/50000 (32%)]\tLoss: 1.278975\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tLoss: 1.335944\n",
            "Train Epoch: 36 [24000/50000 (48%)]\tLoss: 1.257875\n",
            "Train Epoch: 36 [28000/50000 (56%)]\tLoss: 1.406382\n",
            "Train Epoch: 36 [32000/50000 (64%)]\tLoss: 1.304847\n",
            "Train Epoch: 36 [36000/50000 (72%)]\tLoss: 1.489367\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tLoss: 1.298208\n",
            "Train Epoch: 36 [44000/50000 (88%)]\tLoss: 1.343244\n",
            "Train Epoch: 36 [48000/50000 (96%)]\tLoss: 1.318029\n",
            "\n",
            "Test set: Avg. loss: 1.2238, Accuracy: 5736/10000 (57.36%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.39%\n",
            "\n",
            "Train Epoch: 37 [0/50000 (0%)]\tLoss: 1.480953\n",
            "Train Epoch: 37 [4000/50000 (8%)]\tLoss: 1.398032\n",
            "Train Epoch: 37 [8000/50000 (16%)]\tLoss: 1.464357\n",
            "Train Epoch: 37 [12000/50000 (24%)]\tLoss: 1.323780\n",
            "Train Epoch: 37 [16000/50000 (32%)]\tLoss: 1.333232\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tLoss: 1.316714\n",
            "Train Epoch: 37 [24000/50000 (48%)]\tLoss: 1.394169\n",
            "Train Epoch: 37 [28000/50000 (56%)]\tLoss: 1.304857\n",
            "Train Epoch: 37 [32000/50000 (64%)]\tLoss: 1.414606\n",
            "Train Epoch: 37 [36000/50000 (72%)]\tLoss: 1.321661\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tLoss: 1.367718\n",
            "Train Epoch: 37 [44000/50000 (88%)]\tLoss: 1.244085\n",
            "Train Epoch: 37 [48000/50000 (96%)]\tLoss: 1.439556\n",
            "\n",
            "Test set: Avg. loss: 1.1968, Accuracy: 5818/10000 (58.18%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.40%\n",
            "\n",
            "Train Epoch: 38 [0/50000 (0%)]\tLoss: 1.382404\n",
            "Train Epoch: 38 [4000/50000 (8%)]\tLoss: 1.446238\n",
            "Train Epoch: 38 [8000/50000 (16%)]\tLoss: 1.288378\n",
            "Train Epoch: 38 [12000/50000 (24%)]\tLoss: 1.283739\n",
            "Train Epoch: 38 [16000/50000 (32%)]\tLoss: 1.327057\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tLoss: 1.355606\n",
            "Train Epoch: 38 [24000/50000 (48%)]\tLoss: 1.325195\n",
            "Train Epoch: 38 [28000/50000 (56%)]\tLoss: 1.399057\n",
            "Train Epoch: 38 [32000/50000 (64%)]\tLoss: 1.401953\n",
            "Train Epoch: 38 [36000/50000 (72%)]\tLoss: 1.417684\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tLoss: 1.401967\n",
            "Train Epoch: 38 [44000/50000 (88%)]\tLoss: 1.307501\n",
            "Train Epoch: 38 [48000/50000 (96%)]\tLoss: 1.339730\n",
            "\n",
            "Test set: Avg. loss: 1.2247, Accuracy: 5711/10000 (57.11%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.32%\n",
            "\n",
            "Train Epoch: 39 [0/50000 (0%)]\tLoss: 1.305237\n",
            "Train Epoch: 39 [4000/50000 (8%)]\tLoss: 1.429050\n",
            "Train Epoch: 39 [8000/50000 (16%)]\tLoss: 1.338453\n",
            "Train Epoch: 39 [12000/50000 (24%)]\tLoss: 1.374258\n",
            "Train Epoch: 39 [16000/50000 (32%)]\tLoss: 1.323271\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tLoss: 1.287203\n",
            "Train Epoch: 39 [24000/50000 (48%)]\tLoss: 1.316422\n",
            "Train Epoch: 39 [28000/50000 (56%)]\tLoss: 1.344385\n",
            "Train Epoch: 39 [32000/50000 (64%)]\tLoss: 1.300037\n",
            "Train Epoch: 39 [36000/50000 (72%)]\tLoss: 1.280879\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tLoss: 1.377859\n",
            "Train Epoch: 39 [44000/50000 (88%)]\tLoss: 1.358120\n",
            "Train Epoch: 39 [48000/50000 (96%)]\tLoss: 1.456303\n",
            "\n",
            "Test set: Avg. loss: 1.2464, Accuracy: 5628/10000 (56.28%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.30%\n",
            "\n",
            "Train Epoch: 40 [0/50000 (0%)]\tLoss: 1.386376\n",
            "Train Epoch: 40 [4000/50000 (8%)]\tLoss: 1.328155\n",
            "Train Epoch: 40 [8000/50000 (16%)]\tLoss: 1.406949\n",
            "Train Epoch: 40 [12000/50000 (24%)]\tLoss: 1.317236\n",
            "Train Epoch: 40 [16000/50000 (32%)]\tLoss: 1.264579\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tLoss: 1.383955\n",
            "Train Epoch: 40 [24000/50000 (48%)]\tLoss: 1.282459\n",
            "Train Epoch: 40 [28000/50000 (56%)]\tLoss: 1.428414\n",
            "Train Epoch: 40 [32000/50000 (64%)]\tLoss: 1.310526\n",
            "Train Epoch: 40 [36000/50000 (72%)]\tLoss: 1.293957\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tLoss: 1.252586\n",
            "Train Epoch: 40 [44000/50000 (88%)]\tLoss: 1.398418\n",
            "Train Epoch: 40 [48000/50000 (96%)]\tLoss: 1.344965\n",
            "\n",
            "Test set: Avg. loss: 1.2050, Accuracy: 5813/10000 (58.13%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.30%\n",
            "\n",
            "Train Epoch: 41 [0/50000 (0%)]\tLoss: 1.430626\n",
            "Train Epoch: 41 [4000/50000 (8%)]\tLoss: 1.365691\n",
            "Train Epoch: 41 [8000/50000 (16%)]\tLoss: 1.398094\n",
            "Train Epoch: 41 [12000/50000 (24%)]\tLoss: 1.289791\n",
            "Train Epoch: 41 [16000/50000 (32%)]\tLoss: 1.366028\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tLoss: 1.304872\n",
            "Train Epoch: 41 [24000/50000 (48%)]\tLoss: 1.287005\n",
            "Train Epoch: 41 [28000/50000 (56%)]\tLoss: 1.347559\n",
            "Train Epoch: 41 [32000/50000 (64%)]\tLoss: 1.287437\n",
            "Train Epoch: 41 [36000/50000 (72%)]\tLoss: 1.288631\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tLoss: 1.329784\n",
            "Train Epoch: 41 [44000/50000 (88%)]\tLoss: 1.288762\n",
            "Train Epoch: 41 [48000/50000 (96%)]\tLoss: 1.362382\n",
            "\n",
            "Test set: Avg. loss: 1.2004, Accuracy: 5810/10000 (58.10%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.31%\n",
            "\n",
            "Train Epoch: 42 [0/50000 (0%)]\tLoss: 1.459944\n",
            "Train Epoch: 42 [4000/50000 (8%)]\tLoss: 1.253511\n",
            "Train Epoch: 42 [8000/50000 (16%)]\tLoss: 1.338650\n",
            "Train Epoch: 42 [12000/50000 (24%)]\tLoss: 1.331106\n",
            "Train Epoch: 42 [16000/50000 (32%)]\tLoss: 1.309490\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tLoss: 1.278765\n",
            "Train Epoch: 42 [24000/50000 (48%)]\tLoss: 1.414580\n",
            "Train Epoch: 42 [28000/50000 (56%)]\tLoss: 1.283267\n",
            "Train Epoch: 42 [32000/50000 (64%)]\tLoss: 1.394265\n",
            "Train Epoch: 42 [36000/50000 (72%)]\tLoss: 1.436355\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tLoss: 1.336546\n",
            "Train Epoch: 42 [44000/50000 (88%)]\tLoss: 1.315776\n",
            "Train Epoch: 42 [48000/50000 (96%)]\tLoss: 1.365599\n",
            "\n",
            "Test set: Avg. loss: 1.2193, Accuracy: 5684/10000 (56.84%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.30%\n",
            "\n",
            "Train Epoch: 43 [0/50000 (0%)]\tLoss: 1.336067\n",
            "Train Epoch: 43 [4000/50000 (8%)]\tLoss: 1.294826\n",
            "Train Epoch: 43 [8000/50000 (16%)]\tLoss: 1.343134\n",
            "Train Epoch: 43 [12000/50000 (24%)]\tLoss: 1.319289\n",
            "Train Epoch: 43 [16000/50000 (32%)]\tLoss: 1.382331\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tLoss: 1.349167\n",
            "Train Epoch: 43 [24000/50000 (48%)]\tLoss: 1.255737\n",
            "Train Epoch: 43 [28000/50000 (56%)]\tLoss: 1.262434\n",
            "Train Epoch: 43 [32000/50000 (64%)]\tLoss: 1.353110\n",
            "Train Epoch: 43 [36000/50000 (72%)]\tLoss: 1.319337\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tLoss: 1.299795\n",
            "Train Epoch: 43 [44000/50000 (88%)]\tLoss: 1.282264\n",
            "Train Epoch: 43 [48000/50000 (96%)]\tLoss: 1.321527\n",
            "\n",
            "Test set: Avg. loss: 1.2391, Accuracy: 5638/10000 (56.38%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.30%\n",
            "\n",
            "Train Epoch: 44 [0/50000 (0%)]\tLoss: 1.276447\n",
            "Train Epoch: 44 [4000/50000 (8%)]\tLoss: 1.234834\n",
            "Train Epoch: 44 [8000/50000 (16%)]\tLoss: 1.389578\n",
            "Train Epoch: 44 [12000/50000 (24%)]\tLoss: 1.314891\n",
            "Train Epoch: 44 [16000/50000 (32%)]\tLoss: 1.303835\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tLoss: 1.327888\n",
            "Train Epoch: 44 [24000/50000 (48%)]\tLoss: 1.371367\n",
            "Train Epoch: 44 [28000/50000 (56%)]\tLoss: 1.295341\n",
            "Train Epoch: 44 [32000/50000 (64%)]\tLoss: 1.339825\n",
            "Train Epoch: 44 [36000/50000 (72%)]\tLoss: 1.239291\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tLoss: 1.268487\n",
            "Train Epoch: 44 [44000/50000 (88%)]\tLoss: 1.340195\n",
            "Train Epoch: 44 [48000/50000 (96%)]\tLoss: 1.301570\n",
            "\n",
            "Test set: Avg. loss: 1.1545, Accuracy: 5950/10000 (59.50%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.27%\n",
            "\n",
            "Train Epoch: 45 [0/50000 (0%)]\tLoss: 1.313264\n",
            "Train Epoch: 45 [4000/50000 (8%)]\tLoss: 1.315835\n",
            "Train Epoch: 45 [8000/50000 (16%)]\tLoss: 1.274228\n",
            "Train Epoch: 45 [12000/50000 (24%)]\tLoss: 1.332956\n",
            "Train Epoch: 45 [16000/50000 (32%)]\tLoss: 1.326139\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tLoss: 1.253626\n",
            "Train Epoch: 45 [24000/50000 (48%)]\tLoss: 1.367308\n",
            "Train Epoch: 45 [28000/50000 (56%)]\tLoss: 1.281904\n",
            "Train Epoch: 45 [32000/50000 (64%)]\tLoss: 1.279792\n",
            "Train Epoch: 45 [36000/50000 (72%)]\tLoss: 1.303093\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tLoss: 1.377041\n",
            "Train Epoch: 45 [44000/50000 (88%)]\tLoss: 1.248619\n",
            "Train Epoch: 45 [48000/50000 (96%)]\tLoss: 1.380678\n",
            "\n",
            "Test set: Avg. loss: 1.1647, Accuracy: 5906/10000 (59.06%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.29%\n",
            "\n",
            "Train Epoch: 46 [0/50000 (0%)]\tLoss: 1.339069\n",
            "Train Epoch: 46 [4000/50000 (8%)]\tLoss: 1.338679\n",
            "Train Epoch: 46 [8000/50000 (16%)]\tLoss: 1.251853\n",
            "Train Epoch: 46 [12000/50000 (24%)]\tLoss: 1.387766\n",
            "Train Epoch: 46 [16000/50000 (32%)]\tLoss: 1.377870\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tLoss: 1.246720\n",
            "Train Epoch: 46 [24000/50000 (48%)]\tLoss: 1.296242\n",
            "Train Epoch: 46 [28000/50000 (56%)]\tLoss: 1.355526\n",
            "Train Epoch: 46 [32000/50000 (64%)]\tLoss: 1.288466\n",
            "Train Epoch: 46 [36000/50000 (72%)]\tLoss: 1.298226\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tLoss: 1.307614\n",
            "Train Epoch: 46 [44000/50000 (88%)]\tLoss: 1.361159\n",
            "Train Epoch: 46 [48000/50000 (96%)]\tLoss: 1.227110\n",
            "\n",
            "Test set: Avg. loss: 1.1748, Accuracy: 5898/10000 (58.98%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.28%\n",
            "\n",
            "Train Epoch: 47 [0/50000 (0%)]\tLoss: 1.240547\n",
            "Train Epoch: 47 [4000/50000 (8%)]\tLoss: 1.267764\n",
            "Train Epoch: 47 [8000/50000 (16%)]\tLoss: 1.258480\n",
            "Train Epoch: 47 [12000/50000 (24%)]\tLoss: 1.326923\n",
            "Train Epoch: 47 [16000/50000 (32%)]\tLoss: 1.267890\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tLoss: 1.376783\n",
            "Train Epoch: 47 [24000/50000 (48%)]\tLoss: 1.230487\n",
            "Train Epoch: 47 [28000/50000 (56%)]\tLoss: 1.403421\n",
            "Train Epoch: 47 [32000/50000 (64%)]\tLoss: 1.266650\n",
            "Train Epoch: 47 [36000/50000 (72%)]\tLoss: 1.319995\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tLoss: 1.295993\n",
            "Train Epoch: 47 [44000/50000 (88%)]\tLoss: 1.354409\n",
            "Train Epoch: 47 [48000/50000 (96%)]\tLoss: 1.373885\n",
            "\n",
            "Test set: Avg. loss: 1.1937, Accuracy: 5831/10000 (58.31%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.29%\n",
            "\n",
            "Train Epoch: 48 [0/50000 (0%)]\tLoss: 1.331836\n",
            "Train Epoch: 48 [4000/50000 (8%)]\tLoss: 1.392972\n",
            "Train Epoch: 48 [8000/50000 (16%)]\tLoss: 1.364904\n",
            "Train Epoch: 48 [12000/50000 (24%)]\tLoss: 1.394764\n",
            "Train Epoch: 48 [16000/50000 (32%)]\tLoss: 1.299749\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tLoss: 1.332944\n",
            "Train Epoch: 48 [24000/50000 (48%)]\tLoss: 1.315009\n",
            "Train Epoch: 48 [28000/50000 (56%)]\tLoss: 1.294780\n",
            "Train Epoch: 48 [32000/50000 (64%)]\tLoss: 1.270499\n",
            "Train Epoch: 48 [36000/50000 (72%)]\tLoss: 1.325414\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tLoss: 1.238619\n",
            "Train Epoch: 48 [44000/50000 (88%)]\tLoss: 1.311366\n",
            "Train Epoch: 48 [48000/50000 (96%)]\tLoss: 1.360442\n",
            "\n",
            "Test set: Avg. loss: 1.1723, Accuracy: 5899/10000 (58.99%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.28%\n",
            "\n",
            "Train Epoch: 49 [0/50000 (0%)]\tLoss: 1.337297\n",
            "Train Epoch: 49 [4000/50000 (8%)]\tLoss: 1.259082\n",
            "Train Epoch: 49 [8000/50000 (16%)]\tLoss: 1.341410\n",
            "Train Epoch: 49 [12000/50000 (24%)]\tLoss: 1.340968\n",
            "Train Epoch: 49 [16000/50000 (32%)]\tLoss: 1.220611\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tLoss: 1.335091\n",
            "Train Epoch: 49 [24000/50000 (48%)]\tLoss: 1.300086\n",
            "Train Epoch: 49 [28000/50000 (56%)]\tLoss: 1.272030\n",
            "Train Epoch: 49 [32000/50000 (64%)]\tLoss: 1.304580\n",
            "Train Epoch: 49 [36000/50000 (72%)]\tLoss: 1.332353\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tLoss: 1.335870\n",
            "Train Epoch: 49 [44000/50000 (88%)]\tLoss: 1.420889\n",
            "Train Epoch: 49 [48000/50000 (96%)]\tLoss: 1.268497\n",
            "\n",
            "Test set: Avg. loss: 1.1476, Accuracy: 5970/10000 (59.70%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.28%\n",
            "\n",
            "Train Epoch: 50 [0/50000 (0%)]\tLoss: 1.242509\n",
            "Train Epoch: 50 [4000/50000 (8%)]\tLoss: 1.274713\n",
            "Train Epoch: 50 [8000/50000 (16%)]\tLoss: 1.319952\n",
            "Train Epoch: 50 [12000/50000 (24%)]\tLoss: 1.275825\n",
            "Train Epoch: 50 [16000/50000 (32%)]\tLoss: 1.173888\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tLoss: 1.261130\n",
            "Train Epoch: 50 [24000/50000 (48%)]\tLoss: 1.301407\n",
            "Train Epoch: 50 [28000/50000 (56%)]\tLoss: 1.433880\n",
            "Train Epoch: 50 [32000/50000 (64%)]\tLoss: 1.346088\n",
            "Train Epoch: 50 [36000/50000 (72%)]\tLoss: 1.266964\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tLoss: 1.287426\n",
            "Train Epoch: 50 [44000/50000 (88%)]\tLoss: 1.324168\n",
            "Train Epoch: 50 [48000/50000 (96%)]\tLoss: 1.347306\n",
            "\n",
            "Test set: Avg. loss: 1.1316, Accuracy: 6075/10000 (60.75%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.29%\n",
            "\n",
            "Elapsed time: 415.93 seconds\n",
            "\n",
            "R-CNN weight\n",
            "[3.3278652554149635, 2.9992581524862394, 2.7275956826564363, 2.560418778145801, 2.405780141473457, 2.349357936201113, 2.2877114526628106, 2.190539876916009, 2.12732610989792, 2.0865767394234513, 2.002465859341529, 1.9199222627394041, 1.9052942835947162, 1.8264076817787633, 1.783568599997909, 1.7658060538936593, 1.7517005025755705, 1.6811727459851378, 1.678038179025565, 1.6722914729330185, 1.6712466172798202, 1.6346766694181225, 1.6169141233138618, 1.6090777059149297, 1.605943138955357, 1.5787768919723777, 1.5437742275904554, 1.5354153823649241, 1.5113837023415178, 1.516085552780888, 1.5218322588734345, 1.5035472849425857, 1.485262311011737, 1.4648876257745025, 1.4392886622713097, 1.4115999874617313, 1.3922701578776842, 1.3959271526638561, 1.3170405508479033, 1.2966658656106689, 1.3034574273564137, 1.308681705622372, 1.3034574273564137, 1.3034574273564137, 1.2700220464542777, 1.2883070203851377, 1.2846500255989657, 1.2909191595181113, 1.2773360360266217, 1.2799481751596065, 1.285694881252153]\n",
            "R-CNN quaternion\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "R-CNN accuracy\n",
            "[10.0, 29.85, 31.98, 34.66, 37.73, 41.28, 41.81, 42.83, 44.97, 45.44, 47.14, 46.62, 47.91, 48.46, 50.43, 49.92, 51.26, 51.53, 52.96, 52.28, 52.2, 53.11, 54.32, 54.43, 55.74, 54.28, 55.31, 56.35, 55.09, 57.36, 56.65, 56.86, 55.47, 56.31, 57.5, 57.92, 57.36, 58.18, 57.11, 56.28, 58.13, 58.1, 56.84, 56.38, 59.5, 59.06, 58.98, 58.31, 58.99, 59.7, 60.75]\n",
            "Device used: cuda\n",
            "Network variant: CIFARQConvNetA\n",
            "Number of trainable parameters: 68232\n",
            "\n",
            "Retrieve CIFAR10 dataset...\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Start training from CIFAR10 training set to generate the model...\n",
            "Epochs: 50\n",
            "Learning rate: 0.002\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 2.3026, Accuracy: 1091/10000 (10.91%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 22.08%\n",
            "Quaternion sparsity: 2.94%\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.302572\n",
            "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 2.299759\n",
            "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2.284745\n",
            "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 2.282132\n",
            "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.236747\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 2.222217\n",
            "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 2.171008\n",
            "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 2.137985\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.131468\n",
            "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 2.139423\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 2.122981\n",
            "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 2.088496\n",
            "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 2.130439\n",
            "\n",
            "Test set: Avg. loss: 2.0102, Accuracy: 3327/10000 (33.27%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 3.07%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.054538\n",
            "Train Epoch: 2 [4000/50000 (8%)]\tLoss: 2.073117\n",
            "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 2.133820\n",
            "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 2.060634\n",
            "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 2.058554\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 2.036790\n",
            "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 2.082385\n",
            "Train Epoch: 2 [28000/50000 (56%)]\tLoss: 2.072082\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.082698\n",
            "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 1.981179\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 2.052501\n",
            "Train Epoch: 2 [44000/50000 (88%)]\tLoss: 2.001279\n",
            "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.970080\n",
            "\n",
            "Test set: Avg. loss: 1.8326, Accuracy: 3973/10000 (39.73%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 2.21%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.981977\n",
            "Train Epoch: 3 [4000/50000 (8%)]\tLoss: 2.012399\n",
            "Train Epoch: 3 [8000/50000 (16%)]\tLoss: 2.037743\n",
            "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 2.044001\n",
            "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.999700\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1.975595\n",
            "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.974967\n",
            "Train Epoch: 3 [28000/50000 (56%)]\tLoss: 1.966198\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.888126\n",
            "Train Epoch: 3 [36000/50000 (72%)]\tLoss: 1.868888\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1.976498\n",
            "Train Epoch: 3 [44000/50000 (88%)]\tLoss: 1.979198\n",
            "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.928412\n",
            "\n",
            "Test set: Avg. loss: 1.7153, Accuracy: 4386/10000 (43.86%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.82%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.915488\n",
            "Train Epoch: 4 [4000/50000 (8%)]\tLoss: 1.915181\n",
            "Train Epoch: 4 [8000/50000 (16%)]\tLoss: 1.948817\n",
            "Train Epoch: 4 [12000/50000 (24%)]\tLoss: 1.880860\n",
            "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1.924671\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1.911769\n",
            "Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1.878281\n",
            "Train Epoch: 4 [28000/50000 (56%)]\tLoss: 1.834077\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.876225\n",
            "Train Epoch: 4 [36000/50000 (72%)]\tLoss: 1.872958\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1.868308\n",
            "Train Epoch: 4 [44000/50000 (88%)]\tLoss: 1.871613\n",
            "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.817334\n",
            "\n",
            "Test set: Avg. loss: 1.6346, Accuracy: 4824/10000 (48.24%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.68%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.853237\n",
            "Train Epoch: 5 [4000/50000 (8%)]\tLoss: 1.834292\n",
            "Train Epoch: 5 [8000/50000 (16%)]\tLoss: 1.855958\n",
            "Train Epoch: 5 [12000/50000 (24%)]\tLoss: 1.849207\n",
            "Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1.854344\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 1.837719\n",
            "Train Epoch: 5 [24000/50000 (48%)]\tLoss: 1.876953\n",
            "Train Epoch: 5 [28000/50000 (56%)]\tLoss: 1.861367\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.875930\n",
            "Train Epoch: 5 [36000/50000 (72%)]\tLoss: 1.844172\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1.710659\n",
            "Train Epoch: 5 [44000/50000 (88%)]\tLoss: 1.829294\n",
            "Train Epoch: 5 [48000/50000 (96%)]\tLoss: 1.809608\n",
            "\n",
            "Test set: Avg. loss: 1.5480, Accuracy: 4994/10000 (49.94%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.47%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.761396\n",
            "Train Epoch: 6 [4000/50000 (8%)]\tLoss: 1.710831\n",
            "Train Epoch: 6 [8000/50000 (16%)]\tLoss: 1.776790\n",
            "Train Epoch: 6 [12000/50000 (24%)]\tLoss: 1.804384\n",
            "Train Epoch: 6 [16000/50000 (32%)]\tLoss: 1.821254\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tLoss: 1.751478\n",
            "Train Epoch: 6 [24000/50000 (48%)]\tLoss: 1.799069\n",
            "Train Epoch: 6 [28000/50000 (56%)]\tLoss: 1.802720\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.800359\n",
            "Train Epoch: 6 [36000/50000 (72%)]\tLoss: 1.785520\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1.858344\n",
            "Train Epoch: 6 [44000/50000 (88%)]\tLoss: 1.771574\n",
            "Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.772846\n",
            "\n",
            "Test set: Avg. loss: 1.5214, Accuracy: 5095/10000 (50.95%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.35%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.797554\n",
            "Train Epoch: 7 [4000/50000 (8%)]\tLoss: 1.759800\n",
            "Train Epoch: 7 [8000/50000 (16%)]\tLoss: 1.868452\n",
            "Train Epoch: 7 [12000/50000 (24%)]\tLoss: 1.732241\n",
            "Train Epoch: 7 [16000/50000 (32%)]\tLoss: 1.822914\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tLoss: 1.800632\n",
            "Train Epoch: 7 [24000/50000 (48%)]\tLoss: 1.768470\n",
            "Train Epoch: 7 [28000/50000 (56%)]\tLoss: 1.754942\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.741445\n",
            "Train Epoch: 7 [36000/50000 (72%)]\tLoss: 1.799131\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tLoss: 1.766929\n",
            "Train Epoch: 7 [44000/50000 (88%)]\tLoss: 1.761664\n",
            "Train Epoch: 7 [48000/50000 (96%)]\tLoss: 1.741571\n",
            "\n",
            "Test set: Avg. loss: 1.4391, Accuracy: 5409/10000 (54.09%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.27%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.727385\n",
            "Train Epoch: 8 [4000/50000 (8%)]\tLoss: 1.839099\n",
            "Train Epoch: 8 [8000/50000 (16%)]\tLoss: 1.684923\n",
            "Train Epoch: 8 [12000/50000 (24%)]\tLoss: 1.760714\n",
            "Train Epoch: 8 [16000/50000 (32%)]\tLoss: 1.731590\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tLoss: 1.742525\n",
            "Train Epoch: 8 [24000/50000 (48%)]\tLoss: 1.740110\n",
            "Train Epoch: 8 [28000/50000 (56%)]\tLoss: 1.719538\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.670151\n",
            "Train Epoch: 8 [36000/50000 (72%)]\tLoss: 1.862375\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tLoss: 1.752371\n",
            "Train Epoch: 8 [44000/50000 (88%)]\tLoss: 1.708951\n",
            "Train Epoch: 8 [48000/50000 (96%)]\tLoss: 1.663551\n",
            "\n",
            "Test set: Avg. loss: 1.4105, Accuracy: 5485/10000 (54.85%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.26%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.775527\n",
            "Train Epoch: 9 [4000/50000 (8%)]\tLoss: 1.750471\n",
            "Train Epoch: 9 [8000/50000 (16%)]\tLoss: 1.602839\n",
            "Train Epoch: 9 [12000/50000 (24%)]\tLoss: 1.759382\n",
            "Train Epoch: 9 [16000/50000 (32%)]\tLoss: 1.743799\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tLoss: 1.690311\n",
            "Train Epoch: 9 [24000/50000 (48%)]\tLoss: 1.676334\n",
            "Train Epoch: 9 [28000/50000 (56%)]\tLoss: 1.654782\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.723324\n",
            "Train Epoch: 9 [36000/50000 (72%)]\tLoss: 1.676969\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1.671127\n",
            "Train Epoch: 9 [44000/50000 (88%)]\tLoss: 1.811137\n",
            "Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1.682317\n",
            "\n",
            "Test set: Avg. loss: 1.4074, Accuracy: 5645/10000 (56.45%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.11%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.748531\n",
            "Train Epoch: 10 [4000/50000 (8%)]\tLoss: 1.641445\n",
            "Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1.707864\n",
            "Train Epoch: 10 [12000/50000 (24%)]\tLoss: 1.693144\n",
            "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1.750420\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tLoss: 1.609429\n",
            "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1.628002\n",
            "Train Epoch: 10 [28000/50000 (56%)]\tLoss: 1.832928\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.727971\n",
            "Train Epoch: 10 [36000/50000 (72%)]\tLoss: 1.697002\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1.663279\n",
            "Train Epoch: 10 [44000/50000 (88%)]\tLoss: 1.731274\n",
            "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1.706033\n",
            "\n",
            "Test set: Avg. loss: 1.3607, Accuracy: 5686/10000 (56.86%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.10%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.701369\n",
            "Train Epoch: 11 [4000/50000 (8%)]\tLoss: 1.760660\n",
            "Train Epoch: 11 [8000/50000 (16%)]\tLoss: 1.688470\n",
            "Train Epoch: 11 [12000/50000 (24%)]\tLoss: 1.672343\n",
            "Train Epoch: 11 [16000/50000 (32%)]\tLoss: 1.709702\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tLoss: 1.586796\n",
            "Train Epoch: 11 [24000/50000 (48%)]\tLoss: 1.670101\n",
            "Train Epoch: 11 [28000/50000 (56%)]\tLoss: 1.673689\n",
            "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 1.576043\n",
            "Train Epoch: 11 [36000/50000 (72%)]\tLoss: 1.669249\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tLoss: 1.628423\n",
            "Train Epoch: 11 [44000/50000 (88%)]\tLoss: 1.727818\n",
            "Train Epoch: 11 [48000/50000 (96%)]\tLoss: 1.782858\n",
            "\n",
            "Test set: Avg. loss: 1.3583, Accuracy: 5803/10000 (58.03%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.09%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.653150\n",
            "Train Epoch: 12 [4000/50000 (8%)]\tLoss: 1.742028\n",
            "Train Epoch: 12 [8000/50000 (16%)]\tLoss: 1.753446\n",
            "Train Epoch: 12 [12000/50000 (24%)]\tLoss: 1.677562\n",
            "Train Epoch: 12 [16000/50000 (32%)]\tLoss: 1.637168\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tLoss: 1.704233\n",
            "Train Epoch: 12 [24000/50000 (48%)]\tLoss: 1.713311\n",
            "Train Epoch: 12 [28000/50000 (56%)]\tLoss: 1.758475\n",
            "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1.647775\n",
            "Train Epoch: 12 [36000/50000 (72%)]\tLoss: 1.628699\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tLoss: 1.642520\n",
            "Train Epoch: 12 [44000/50000 (88%)]\tLoss: 1.554905\n",
            "Train Epoch: 12 [48000/50000 (96%)]\tLoss: 1.599058\n",
            "\n",
            "Test set: Avg. loss: 1.3198, Accuracy: 5946/10000 (59.46%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 1.01%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.608036\n",
            "Train Epoch: 13 [4000/50000 (8%)]\tLoss: 1.555430\n",
            "Train Epoch: 13 [8000/50000 (16%)]\tLoss: 1.552141\n",
            "Train Epoch: 13 [12000/50000 (24%)]\tLoss: 1.606813\n",
            "Train Epoch: 13 [16000/50000 (32%)]\tLoss: 1.658140\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tLoss: 1.638834\n",
            "Train Epoch: 13 [24000/50000 (48%)]\tLoss: 1.735870\n",
            "Train Epoch: 13 [28000/50000 (56%)]\tLoss: 1.654635\n",
            "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 1.681893\n",
            "Train Epoch: 13 [36000/50000 (72%)]\tLoss: 1.651691\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tLoss: 1.607047\n",
            "Train Epoch: 13 [44000/50000 (88%)]\tLoss: 1.639029\n",
            "Train Epoch: 13 [48000/50000 (96%)]\tLoss: 1.645465\n",
            "\n",
            "Test set: Avg. loss: 1.2642, Accuracy: 5994/10000 (59.94%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.99%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.669570\n",
            "Train Epoch: 14 [4000/50000 (8%)]\tLoss: 1.710378\n",
            "Train Epoch: 14 [8000/50000 (16%)]\tLoss: 1.637755\n",
            "Train Epoch: 14 [12000/50000 (24%)]\tLoss: 1.615990\n",
            "Train Epoch: 14 [16000/50000 (32%)]\tLoss: 1.616025\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tLoss: 1.656468\n",
            "Train Epoch: 14 [24000/50000 (48%)]\tLoss: 1.638456\n",
            "Train Epoch: 14 [28000/50000 (56%)]\tLoss: 1.651662\n",
            "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 1.643324\n",
            "Train Epoch: 14 [36000/50000 (72%)]\tLoss: 1.548377\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1.553917\n",
            "Train Epoch: 14 [44000/50000 (88%)]\tLoss: 1.678196\n",
            "Train Epoch: 14 [48000/50000 (96%)]\tLoss: 1.636744\n",
            "\n",
            "Test set: Avg. loss: 1.2780, Accuracy: 5989/10000 (59.89%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.98%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.563468\n",
            "Train Epoch: 15 [4000/50000 (8%)]\tLoss: 1.530330\n",
            "Train Epoch: 15 [8000/50000 (16%)]\tLoss: 1.685851\n",
            "Train Epoch: 15 [12000/50000 (24%)]\tLoss: 1.609046\n",
            "Train Epoch: 15 [16000/50000 (32%)]\tLoss: 1.539338\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tLoss: 1.639353\n",
            "Train Epoch: 15 [24000/50000 (48%)]\tLoss: 1.512810\n",
            "Train Epoch: 15 [28000/50000 (56%)]\tLoss: 1.657114\n",
            "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 1.581059\n",
            "Train Epoch: 15 [36000/50000 (72%)]\tLoss: 1.487470\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1.551730\n",
            "Train Epoch: 15 [44000/50000 (88%)]\tLoss: 1.654148\n",
            "Train Epoch: 15 [48000/50000 (96%)]\tLoss: 1.627594\n",
            "\n",
            "Test set: Avg. loss: 1.2612, Accuracy: 6014/10000 (60.14%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.97%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.594342\n",
            "Train Epoch: 16 [4000/50000 (8%)]\tLoss: 1.592426\n",
            "Train Epoch: 16 [8000/50000 (16%)]\tLoss: 1.510195\n",
            "Train Epoch: 16 [12000/50000 (24%)]\tLoss: 1.673755\n",
            "Train Epoch: 16 [16000/50000 (32%)]\tLoss: 1.603475\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tLoss: 1.610304\n",
            "Train Epoch: 16 [24000/50000 (48%)]\tLoss: 1.669352\n",
            "Train Epoch: 16 [28000/50000 (56%)]\tLoss: 1.571821\n",
            "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 1.618621\n",
            "Train Epoch: 16 [36000/50000 (72%)]\tLoss: 1.618569\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tLoss: 1.617900\n",
            "Train Epoch: 16 [44000/50000 (88%)]\tLoss: 1.587164\n",
            "Train Epoch: 16 [48000/50000 (96%)]\tLoss: 1.469751\n",
            "\n",
            "Test set: Avg. loss: 1.2443, Accuracy: 6136/10000 (61.36%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.92%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.554861\n",
            "Train Epoch: 17 [4000/50000 (8%)]\tLoss: 1.582783\n",
            "Train Epoch: 17 [8000/50000 (16%)]\tLoss: 1.593012\n",
            "Train Epoch: 17 [12000/50000 (24%)]\tLoss: 1.514124\n",
            "Train Epoch: 17 [16000/50000 (32%)]\tLoss: 1.620034\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tLoss: 1.574823\n",
            "Train Epoch: 17 [24000/50000 (48%)]\tLoss: 1.566461\n",
            "Train Epoch: 17 [28000/50000 (56%)]\tLoss: 1.568541\n",
            "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 1.607024\n",
            "Train Epoch: 17 [36000/50000 (72%)]\tLoss: 1.619908\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tLoss: 1.617759\n",
            "Train Epoch: 17 [44000/50000 (88%)]\tLoss: 1.642042\n",
            "Train Epoch: 17 [48000/50000 (96%)]\tLoss: 1.494526\n",
            "\n",
            "Test set: Avg. loss: 1.2264, Accuracy: 6139/10000 (61.39%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.88%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.649268\n",
            "Train Epoch: 18 [4000/50000 (8%)]\tLoss: 1.577785\n",
            "Train Epoch: 18 [8000/50000 (16%)]\tLoss: 1.511036\n",
            "Train Epoch: 18 [12000/50000 (24%)]\tLoss: 1.563256\n",
            "Train Epoch: 18 [16000/50000 (32%)]\tLoss: 1.513101\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tLoss: 1.581420\n",
            "Train Epoch: 18 [24000/50000 (48%)]\tLoss: 1.609859\n",
            "Train Epoch: 18 [28000/50000 (56%)]\tLoss: 1.547160\n",
            "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 1.640343\n",
            "Train Epoch: 18 [36000/50000 (72%)]\tLoss: 1.575786\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tLoss: 1.668954\n",
            "Train Epoch: 18 [44000/50000 (88%)]\tLoss: 1.588706\n",
            "Train Epoch: 18 [48000/50000 (96%)]\tLoss: 1.666853\n",
            "\n",
            "Test set: Avg. loss: 1.2328, Accuracy: 6211/10000 (62.11%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.85%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.601844\n",
            "Train Epoch: 19 [4000/50000 (8%)]\tLoss: 1.490150\n",
            "Train Epoch: 19 [8000/50000 (16%)]\tLoss: 1.583177\n",
            "Train Epoch: 19 [12000/50000 (24%)]\tLoss: 1.662629\n",
            "Train Epoch: 19 [16000/50000 (32%)]\tLoss: 1.521519\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tLoss: 1.674702\n",
            "Train Epoch: 19 [24000/50000 (48%)]\tLoss: 1.548369\n",
            "Train Epoch: 19 [28000/50000 (56%)]\tLoss: 1.394224\n",
            "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 1.446335\n",
            "Train Epoch: 19 [36000/50000 (72%)]\tLoss: 1.613756\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tLoss: 1.539652\n",
            "Train Epoch: 19 [44000/50000 (88%)]\tLoss: 1.471174\n",
            "Train Epoch: 19 [48000/50000 (96%)]\tLoss: 1.634582\n",
            "\n",
            "Test set: Avg. loss: 1.2214, Accuracy: 6140/10000 (61.40%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.84%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.521368\n",
            "Train Epoch: 20 [4000/50000 (8%)]\tLoss: 1.638244\n",
            "Train Epoch: 20 [8000/50000 (16%)]\tLoss: 1.601018\n",
            "Train Epoch: 20 [12000/50000 (24%)]\tLoss: 1.572319\n",
            "Train Epoch: 20 [16000/50000 (32%)]\tLoss: 1.548227\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tLoss: 1.487752\n",
            "Train Epoch: 20 [24000/50000 (48%)]\tLoss: 1.496274\n",
            "Train Epoch: 20 [28000/50000 (56%)]\tLoss: 1.607131\n",
            "Train Epoch: 20 [32000/50000 (64%)]\tLoss: 1.542962\n",
            "Train Epoch: 20 [36000/50000 (72%)]\tLoss: 1.565973\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tLoss: 1.535150\n",
            "Train Epoch: 20 [44000/50000 (88%)]\tLoss: 1.571928\n",
            "Train Epoch: 20 [48000/50000 (96%)]\tLoss: 1.642362\n",
            "\n",
            "Test set: Avg. loss: 1.1980, Accuracy: 6315/10000 (63.15%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.80%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.587889\n",
            "Train Epoch: 21 [4000/50000 (8%)]\tLoss: 1.633376\n",
            "Train Epoch: 21 [8000/50000 (16%)]\tLoss: 1.528512\n",
            "Train Epoch: 21 [12000/50000 (24%)]\tLoss: 1.549425\n",
            "Train Epoch: 21 [16000/50000 (32%)]\tLoss: 1.484256\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tLoss: 1.530972\n",
            "Train Epoch: 21 [24000/50000 (48%)]\tLoss: 1.559211\n",
            "Train Epoch: 21 [28000/50000 (56%)]\tLoss: 1.532833\n",
            "Train Epoch: 21 [32000/50000 (64%)]\tLoss: 1.533918\n",
            "Train Epoch: 21 [36000/50000 (72%)]\tLoss: 1.624428\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tLoss: 1.494292\n",
            "Train Epoch: 21 [44000/50000 (88%)]\tLoss: 1.598972\n",
            "Train Epoch: 21 [48000/50000 (96%)]\tLoss: 1.581675\n",
            "\n",
            "Test set: Avg. loss: 1.1902, Accuracy: 6258/10000 (62.58%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.79%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLoss: 1.582334\n",
            "Train Epoch: 22 [4000/50000 (8%)]\tLoss: 1.589881\n",
            "Train Epoch: 22 [8000/50000 (16%)]\tLoss: 1.609102\n",
            "Train Epoch: 22 [12000/50000 (24%)]\tLoss: 1.490014\n",
            "Train Epoch: 22 [16000/50000 (32%)]\tLoss: 1.556660\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tLoss: 1.603406\n",
            "Train Epoch: 22 [24000/50000 (48%)]\tLoss: 1.575086\n",
            "Train Epoch: 22 [28000/50000 (56%)]\tLoss: 1.444672\n",
            "Train Epoch: 22 [32000/50000 (64%)]\tLoss: 1.580287\n",
            "Train Epoch: 22 [36000/50000 (72%)]\tLoss: 1.634280\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tLoss: 1.673117\n",
            "Train Epoch: 22 [44000/50000 (88%)]\tLoss: 1.628834\n",
            "Train Epoch: 22 [48000/50000 (96%)]\tLoss: 1.538126\n",
            "\n",
            "Test set: Avg. loss: 1.1810, Accuracy: 6356/10000 (63.56%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.80%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.541723\n",
            "Train Epoch: 23 [4000/50000 (8%)]\tLoss: 1.602390\n",
            "Train Epoch: 23 [8000/50000 (16%)]\tLoss: 1.517073\n",
            "Train Epoch: 23 [12000/50000 (24%)]\tLoss: 1.485146\n",
            "Train Epoch: 23 [16000/50000 (32%)]\tLoss: 1.494521\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tLoss: 1.626694\n",
            "Train Epoch: 23 [24000/50000 (48%)]\tLoss: 1.515847\n",
            "Train Epoch: 23 [28000/50000 (56%)]\tLoss: 1.615284\n",
            "Train Epoch: 23 [32000/50000 (64%)]\tLoss: 1.503601\n",
            "Train Epoch: 23 [36000/50000 (72%)]\tLoss: 1.471480\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tLoss: 1.602080\n",
            "Train Epoch: 23 [44000/50000 (88%)]\tLoss: 1.619836\n",
            "Train Epoch: 23 [48000/50000 (96%)]\tLoss: 1.721309\n",
            "\n",
            "Test set: Avg. loss: 1.1913, Accuracy: 6358/10000 (63.58%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.80%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 24 [0/50000 (0%)]\tLoss: 1.471930\n",
            "Train Epoch: 24 [4000/50000 (8%)]\tLoss: 1.531887\n",
            "Train Epoch: 24 [8000/50000 (16%)]\tLoss: 1.550657\n",
            "Train Epoch: 24 [12000/50000 (24%)]\tLoss: 1.517353\n",
            "Train Epoch: 24 [16000/50000 (32%)]\tLoss: 1.515141\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tLoss: 1.547058\n",
            "Train Epoch: 24 [24000/50000 (48%)]\tLoss: 1.513331\n",
            "Train Epoch: 24 [28000/50000 (56%)]\tLoss: 1.615134\n",
            "Train Epoch: 24 [32000/50000 (64%)]\tLoss: 1.509535\n",
            "Train Epoch: 24 [36000/50000 (72%)]\tLoss: 1.583718\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tLoss: 1.487246\n",
            "Train Epoch: 24 [44000/50000 (88%)]\tLoss: 1.410668\n",
            "Train Epoch: 24 [48000/50000 (96%)]\tLoss: 1.523850\n",
            "\n",
            "Test set: Avg. loss: 1.1517, Accuracy: 6393/10000 (63.93%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.76%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 25 [0/50000 (0%)]\tLoss: 1.541405\n",
            "Train Epoch: 25 [4000/50000 (8%)]\tLoss: 1.526478\n",
            "Train Epoch: 25 [8000/50000 (16%)]\tLoss: 1.523066\n",
            "Train Epoch: 25 [12000/50000 (24%)]\tLoss: 1.386008\n",
            "Train Epoch: 25 [16000/50000 (32%)]\tLoss: 1.522592\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tLoss: 1.559772\n",
            "Train Epoch: 25 [24000/50000 (48%)]\tLoss: 1.513488\n",
            "Train Epoch: 25 [28000/50000 (56%)]\tLoss: 1.552412\n",
            "Train Epoch: 25 [32000/50000 (64%)]\tLoss: 1.500760\n",
            "Train Epoch: 25 [36000/50000 (72%)]\tLoss: 1.566309\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tLoss: 1.435925\n",
            "Train Epoch: 25 [44000/50000 (88%)]\tLoss: 1.526462\n",
            "Train Epoch: 25 [48000/50000 (96%)]\tLoss: 1.501320\n",
            "\n",
            "Test set: Avg. loss: 1.1537, Accuracy: 6469/10000 (64.69%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.72%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 26 [0/50000 (0%)]\tLoss: 1.436855\n",
            "Train Epoch: 26 [4000/50000 (8%)]\tLoss: 1.536245\n",
            "Train Epoch: 26 [8000/50000 (16%)]\tLoss: 1.477820\n",
            "Train Epoch: 26 [12000/50000 (24%)]\tLoss: 1.543463\n",
            "Train Epoch: 26 [16000/50000 (32%)]\tLoss: 1.553219\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tLoss: 1.614114\n",
            "Train Epoch: 26 [24000/50000 (48%)]\tLoss: 1.572338\n",
            "Train Epoch: 26 [28000/50000 (56%)]\tLoss: 1.579245\n",
            "Train Epoch: 26 [32000/50000 (64%)]\tLoss: 1.510711\n",
            "Train Epoch: 26 [36000/50000 (72%)]\tLoss: 1.480050\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tLoss: 1.526764\n",
            "Train Epoch: 26 [44000/50000 (88%)]\tLoss: 1.510781\n",
            "Train Epoch: 26 [48000/50000 (96%)]\tLoss: 1.633078\n",
            "\n",
            "Test set: Avg. loss: 1.1469, Accuracy: 6522/10000 (65.22%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.72%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 27 [0/50000 (0%)]\tLoss: 1.583417\n",
            "Train Epoch: 27 [4000/50000 (8%)]\tLoss: 1.455921\n",
            "Train Epoch: 27 [8000/50000 (16%)]\tLoss: 1.558528\n",
            "Train Epoch: 27 [12000/50000 (24%)]\tLoss: 1.496431\n",
            "Train Epoch: 27 [16000/50000 (32%)]\tLoss: 1.444759\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tLoss: 1.606154\n",
            "Train Epoch: 27 [24000/50000 (48%)]\tLoss: 1.470203\n",
            "Train Epoch: 27 [28000/50000 (56%)]\tLoss: 1.539581\n",
            "Train Epoch: 27 [32000/50000 (64%)]\tLoss: 1.498653\n",
            "Train Epoch: 27 [36000/50000 (72%)]\tLoss: 1.413921\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tLoss: 1.550883\n",
            "Train Epoch: 27 [44000/50000 (88%)]\tLoss: 1.404046\n",
            "Train Epoch: 27 [48000/50000 (96%)]\tLoss: 1.479792\n",
            "\n",
            "Test set: Avg. loss: 1.1370, Accuracy: 6449/10000 (64.49%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.73%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 28 [0/50000 (0%)]\tLoss: 1.428271\n",
            "Train Epoch: 28 [4000/50000 (8%)]\tLoss: 1.547611\n",
            "Train Epoch: 28 [8000/50000 (16%)]\tLoss: 1.567257\n",
            "Train Epoch: 28 [12000/50000 (24%)]\tLoss: 1.510206\n",
            "Train Epoch: 28 [16000/50000 (32%)]\tLoss: 1.600783\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tLoss: 1.483435\n",
            "Train Epoch: 28 [24000/50000 (48%)]\tLoss: 1.487023\n",
            "Train Epoch: 28 [28000/50000 (56%)]\tLoss: 1.531407\n",
            "Train Epoch: 28 [32000/50000 (64%)]\tLoss: 1.543195\n",
            "Train Epoch: 28 [36000/50000 (72%)]\tLoss: 1.603979\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tLoss: 1.521990\n",
            "Train Epoch: 28 [44000/50000 (88%)]\tLoss: 1.505996\n",
            "Train Epoch: 28 [48000/50000 (96%)]\tLoss: 1.571348\n",
            "\n",
            "Test set: Avg. loss: 1.1451, Accuracy: 6415/10000 (64.15%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.78%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 29 [0/50000 (0%)]\tLoss: 1.466288\n",
            "Train Epoch: 29 [4000/50000 (8%)]\tLoss: 1.406875\n",
            "Train Epoch: 29 [8000/50000 (16%)]\tLoss: 1.448567\n",
            "Train Epoch: 29 [12000/50000 (24%)]\tLoss: 1.466461\n",
            "Train Epoch: 29 [16000/50000 (32%)]\tLoss: 1.451983\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tLoss: 1.555119\n",
            "Train Epoch: 29 [24000/50000 (48%)]\tLoss: 1.476212\n",
            "Train Epoch: 29 [28000/50000 (56%)]\tLoss: 1.524747\n",
            "Train Epoch: 29 [32000/50000 (64%)]\tLoss: 1.411133\n",
            "Train Epoch: 29 [36000/50000 (72%)]\tLoss: 1.579049\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tLoss: 1.572275\n",
            "Train Epoch: 29 [44000/50000 (88%)]\tLoss: 1.628010\n",
            "Train Epoch: 29 [48000/50000 (96%)]\tLoss: 1.409838\n",
            "\n",
            "Test set: Avg. loss: 1.1504, Accuracy: 6341/10000 (63.41%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.72%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 30 [0/50000 (0%)]\tLoss: 1.500769\n",
            "Train Epoch: 30 [4000/50000 (8%)]\tLoss: 1.424583\n",
            "Train Epoch: 30 [8000/50000 (16%)]\tLoss: 1.498314\n",
            "Train Epoch: 30 [12000/50000 (24%)]\tLoss: 1.450440\n",
            "Train Epoch: 30 [16000/50000 (32%)]\tLoss: 1.614155\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tLoss: 1.524918\n",
            "Train Epoch: 30 [24000/50000 (48%)]\tLoss: 1.480104\n",
            "Train Epoch: 30 [28000/50000 (56%)]\tLoss: 1.495745\n",
            "Train Epoch: 30 [32000/50000 (64%)]\tLoss: 1.510415\n",
            "Train Epoch: 30 [36000/50000 (72%)]\tLoss: 1.507685\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tLoss: 1.405976\n",
            "Train Epoch: 30 [44000/50000 (88%)]\tLoss: 1.615080\n",
            "Train Epoch: 30 [48000/50000 (96%)]\tLoss: 1.423761\n",
            "\n",
            "Test set: Avg. loss: 1.1250, Accuracy: 6538/10000 (65.38%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.73%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 31 [0/50000 (0%)]\tLoss: 1.596611\n",
            "Train Epoch: 31 [4000/50000 (8%)]\tLoss: 1.540335\n",
            "Train Epoch: 31 [8000/50000 (16%)]\tLoss: 1.517149\n",
            "Train Epoch: 31 [12000/50000 (24%)]\tLoss: 1.408888\n",
            "Train Epoch: 31 [16000/50000 (32%)]\tLoss: 1.537680\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tLoss: 1.480155\n",
            "Train Epoch: 31 [24000/50000 (48%)]\tLoss: 1.507733\n",
            "Train Epoch: 31 [28000/50000 (56%)]\tLoss: 1.573826\n",
            "Train Epoch: 31 [32000/50000 (64%)]\tLoss: 1.465673\n",
            "Train Epoch: 31 [36000/50000 (72%)]\tLoss: 1.605352\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tLoss: 1.569815\n",
            "Train Epoch: 31 [44000/50000 (88%)]\tLoss: 1.511607\n",
            "Train Epoch: 31 [48000/50000 (96%)]\tLoss: 1.511990\n",
            "\n",
            "Test set: Avg. loss: 1.1176, Accuracy: 6487/10000 (64.87%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.71%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 32 [0/50000 (0%)]\tLoss: 1.451220\n",
            "Train Epoch: 32 [4000/50000 (8%)]\tLoss: 1.464954\n",
            "Train Epoch: 32 [8000/50000 (16%)]\tLoss: 1.599231\n",
            "Train Epoch: 32 [12000/50000 (24%)]\tLoss: 1.598804\n",
            "Train Epoch: 32 [16000/50000 (32%)]\tLoss: 1.549285\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tLoss: 1.524184\n",
            "Train Epoch: 32 [24000/50000 (48%)]\tLoss: 1.662088\n",
            "Train Epoch: 32 [28000/50000 (56%)]\tLoss: 1.526508\n",
            "Train Epoch: 32 [32000/50000 (64%)]\tLoss: 1.573983\n",
            "Train Epoch: 32 [36000/50000 (72%)]\tLoss: 1.426160\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tLoss: 1.474607\n",
            "Train Epoch: 32 [44000/50000 (88%)]\tLoss: 1.368252\n",
            "Train Epoch: 32 [48000/50000 (96%)]\tLoss: 1.414428\n",
            "\n",
            "Test set: Avg. loss: 1.0889, Accuracy: 6604/10000 (66.04%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.75%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 33 [0/50000 (0%)]\tLoss: 1.485455\n",
            "Train Epoch: 33 [4000/50000 (8%)]\tLoss: 1.534691\n",
            "Train Epoch: 33 [8000/50000 (16%)]\tLoss: 1.494302\n",
            "Train Epoch: 33 [12000/50000 (24%)]\tLoss: 1.501469\n",
            "Train Epoch: 33 [16000/50000 (32%)]\tLoss: 1.522591\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tLoss: 1.524417\n",
            "Train Epoch: 33 [24000/50000 (48%)]\tLoss: 1.525765\n",
            "Train Epoch: 33 [28000/50000 (56%)]\tLoss: 1.458755\n",
            "Train Epoch: 33 [32000/50000 (64%)]\tLoss: 1.546254\n",
            "Train Epoch: 33 [36000/50000 (72%)]\tLoss: 1.497864\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tLoss: 1.552482\n",
            "Train Epoch: 33 [44000/50000 (88%)]\tLoss: 1.554713\n",
            "Train Epoch: 33 [48000/50000 (96%)]\tLoss: 1.413077\n",
            "\n",
            "Test set: Avg. loss: 1.1013, Accuracy: 6585/10000 (65.85%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.66%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 34 [0/50000 (0%)]\tLoss: 1.449152\n",
            "Train Epoch: 34 [4000/50000 (8%)]\tLoss: 1.583599\n",
            "Train Epoch: 34 [8000/50000 (16%)]\tLoss: 1.401846\n",
            "Train Epoch: 34 [12000/50000 (24%)]\tLoss: 1.551815\n",
            "Train Epoch: 34 [16000/50000 (32%)]\tLoss: 1.529279\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tLoss: 1.553391\n",
            "Train Epoch: 34 [24000/50000 (48%)]\tLoss: 1.549108\n",
            "Train Epoch: 34 [28000/50000 (56%)]\tLoss: 1.436877\n",
            "Train Epoch: 34 [32000/50000 (64%)]\tLoss: 1.518374\n",
            "Train Epoch: 34 [36000/50000 (72%)]\tLoss: 1.589859\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tLoss: 1.487385\n",
            "Train Epoch: 34 [44000/50000 (88%)]\tLoss: 1.496439\n",
            "Train Epoch: 34 [48000/50000 (96%)]\tLoss: 1.602886\n",
            "\n",
            "Test set: Avg. loss: 1.1219, Accuracy: 6497/10000 (64.97%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.76%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 35 [0/50000 (0%)]\tLoss: 1.433928\n",
            "Train Epoch: 35 [4000/50000 (8%)]\tLoss: 1.445207\n",
            "Train Epoch: 35 [8000/50000 (16%)]\tLoss: 1.602022\n",
            "Train Epoch: 35 [12000/50000 (24%)]\tLoss: 1.399974\n",
            "Train Epoch: 35 [16000/50000 (32%)]\tLoss: 1.438005\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tLoss: 1.439669\n",
            "Train Epoch: 35 [24000/50000 (48%)]\tLoss: 1.513906\n",
            "Train Epoch: 35 [28000/50000 (56%)]\tLoss: 1.449405\n",
            "Train Epoch: 35 [32000/50000 (64%)]\tLoss: 1.696419\n",
            "Train Epoch: 35 [36000/50000 (72%)]\tLoss: 1.498941\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tLoss: 1.474826\n",
            "Train Epoch: 35 [44000/50000 (88%)]\tLoss: 1.437404\n",
            "Train Epoch: 35 [48000/50000 (96%)]\tLoss: 1.412486\n",
            "\n",
            "Test set: Avg. loss: 1.1068, Accuracy: 6598/10000 (65.98%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.67%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 36 [0/50000 (0%)]\tLoss: 1.517210\n",
            "Train Epoch: 36 [4000/50000 (8%)]\tLoss: 1.364523\n",
            "Train Epoch: 36 [8000/50000 (16%)]\tLoss: 1.528008\n",
            "Train Epoch: 36 [12000/50000 (24%)]\tLoss: 1.460067\n",
            "Train Epoch: 36 [16000/50000 (32%)]\tLoss: 1.505189\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tLoss: 1.465693\n",
            "Train Epoch: 36 [24000/50000 (48%)]\tLoss: 1.365482\n",
            "Train Epoch: 36 [28000/50000 (56%)]\tLoss: 1.587026\n",
            "Train Epoch: 36 [32000/50000 (64%)]\tLoss: 1.382068\n",
            "Train Epoch: 36 [36000/50000 (72%)]\tLoss: 1.429551\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tLoss: 1.468592\n",
            "Train Epoch: 36 [44000/50000 (88%)]\tLoss: 1.494442\n",
            "Train Epoch: 36 [48000/50000 (96%)]\tLoss: 1.553884\n",
            "\n",
            "Test set: Avg. loss: 1.0777, Accuracy: 6572/10000 (65.72%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.70%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 37 [0/50000 (0%)]\tLoss: 1.468142\n",
            "Train Epoch: 37 [4000/50000 (8%)]\tLoss: 1.436351\n",
            "Train Epoch: 37 [8000/50000 (16%)]\tLoss: 1.445404\n",
            "Train Epoch: 37 [12000/50000 (24%)]\tLoss: 1.434414\n",
            "Train Epoch: 37 [16000/50000 (32%)]\tLoss: 1.544006\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tLoss: 1.597486\n",
            "Train Epoch: 37 [24000/50000 (48%)]\tLoss: 1.522602\n",
            "Train Epoch: 37 [28000/50000 (56%)]\tLoss: 1.411384\n",
            "Train Epoch: 37 [32000/50000 (64%)]\tLoss: 1.508915\n",
            "Train Epoch: 37 [36000/50000 (72%)]\tLoss: 1.479118\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tLoss: 1.557243\n",
            "Train Epoch: 37 [44000/50000 (88%)]\tLoss: 1.460404\n",
            "Train Epoch: 37 [48000/50000 (96%)]\tLoss: 1.507313\n",
            "\n",
            "Test set: Avg. loss: 1.0932, Accuracy: 6580/10000 (65.80%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.72%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 38 [0/50000 (0%)]\tLoss: 1.387074\n",
            "Train Epoch: 38 [4000/50000 (8%)]\tLoss: 1.399308\n",
            "Train Epoch: 38 [8000/50000 (16%)]\tLoss: 1.461331\n",
            "Train Epoch: 38 [12000/50000 (24%)]\tLoss: 1.589083\n",
            "Train Epoch: 38 [16000/50000 (32%)]\tLoss: 1.442731\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tLoss: 1.424080\n",
            "Train Epoch: 38 [24000/50000 (48%)]\tLoss: 1.435549\n",
            "Train Epoch: 38 [28000/50000 (56%)]\tLoss: 1.537737\n",
            "Train Epoch: 38 [32000/50000 (64%)]\tLoss: 1.436321\n",
            "Train Epoch: 38 [36000/50000 (72%)]\tLoss: 1.523935\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tLoss: 1.434597\n",
            "Train Epoch: 38 [44000/50000 (88%)]\tLoss: 1.469570\n",
            "Train Epoch: 38 [48000/50000 (96%)]\tLoss: 1.562796\n",
            "\n",
            "Test set: Avg. loss: 1.0914, Accuracy: 6656/10000 (66.56%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.68%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 39 [0/50000 (0%)]\tLoss: 1.404865\n",
            "Train Epoch: 39 [4000/50000 (8%)]\tLoss: 1.488643\n",
            "Train Epoch: 39 [8000/50000 (16%)]\tLoss: 1.553980\n",
            "Train Epoch: 39 [12000/50000 (24%)]\tLoss: 1.494491\n",
            "Train Epoch: 39 [16000/50000 (32%)]\tLoss: 1.342422\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tLoss: 1.456665\n",
            "Train Epoch: 39 [24000/50000 (48%)]\tLoss: 1.534851\n",
            "Train Epoch: 39 [28000/50000 (56%)]\tLoss: 1.469578\n",
            "Train Epoch: 39 [32000/50000 (64%)]\tLoss: 1.525952\n",
            "Train Epoch: 39 [36000/50000 (72%)]\tLoss: 1.440552\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tLoss: 1.506655\n",
            "Train Epoch: 39 [44000/50000 (88%)]\tLoss: 1.542341\n",
            "Train Epoch: 39 [48000/50000 (96%)]\tLoss: 1.441147\n",
            "\n",
            "Test set: Avg. loss: 1.0847, Accuracy: 6699/10000 (66.99%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.68%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 40 [0/50000 (0%)]\tLoss: 1.398785\n",
            "Train Epoch: 40 [4000/50000 (8%)]\tLoss: 1.425365\n",
            "Train Epoch: 40 [8000/50000 (16%)]\tLoss: 1.362478\n",
            "Train Epoch: 40 [12000/50000 (24%)]\tLoss: 1.441563\n",
            "Train Epoch: 40 [16000/50000 (32%)]\tLoss: 1.570871\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tLoss: 1.543760\n",
            "Train Epoch: 40 [24000/50000 (48%)]\tLoss: 1.328872\n",
            "Train Epoch: 40 [28000/50000 (56%)]\tLoss: 1.569846\n",
            "Train Epoch: 40 [32000/50000 (64%)]\tLoss: 1.352396\n",
            "Train Epoch: 40 [36000/50000 (72%)]\tLoss: 1.547028\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tLoss: 1.380228\n",
            "Train Epoch: 40 [44000/50000 (88%)]\tLoss: 1.479295\n",
            "Train Epoch: 40 [48000/50000 (96%)]\tLoss: 1.535331\n",
            "\n",
            "Test set: Avg. loss: 1.0756, Accuracy: 6704/10000 (67.04%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.69%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 41 [0/50000 (0%)]\tLoss: 1.513697\n",
            "Train Epoch: 41 [4000/50000 (8%)]\tLoss: 1.377150\n",
            "Train Epoch: 41 [8000/50000 (16%)]\tLoss: 1.513278\n",
            "Train Epoch: 41 [12000/50000 (24%)]\tLoss: 1.522085\n",
            "Train Epoch: 41 [16000/50000 (32%)]\tLoss: 1.415102\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tLoss: 1.516360\n",
            "Train Epoch: 41 [24000/50000 (48%)]\tLoss: 1.369974\n",
            "Train Epoch: 41 [28000/50000 (56%)]\tLoss: 1.349025\n",
            "Train Epoch: 41 [32000/50000 (64%)]\tLoss: 1.532740\n",
            "Train Epoch: 41 [36000/50000 (72%)]\tLoss: 1.473017\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tLoss: 1.375981\n",
            "Train Epoch: 41 [44000/50000 (88%)]\tLoss: 1.421446\n",
            "Train Epoch: 41 [48000/50000 (96%)]\tLoss: 1.446728\n",
            "\n",
            "Test set: Avg. loss: 1.0746, Accuracy: 6717/10000 (67.17%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.68%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 42 [0/50000 (0%)]\tLoss: 1.422689\n",
            "Train Epoch: 42 [4000/50000 (8%)]\tLoss: 1.481343\n",
            "Train Epoch: 42 [8000/50000 (16%)]\tLoss: 1.321383\n",
            "Train Epoch: 42 [12000/50000 (24%)]\tLoss: 1.383314\n",
            "Train Epoch: 42 [16000/50000 (32%)]\tLoss: 1.490645\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tLoss: 1.568077\n",
            "Train Epoch: 42 [24000/50000 (48%)]\tLoss: 1.432903\n",
            "Train Epoch: 42 [28000/50000 (56%)]\tLoss: 1.456210\n",
            "Train Epoch: 42 [32000/50000 (64%)]\tLoss: 1.398168\n",
            "Train Epoch: 42 [36000/50000 (72%)]\tLoss: 1.379357\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tLoss: 1.491138\n",
            "Train Epoch: 42 [44000/50000 (88%)]\tLoss: 1.484124\n",
            "Train Epoch: 42 [48000/50000 (96%)]\tLoss: 1.403004\n",
            "\n",
            "Test set: Avg. loss: 1.0779, Accuracy: 6635/10000 (66.35%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.70%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 43 [0/50000 (0%)]\tLoss: 1.534047\n",
            "Train Epoch: 43 [4000/50000 (8%)]\tLoss: 1.396194\n",
            "Train Epoch: 43 [8000/50000 (16%)]\tLoss: 1.531825\n",
            "Train Epoch: 43 [12000/50000 (24%)]\tLoss: 1.465773\n",
            "Train Epoch: 43 [16000/50000 (32%)]\tLoss: 1.484482\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tLoss: 1.464269\n",
            "Train Epoch: 43 [24000/50000 (48%)]\tLoss: 1.422667\n",
            "Train Epoch: 43 [28000/50000 (56%)]\tLoss: 1.513658\n",
            "Train Epoch: 43 [32000/50000 (64%)]\tLoss: 1.446991\n",
            "Train Epoch: 43 [36000/50000 (72%)]\tLoss: 1.549318\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tLoss: 1.472065\n",
            "Train Epoch: 43 [44000/50000 (88%)]\tLoss: 1.436271\n",
            "Train Epoch: 43 [48000/50000 (96%)]\tLoss: 1.470439\n",
            "\n",
            "Test set: Avg. loss: 1.0890, Accuracy: 6679/10000 (66.79%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.68%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 44 [0/50000 (0%)]\tLoss: 1.448547\n",
            "Train Epoch: 44 [4000/50000 (8%)]\tLoss: 1.433347\n",
            "Train Epoch: 44 [8000/50000 (16%)]\tLoss: 1.515854\n",
            "Train Epoch: 44 [12000/50000 (24%)]\tLoss: 1.487175\n",
            "Train Epoch: 44 [16000/50000 (32%)]\tLoss: 1.353103\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tLoss: 1.466556\n",
            "Train Epoch: 44 [24000/50000 (48%)]\tLoss: 1.408776\n",
            "Train Epoch: 44 [28000/50000 (56%)]\tLoss: 1.416254\n",
            "Train Epoch: 44 [32000/50000 (64%)]\tLoss: 1.464297\n",
            "Train Epoch: 44 [36000/50000 (72%)]\tLoss: 1.438732\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tLoss: 1.403757\n",
            "Train Epoch: 44 [44000/50000 (88%)]\tLoss: 1.393513\n",
            "Train Epoch: 44 [48000/50000 (96%)]\tLoss: 1.475317\n",
            "\n",
            "Test set: Avg. loss: 1.0869, Accuracy: 6630/10000 (66.30%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.66%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 45 [0/50000 (0%)]\tLoss: 1.346584\n",
            "Train Epoch: 45 [4000/50000 (8%)]\tLoss: 1.444797\n",
            "Train Epoch: 45 [8000/50000 (16%)]\tLoss: 1.506825\n",
            "Train Epoch: 45 [12000/50000 (24%)]\tLoss: 1.445957\n",
            "Train Epoch: 45 [16000/50000 (32%)]\tLoss: 1.338832\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tLoss: 1.452296\n",
            "Train Epoch: 45 [24000/50000 (48%)]\tLoss: 1.361462\n",
            "Train Epoch: 45 [28000/50000 (56%)]\tLoss: 1.360227\n",
            "Train Epoch: 45 [32000/50000 (64%)]\tLoss: 1.447032\n",
            "Train Epoch: 45 [36000/50000 (72%)]\tLoss: 1.450228\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tLoss: 1.378880\n",
            "Train Epoch: 45 [44000/50000 (88%)]\tLoss: 1.461018\n",
            "Train Epoch: 45 [48000/50000 (96%)]\tLoss: 1.450070\n",
            "\n",
            "Test set: Avg. loss: 1.0566, Accuracy: 6717/10000 (67.17%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.66%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 46 [0/50000 (0%)]\tLoss: 1.514591\n",
            "Train Epoch: 46 [4000/50000 (8%)]\tLoss: 1.417964\n",
            "Train Epoch: 46 [8000/50000 (16%)]\tLoss: 1.444136\n",
            "Train Epoch: 46 [12000/50000 (24%)]\tLoss: 1.539145\n",
            "Train Epoch: 46 [16000/50000 (32%)]\tLoss: 1.580015\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tLoss: 1.528444\n",
            "Train Epoch: 46 [24000/50000 (48%)]\tLoss: 1.543626\n",
            "Train Epoch: 46 [28000/50000 (56%)]\tLoss: 1.379595\n",
            "Train Epoch: 46 [32000/50000 (64%)]\tLoss: 1.519529\n",
            "Train Epoch: 46 [36000/50000 (72%)]\tLoss: 1.425594\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tLoss: 1.420850\n",
            "Train Epoch: 46 [44000/50000 (88%)]\tLoss: 1.529563\n",
            "Train Epoch: 46 [48000/50000 (96%)]\tLoss: 1.464366\n",
            "\n",
            "Test set: Avg. loss: 1.0626, Accuracy: 6725/10000 (67.25%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.65%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 47 [0/50000 (0%)]\tLoss: 1.436732\n",
            "Train Epoch: 47 [4000/50000 (8%)]\tLoss: 1.446695\n",
            "Train Epoch: 47 [8000/50000 (16%)]\tLoss: 1.523795\n",
            "Train Epoch: 47 [12000/50000 (24%)]\tLoss: 1.410161\n",
            "Train Epoch: 47 [16000/50000 (32%)]\tLoss: 1.525602\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tLoss: 1.415715\n",
            "Train Epoch: 47 [24000/50000 (48%)]\tLoss: 1.431602\n",
            "Train Epoch: 47 [28000/50000 (56%)]\tLoss: 1.412887\n",
            "Train Epoch: 47 [32000/50000 (64%)]\tLoss: 1.415568\n",
            "Train Epoch: 47 [36000/50000 (72%)]\tLoss: 1.455824\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tLoss: 1.445811\n",
            "Train Epoch: 47 [44000/50000 (88%)]\tLoss: 1.490949\n",
            "Train Epoch: 47 [48000/50000 (96%)]\tLoss: 1.459946\n",
            "\n",
            "Test set: Avg. loss: 1.0921, Accuracy: 6624/10000 (66.24%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.66%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 48 [0/50000 (0%)]\tLoss: 1.503283\n",
            "Train Epoch: 48 [4000/50000 (8%)]\tLoss: 1.349419\n",
            "Train Epoch: 48 [8000/50000 (16%)]\tLoss: 1.485920\n",
            "Train Epoch: 48 [12000/50000 (24%)]\tLoss: 1.477655\n",
            "Train Epoch: 48 [16000/50000 (32%)]\tLoss: 1.506607\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tLoss: 1.485307\n",
            "Train Epoch: 48 [24000/50000 (48%)]\tLoss: 1.337881\n",
            "Train Epoch: 48 [28000/50000 (56%)]\tLoss: 1.506651\n",
            "Train Epoch: 48 [32000/50000 (64%)]\tLoss: 1.296966\n",
            "Train Epoch: 48 [36000/50000 (72%)]\tLoss: 1.383398\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tLoss: 1.394212\n",
            "Train Epoch: 48 [44000/50000 (88%)]\tLoss: 1.422163\n",
            "Train Epoch: 48 [48000/50000 (96%)]\tLoss: 1.443923\n",
            "\n",
            "Test set: Avg. loss: 1.0603, Accuracy: 6756/10000 (67.56%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.64%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 49 [0/50000 (0%)]\tLoss: 1.360224\n",
            "Train Epoch: 49 [4000/50000 (8%)]\tLoss: 1.454694\n",
            "Train Epoch: 49 [8000/50000 (16%)]\tLoss: 1.380106\n",
            "Train Epoch: 49 [12000/50000 (24%)]\tLoss: 1.327707\n",
            "Train Epoch: 49 [16000/50000 (32%)]\tLoss: 1.405817\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tLoss: 1.454478\n",
            "Train Epoch: 49 [24000/50000 (48%)]\tLoss: 1.381323\n",
            "Train Epoch: 49 [28000/50000 (56%)]\tLoss: 1.447120\n",
            "Train Epoch: 49 [32000/50000 (64%)]\tLoss: 1.442997\n",
            "Train Epoch: 49 [36000/50000 (72%)]\tLoss: 1.450337\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tLoss: 1.483011\n",
            "Train Epoch: 49 [44000/50000 (88%)]\tLoss: 1.581717\n",
            "Train Epoch: 49 [48000/50000 (96%)]\tLoss: 1.398035\n",
            "\n",
            "Test set: Avg. loss: 1.0611, Accuracy: 6744/10000 (67.44%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.67%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 50 [0/50000 (0%)]\tLoss: 1.365465\n",
            "Train Epoch: 50 [4000/50000 (8%)]\tLoss: 1.393938\n",
            "Train Epoch: 50 [8000/50000 (16%)]\tLoss: 1.514756\n",
            "Train Epoch: 50 [12000/50000 (24%)]\tLoss: 1.347486\n",
            "Train Epoch: 50 [16000/50000 (32%)]\tLoss: 1.357545\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tLoss: 1.452970\n",
            "Train Epoch: 50 [24000/50000 (48%)]\tLoss: 1.402322\n",
            "Train Epoch: 50 [28000/50000 (56%)]\tLoss: 1.386113\n",
            "Train Epoch: 50 [32000/50000 (64%)]\tLoss: 1.457819\n",
            "Train Epoch: 50 [36000/50000 (72%)]\tLoss: 1.412359\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tLoss: 1.367202\n",
            "Train Epoch: 50 [44000/50000 (88%)]\tLoss: 1.575484\n",
            "Train Epoch: 50 [48000/50000 (96%)]\tLoss: 1.454708\n",
            "\n",
            "Test set: Avg. loss: 1.0701, Accuracy: 6719/10000 (67.19%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 0.62%\n",
            "Quaternion sparsity: 0.00%\n",
            "\n",
            "Elapsed time: 438.24 seconds\n",
            "\n",
            "Q-CNN weight\n",
            "[22.084945480126628, 3.073338023214911, 2.213037870793766, 1.8246570524094285, 1.6751670770313032, 1.4743815218665746, 1.3512721303787112, 1.2691992027201282, 1.2574744987689068, 1.1138468753663977, 1.0977254074334586, 1.0918630554578534, 1.0127213037870786, 0.9907374838785343, 0.9790127799273018, 0.9731504279516967, 0.9159924961894661, 0.8822839723297005, 0.85443780044554, 0.8383163325126008, 0.7958142806894108, 0.7943486926955123, 0.7958142806894108, 0.798745456677219, 0.7577089928479275, 0.7210692930003537, 0.7181381170125456, 0.7342595849454847, 0.7767616367686747, 0.7210692930003537, 0.7283972329698685, 0.7064134130613242, 0.7518466408723223, 0.6624457732442246, 0.7577089928479275, 0.6668425372259312, 0.7020166490796065, 0.7225348809942522, 0.6829640051588703, 0.6771016531832541, 0.6888263571344866, 0.6829640051588703, 0.6976198850978999, 0.6771016531832541, 0.6595145972564165, 0.6551178332747098, 0.6477898933051951, 0.6565834212686084, 0.644858717317387, 0.6683081252198408, 0.6243404854027412]\n",
            "Q-CNN quaternion\n",
            "[2.9356283895307755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Q-CNN accuracy\n",
            "[10.91, 33.27, 39.73, 43.86, 48.24, 49.94, 50.95, 54.09, 54.85, 56.45, 56.86, 58.03, 59.46, 59.94, 59.89, 60.14, 61.36, 61.39, 62.11, 61.4, 63.15, 62.58, 63.56, 63.58, 63.93, 64.69, 65.22, 64.49, 64.15, 63.41, 65.38, 64.87, 66.04, 65.85, 64.97, 65.98, 65.72, 65.8, 66.56, 66.99, 67.04, 67.17, 66.35, 66.79, 66.3, 67.17, 67.25, 66.24, 67.56, 67.44, 67.19]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESlw/x99IDrsPSncbAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZjkOBTMzy3EomJlZzv8HajvMpxeZG2kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcjeX7wPHPbQwzjG0sgyjbKGQn\nkcoWhVBfoa8KEUobbUr9KqWFsqakUvRNlCVStjQjUZaxL8ku2WaYYcY22/X74z7DYJYzY845M+dc\n79freZ1znvW+Gz3X89yrERGUUkr5rnyeToBSSinP0kCglFI+TgOBUkr5OA0ESinl4zQQKKWUj9NA\noJRSPk4DgVJK+TiXBQJjzI3GmI2pltPGmGeNMcHGmKXGmF2OzxKuSoNSSqnMGXd0KDPG+AH/Ak2A\nQcBJEXnPGDMUKCEiL7k8EUoppdLkrkDQFnhdRG4zxuwEWojIEWNMOSBcRG7M6PhSpUpJpUqVsnXt\nM2fOULhw4Wwdm1dpnn2D5tn7XWt+IyIiokSkdGb75c/2FbKmB/Ct43uIiBxxfD8KhGR2cKVKlVi3\nbl22LhweHk6LFi2ydWxepXn2DZpn73et+TXGHHBqP1e/ERhjCgCHgVoicswYEyMixVNtjxaRq+oJ\njDH9gf4AISEhDWfMmJGt68fFxREUFJS9xOdRmmffoHn2ftea35YtW0aISKNMdxQRly5AZ2BJqt87\ngXKO7+WAnZmdo2HDhpJdYWFh2T42r9I8+wbNs/e71vwC68SJ+7Q7mo8+yKViIYD5QC/H917APDek\nQSmlVDpcWkdgjCkM3AUMSLX6PeA7Y0xf4ADQLTvnTkhI4NChQ5w/fz7D/YoVK8aOHTuyc4k8JSAg\ngAoVKuDv7+/ppCil8hiXBgIROQOUvGLdCaD1tZ770KFDFClShEqVKmGMSXe/2NhYihQpcq2Xy9VE\nhBMnTnDo0CEqV67s6eQopfKYPNuz+Pz585QsWTLDIOArjDGULFky07cjpZRKS54NBIAGgVT0v4VS\nKrvc1Y9AKaV82tatMH8+FCkCwcFQooRdgoOhVCn76annOQ0E18DPz4/atWuTmJhI5cqV+frrryle\nvHia+06bNo2RI0dijCF//vz07NmT559/nt69e7N06VL27t1LwYIFiYqKolGjRuzfv5/9+/dTuXJl\nxo8fz1NPPQXAk08+SaNGjejdu7cbc6pU7pOQACdPQlwcVK4M+VxUvnHqFOzZY69z5szln9Wrw113\nZXwDF4FPP4Vnn4ULF9Lfr1gxqFbt8iU2thi33AKFCuV8vlLTQHANAgMD2bhxIwC9evVi4sSJDBs2\n7Kr9Fi5cyNixY1myZAnly5fnwoULTJs27eJ2Pz8/pkyZwuOPP37VsWXKlGHcuHEMGDCAAgUKuC4z\nSnnY4cPw22+wfr290V64cGk5f94uBw40JD7eBoDTpy8dW6MGvPAC9OwJOfW/yfHj8OGH8PHH9qaf\nntatYfRoqFPn6m2nTkH//vDdd9CuHUyZAv7+EB1tl5Mn7eexYzbY7N4Na9fC999DcjJAfVq3hpo1\ncyZP6dFAkEOaNm3K5s2b09z27rvv8sEHH1C+fHkAChYsyGOPPXZx+7PPPsuYMWMuW5eidOnS3Hbb\nbUydOjXN7UrlVfv22Rv/8uX2c88eu75gQQgKsp8BAfYz5XuJEvGEhkLJkpcWgMmT4dFH4bXX7JN3\n//5QtOila504Ya8RHg6rVkHZsvbG3K6dffJO/UR/5Ah88AF88okNQt27wwMP2PMVLmzTVriwfUr/\n/nt4/XWoXx8eewyGD4cyZex5IiLssfv3w7vvwosvXnprKZ3J6D/x8XDgAMyZs5mqVdOIMDnMKwLB\ns4ueZePRjWluS0pKws/PL8vnrFe2HmPvHuvUvklJSSxbtoy+ffumuX3r1q00bNgw3eOvv/56mjdv\nztdff82999571faXXnqJe+65h0cffdS5xCuVSyUkwOzZMH48/PGHXRccDHfcAYMG2c+6dSF/Onem\n8PAtaY698/jjsGQJjBxp3wzeftvemC9csDf/LVvsfoGB0KQJbN8OCxbYdZUrQ9u20KaNDRaffWbT\n2bMnvPIK3JjBkJhPPgn//a8NABMnwrffwquv2sD1wgs2KCxfDrfdlrX/TgUKQGgoNGlykoIFs3Zs\ndnhFIPCUc+fOUa9ePf79919q1KjBXXfdle1zvfzyy3Tu3JkOHTpcta1KlSo0adKE6dOnX0tylQ/7\n6y9bBFGhgn0azm6/QxF7cw0IyNpxx47Zp/ZPPrFP3NWq2afudu1ssce1lu8bc+kJf+1aGDXKFtcE\nBNibcI8ecOed0LjxpaKjPXtg8WK7fPONLcfPnx969YKXX4aqVZ27dnAwjB0LAwfC88/bJ3+Ajh3h\nq68uvbXkZl4RCDJ6cndlh7KUOoKzZ8/Srl07Jk6cyNNPP82wYcP46aefANi4cSO1atUiIiKCVq1a\npXuu0NBQ6tWrx3fffZfm9ldeeYWuXbty5513uiQvyjudOWOfUMeNszdxsDfNsmXhuuvs0qqVfXoO\nDMz4XAsXwnPPwY4dNqCEhtrK0urV7feQEDh71panp65QjYiAmTNtcUe7dvD553D33a6r3G3c2JbJ\nnzxpW+ikF/SqVoUnnrBLQgKsW2f/e1x/ffaue9NN9i3jl1/g33/hkUc81wooq7wiEHhaoUKFGD9+\nPF26dOGJJ55gxIgRjBgx4uL2l19+mRdeeIGffvqJsmXLEh8fz7Rp0+jXr99l5xk2bFiabwQAN910\nEzVr1uTHH3+kcePGLs2P8g7Ll0PfvvbJ94knoEMHe4M6dOjS519/wbx58M479kl24MCrW6hs324D\nwKJF9ob/2mu23HvXLnvDjY7OOB1BQbbM/sknMy5myWnBwc7v6+8PTZvmzHXbtMmZ87iTBoIcUr9+\nferUqcO3337Lww8/fNm29u3bc+zYMdq0aYOIYIxJs7y/Vq1aNGjQgPXr16d5jWHDhlG/fn2XpF/l\njPh4WLHCVoR26WLbh+e09ettmXbx4o3o1g3uucfexFLK1ePiYOhQW2ZdpQqEhUFGQ9r/9hu8+aa9\n2b///qWAcO4cvPEGTJpkb+ajR9ty/Ctb5Zw4YYNCVNSlytSUCtWgIFvJml6Zv8olnBmi1NNLWsNQ\nb9++3alhWE+fPu3Uft4g5b+Jrw3VK+LZPEdGikybJvLAAyJFi4rYQhiRggVFHn5YZNUqkeTknLnW\n7t0iZcqIVKwoUrdutPj52WsVL26vP3q0SKVKIsaIPPOMSFyc8+desUKkTRt7vtKl7Tn9/ESeeMLm\nMTfwtX/b7hqGWuO0UtkQFwdff20rGf/4w7b5LlfONhfs2NGWoU+ZAtOm2f3q1bMtW/77X/uUnB3H\nj9uy9cREW+xz9OhG6tVrwS+/2PL7n3+2zRlDQ+1TfvPmWTt/8+awdKltXvnee+DnZ1vf1KqVvfSq\nvEMDgVJZsHevLXL54gvbWahOHVtmfu+9ti156grQBg1s+/Hp022npAEDYPBgGxTq17+01KpFpk0E\nY2OhfXtbtv/rr7Zi8uhRKF4cuna1i4gtorn++qy36kmtWTM7FILyHRoIlMqECCxbZtu+L1hgn5S7\ndoWnn4Zbb824ZUiRIjYA9O9v3xxmzrRl/FOn2oACtqKyTh3bbr13bzv+TGrx8fZ6GzfCDz/Ya6bF\nGNuCR6ms0kCgVCrJyfapPyLCLuvX2yU62vYGffVVW5Hq6CTuNGPsk3azZpeus2cPbNhgl7AwGDIE\nhg2zAWHQIPvmkJxse8wuWWKLmjp2zPk8K6WBQPmUQ4dsL9AtWyAp6erl8GFb5AO2dUzt2nZ4gTvu\ngP/859qKXFLLl8+W5YeGQjfHHH0bNti3hG++sW3tmzWDG26wvVVHjIA+fXLm2kpdSQOByvNWrYJ9\n+wpz553pF9OcPm2bRo4ebYt6br/dFsn4+V1a8uWzzSwbNICGDW3ZvTvH+atf3waAUaNsj9SPP7Z5\nGzTI9nRVymWcaVrk6SW3Nh/9559/pFOnTlKtWjWpXLmyDBo0SM6fP5/mvqNGjZIbb7xR6tatK40a\nNZKpU6eKiMidd94pqfO3du1aufPOO0XENh0DZP78+Re3d+jQId0mZb7YfHTSpEvNNUNDRV56SWTN\nmkvNNePjRSZOtM0hQaRnT5H9+z2bZmclJYls2WI/0+JLf+cUvpZndzUfzdMzlHmSiHD//ffTpUsX\ndu3axa5duzh37hwvpgw0ksqkSZNYunQpa9asYePGjSxbtgxJ6e8PHD9+nIULF6Z5nQoVKlzWS1ld\n8vHHtry+QwcYPPhvKlWywwbfcostUnniCVu0M2iQHc9m7Vr43//strwgXz64+WbXDcWgVAr9J5ZN\nv/76KwEBAfRxFNz6+fkxZswYpk2bRtwVg5e/8847fPLJJxR1jItbtGhRevXqdXH7Cy+8kO7Nvm7d\nuhQrVoylS5e6KCd504QJ9gZ/7712NMtOnQ6zZIkd3GzqVFvMMmWK3XfePFsZ26iRZ9OsVG7lFXUE\nzz5rm9alJSkpkGyMQk29enZEwfRs27btqqGlixYtSqVKldi9ezf16tUD4PTp08TGxlKlSpV0z9W0\naVPmzp1LWFhYmgPkDRs2jNdee+2aRjfNC2JjbcXoDz/YoYL79rUds640dqxtj9+li22OmbocPzjY\nDvb1yCO22aW/f94Z+EspT9E3glzi1Vdf5e23305z2x133AHA77//7s4kuc2mTbYY57rrbJv7bdvs\nGDc33ACdO8NPP9kWPWCLfgYPti14vvsu48rcAgU0CCjlDK94I8joyT029pxLhqGuWbMms2bNumzd\n6dOnOXr0KOPGjWPDhg2UL1+en3/+maCgIPbu3ZvhW0GrVq149dVX+fPPP9PcPmzYMN5++23y57HR\nu5KT7Y3+zBk71G9iol0SEmzP2K++sh2tAgLs8AwDB9q3gX37bAuaKVNsL9eKFe248jNm2Oac33yT\n/TH1lVKX0zeCbGrdujVnz569OPdwUlISzz33HE8++SRffvklGzdu5OeffwbsMNSDBg3itGOS1bi4\nuMvmLE7x6quvMnLkyDSv17ZtW6Kjo9OdDjM3OnTIjo3ToIFtrtmqlR01s317+6Q/YIAdM37MGDt0\nwldfXeqpW6WKHRr5n39g1iw7J+2MGfDgg3bIBg0CSuWcvPV4mYsYY5g7dy6DBg3irbfeIjIyku7d\nu6c5ef3jjz9OXFwcjRs3xt/fH39/f5577rmr9mvfvj2lM5jMdNiwYXTu3DlH8+EKIvZmPWiQffIf\nPdq23smf3y7+/vYzMNDe4DMqvvH3t8VA//mPHe44OFiLe5RvEBF2x+2mBS3cczFXLUBxYBbwF7AD\naAoEA0uBXY7PEpmdJ7f2I0ht5cqVcv3110tERIRbrpeW3NCPIDJSpGtX22a/WTORXbvcc11fa18u\nonn2Zn9H/S33/O8eMW8YiTic/XsKuaQfwThgkYjcBNR1BIOhwDIRCQWWOX7nec2aNePAgQM0aNDA\n00nxmJ9+sk/+8+bZYYx/+83OTauUcs6Z+DO8suwVbv7kZn4/+DsDqwykdpnaLr+uy4qGjDHFgDuA\n3gAiEg/EG2M6w8V3nalAOPCSq9Khsi45GTZvht27bcubggVtZW7BgnaJj7cDpu3effkSGWkDwaJF\nULeup3OhVN4hIny37TueX/o8h04f4pG6j/Be6/fYGbETfz/XV4gZSdXDNUdPbEw9YDKwHfs2EAE8\nA/wrIsUd+xggOuX3Fcf3B/oDhISENJwxY8Zl24sVK0bVqlUxmRQYJyUl4ZedjgR5jIiwZ88eTp06\nRVxcHEFZnP0kMrIAERHBrFtXgvXrSxAd7dwgO2XKnOe6685Rvvw5qlaNo0OHIxQo4Jp/UxnJTp7z\nOs1z3icibDq1iWkHprEhZgPVgqrxdLWnqV3MvgVca35btmwZISKZdqV0ZSBoBPwJ3CYiq40x44DT\nwFOpb/zGmGgRKZHeeQAaNWok69atu2zdvn37KFKkCCVLlswwGMTGxrqk+WhuIiKcOHGC2NhYKleu\nTHh4OC0ymqT24nHw6ae2l+727XZdmTJw1112qVfPNvW8cOHScv68HaCtShW7BAa6Nm/OcjbP3kTz\nnHNEhIELBnI47jAtbmjBnZXupF7ZeuTP55pCkxNnTzB101QmR0xm54mdBAcG83bLt+nfsD9++S49\nuF5rfo0xTgUCV7YaOgQcEpHVjt+zsPUBx4wx5UTkiDGmHHA8OyevUKEChw4dIjIyMsP9zp8/T0BO\njR2ciwUEBFAhrW646Th/3rbZnzrVTnw+apS9+deurWPbKN8zf+d8Jq+fTLmgciz4ewEARQsWpfn1\nzWlxQwtqlK5BUIEgggoEUdi/sP0sUJjiAcXJZ5z7H0ZEWHFwBZ9GfMqs7bOIT4qnWcVmfNX8Kx6o\n9QCF/Au5MosZclkgEJGjxph/jDE3ishOoDW2mGg70At4z/E5Lzvn9/f3p3LlypnuFx4eTv369bNz\nCa/1779w//2wZo3twfvaa3rzV74rPime55c+T83SNdk0cBPHzxxn+f7lLD+wnPD94fy86+d0jy1T\nuAwP1X6IPvX7cHOZm9Pc50jsEaZumsqUDVPYdXIXxQoWY0DDATzW4DFqh7i+ItgZru5H8BTwjTGm\nALAX6IPtxPadMaYvcADo5uI0qFRWrrRt8s+csWP65IFuCUq51EdrPmL3yd0s7LmQ/PnyU75IeR6s\n/SAP1n4QgKNxR/nn1D/ExcdxJuGM/Yw/Q2x8LCsOrmDCmgmM/nM0Dcs1pE+9PjxY+0GCCgTx098/\nMWXjFBbuWkiSJHHHDXcw7PZhHn/6T4tLA4GIbATSKp9q7crrqrRNngxPPmnH8Pn1Vzs0s1KuFH0u\nmsOxh6lVptY1n+vE2RNEXojkWNwx8ufLT/58+fH388c/n3+2W9ZEnY1i+PLh3F3tbu6udnea+5QN\nKkvZoLJpbnv21meJPBPJ9C3T+XLjlzy58EmGLBlCkQJFOHHuBOWCyvHibS/Sp14fQkuGZiuN7qA9\ni33A6dN2oLYpU+yQD9OnXz1BulKZOX3hNKv+WUVCUgKtq7TO8Kn2n1P/MObPMUyOmMyZhDN0uakL\nY9uN5Ybi2ZsM4vP1nzNgwQCSJdk2QUnFz/jx5C1P8l6b9wjIn7X6wNfDXicuPo4P236YrXQBlC5c\nmmdufYZnbn2GDUc28NXGr4g8G0nP2j1pV62dyyqcc1LuT6G6JosWwWOP2bl4X3nFztfrA61pVQ44\nEnuEFQdX8PvB31lxcAWbj222N2IgMH8g7aq1476b7qNj9Y4EBwYDsPX4VkatGsX0LdMREXrc3IPQ\n4FBGrhpJjYk1ePWOV3mu6XMUzF/Q6XR8s/kb+v/YnzZV2lAnfx2qVqtKYnIiCckJJCYn8lfUX4xb\nPY5l+5Yx/f7pTpe7b4/czqcRnzKw0UBqls6Z1+P65epTv1zeq5PUQOCloqNhyBA7kFuNGnbu2yZN\nPJ0qlRUJSQlM3TSVZhWb5diNyhm7TuzisR8fY/mB5QAU8i9E0wpN+b87/o/bb7idZEnmh79+uLj4\nGT9aVGpBAb8CLNy9kEL+hRjUeBCDbx188Q2gT/0+PLvoWYb9Ooypm6Yysf1E2lRpk2la5u6YS68f\netGiUgvm9ZjH6pWradG4xVX7PVDzAfrM60Pjzxrzfpv3earJU5m25nluyXMUKViEN1q8keX/Rl7H\nmXEoPL2kNdaQs3xlbJLURozYLOXKifj5ibz8ssi5c55Oket549958KLBwhsIbyDNpzSXaRunydn4\nsxe353Sek5KTZNyf4yTw7UAp/l5xeXfFu7Lm0BqJT4xPd//Vh1bL0KVD5cYJN0rZD8rK8PDhEnUm\nKt1r/Pz3z1J1XFXhDaTrd11ly7EtGe7rP9xfmn7eVGIvxIpIxnk+FndM7p1+r/AG0u7rdnL49OEM\nz80byOhVo9PdJzdw15zFLutQlpPS6lDmLF/odHPhAkRE2BZBy5bB4sW2P8CXX8IVk6h5LW/7O8/c\nOpMes3vQv0F/QkuGMjliMrtO7qJEQAkeqfsIfev3ZVPEJsrdVI4Dpw5w8NRBDp46yJG4Izx484M8\nUveRLF1vb/Re+szrw28HfqN9aHs+u/czyhcp75K8nU88z6iVo3h/5fucSTjDvdXvZWjzoTSr2Ozi\nPmH7wmg/vT01S9dk2SPLKB5g+6Bm9ncWET6N+JQhi4dQuEBhBjQcQOvKrWlasenF+oOEpATqTqpL\nYnIiW5/YSgE/53rRe4K7OpRpIMijVq2yE7asXGknZb9wwa6vVg2aN9/Hp59WznD2Lm+TV/7OIpLp\nsCjbjm+jyedNqFu2LmG9wijgVwARIXx/OJPXT2b29tkkJCdcdozBUL5IeQrmL8je6L0MaDiAcXeP\ny7QsPlmS+WTtJ7z4y4vkz5efcXePo1fdXpmmMSecOHuCiWsnMn71eE6cO8Ht19/OS7e9RInAErT9\nui2VilcivHc4pQqVuniMs3/nHZE7GPTzIH478BtJkkRA/gBuq3gbrSu35vSF07y38j3m9ZhHpxs7\nuTCH185dgcDjxT7OLFo0dLkJE0SMEfH3F2nSROS550TmzBE5etRu98Y8Zya35/lY3DHpN6+fBL8f\nLJPWTpLk5OQ09zt1/pRUn1BdQkaFyL+n/01zn+Nxx2Xyusky9NuhEr4vXPae3CsXEi+IiEhCUoK8\ntPQl4Q2kyWdN5J9T/6R5juTkZFm4a6E0/bzpxaKUgzEHcyazWRR3IU7G/TlOrh9zvfAGYt4wEjo+\nVI7EHrlq36z+nWPOxciPO3+UwYsGS51P6lwsams1tVW6f4PcxF1FQx6/yTuzaCCwkpNFhg2zf7VO\nnUTSm2rBm/LsrNya5/jEeBn35zgp9m4xyT88v9T9pK7wBtL5284SeSbysn2Tk5Plvhn3id+bfhK+\nLzzTc2eU59nbZ0uRd4pI6ZGl5de9v15cn5iUKDO3zpT6k+oLbyAVRleQzyM+zxU3xfjEeJm2cZo8\nOOvBdIPStf6dj8cdlx92/JBmkMmN3BUIdGCBPCIxEfr1gxEj7Ofs2eDlY+nlCnHxcfSb34+WU1ty\n6PShLB27bO8y6n9an2cWPUOTCk3Y8vgW1g9Yz+i2o1m4eyG1P6nN0j1LL+4/atUo5v41l5F3jeTO\nSndeU7rvr3E/ax5bQ6lCpWjzdRtGrhzJF+u/oMbEGnSf1Z0zCWeY0mkKe57eQ98Gfd1SFJQZfz9/\nHq77MNP/M52KxSq65BqlC5em802d0+0g5qu0+WgecPYs9OgBP/5oxwV6802drtEdNh3dRPdZ3dl1\ncheB+QNpNLkRc7vPpWnFphkedyDmAM8teY7ZO2ZTuXhlfuj+A51u7HTxZju46WBaVW7Ff+f8l7b/\na8uQW4fQpkobXl72Mg/UfIDBtw7OkfTfVOomVvdbTd/5fXnpFzvlR/2y9fn+ge+576b7LhvlUvk2\nDQS53MmTcO+98McfMHEiPPGEp1Pk/USET9Z9wpDFQwgODGbZI8soU7gMnb7tRIupLZjccTK96vW6\n6rgz8Wd4f+X7jFo1CoPhrZZv8Xyz59Ps7Vq3bF3WPbaOF5e+yOg/RzP6z9HUKFWDLzp9kaNP50UK\nFmFm15l02tKJ0oVK07Zq21zx9K9yFw0EudTZs3YoiPfeg3/+ge+/t4PFKdeKOR9Dv/n9mL1jNvdU\nu4epXaZSunBpAFb3W023Wd3oPa83W45v4f027+OXzw8RYea2mbyw9AUOnT5Ej5t7MLLNyEyLNwL9\nA5nQfgJ3V7ubCWsmMPbusRQpmPPlfcYYHqrzUI6fV3kPDQS5zIED8PHH8Nlntndw3bqwdCnccYen\nU+Y9kiWZ6HPRxJyPIeZ8DKcunCLmfAxRZ6N49/d3OXT6EKPuGsWQpkMu651aslBJFvVcxODFg/nw\njw/ZFrmNobcN5dWwV/n94O/UL1uf6fdP5/Ybbs9SejpU70CH6h1yOptKOU0DQS6xciV8+KGd+N0Y\nuO8+ePppaN5c6wNy0upDq+k9rzd/Rf2V5vZKxSvxe5/faVIh7fE4/P38+aj9R9QuU5snFz7Jot2L\nKFWoFJM7TubR+o9qubvKkzQQ5AKrV8Ptt0NwMLz0Ejz+OFR0TaMJnxWfFM+b4W/y3sr3uK7IdXzY\n9kNKFSpF8YDily3lgso5NaTxgEYDqFWmFmH7wniqyVMXe74qlRdpIPAwETtEdEgI/PUXFCvm6RR5\nn83HNvPI3EfYdGwTfer1YUy7MRQLuPb/0M2vb07z65vnQAqV8iwNBB723Xe2RdDnn2sQyGmJyYl8\nsOoD/i/s/wgODM4TQwoo5QkaCDzo/HlbFFS3LvTu7enU5A7JksyKAyuoXrI65YqUy9Y5RISle5cy\n7NdhrDu8jq41u/JJh08uG7NGKXWJBgIPGjvWthKaMkUniwGIvRBLrx96MfevufgZP+4JvYdH6z1K\nh+odnB4hcvn+5bwW9horDq7g+mLX8+1/vqV7re7adl6pDGgg8JBjx+Cdd6BTJ2jVytOp8bzdJ3fT\nZUYX/or6ixGtRhAXH8dXG79iwd8LKF2oNI/UfYQ+9fpQo3SNNCcc2XZqG29Pe5tl+5ZRLqgcH93z\nEf0a9MvSTFhK+SoNBB7y+utw7hyMGuXplHje4t2L6TG7B/lMPhY/tJjWVVoDMLzlcBbvXsyUjVMY\nt3ocH/5h55Ut5F+IoAJBBBUIorB/YfKZfGw6tonShUozuu1oBjYaSKB/oCezpFSeooHAA7ZssR3G\nnnoKqlf3dGo8R0T4YNUHDF02lJvL3MwP3X+gconKF7fnz5f/Ymer42eOM3v7bI7GHSUuPo4zCWcu\nfp5NOMtjhR5jdI/RBBUI8mCOlMqbNBC4mQg895xtIfR//+fp1LhffFI8e07uYeeJnUzfMp3vt39P\nt1rdmNJpCoULFE73uDKFy/B448fT3R4eHq5BQKls0kDgZgsX2iEjxo61Hci83eHYw4z7cxzbIrex\n88RO9kXvI0mSAPAzfrzb+l1euu0lrcxVyoPSDQTGmPVOHB8pIu1yMD1e7dQpeP55CA21vYe93dI9\nS+k5pycx52OoUboG9cvWp0fXbdGWAAAe+klEQVStHtxY6kZuLHkjN5a6kaIFi3o6mUr5vIzeCAoC\nGfW+McCcjE5ujNkPxAJJQKKINDLGBAMzgUrAfqCbiEQ7n+S8JzERvvjCziUQFQULFuDV8wknJSfx\n1m9vMXz5cGqUrkF473Bqlq7p6WQppdKRUSAYJCJ7MjrYGPO0E9doKSJRqX4PBZaJyHvGmKGO3y85\ncZ48adkyO4TEli12BNExY6BBA0+nKvtWHFjB1E1TaVmpJe2qtbuqk9axuGP0nNOTZfuW8UjdR/i4\n/ccZlv0rpTwv3UAgIuFXrjPG3AAUEpEd6e3jhM5AC8f3qUA4XhgI/v7bFgP9+CNUrgyzZsH99+ft\nkUQ3HNlAh+kdOJNwhi82fIHB0KRCE+6pdg/tQ9sTFx/Hg7MfJOZ8DF90+oI+9fpo2b9SeYDTlcXG\nmJeAmoAYYxCR3k4cJsASY4wAn4rIZCBERI44th8FQrKY5lzvm2+gTx8ICID337fDSQdcPUlVnrIv\neh/3fHMPxQOKs+2JbRyNO8rPu35m4e6FvBH+Bq+Hvw5A9ZLVWfzQYuqE1PFwipVSzjJ2ovs0Nhjz\nBDBJRJIdv2eKSHfH980ikun/6caY60TkX2NMGWAp8BQwX0SKp9onWkRKpHFsf6A/QEhISMMZM2Zk\nPXdAXFwcQUHua1a4YEE5Ro+uTr16Mbz66naCgxPcdu0UOZ3nmPgYntr4FKcSTjGh3gRuKHzDVdvX\nRq8l6kIUnct3plD+Qjl2bWe5+++cG2ievd+15rdly5YRItIo0x1FJM0F6IW9ebd3/O4P/AT8DIxJ\n77gMzvcG8DywEyjnWFcO2JnZsQ0bNpTsCgsLy/axWTV2rAiItG8vcvas2y57lZzMc+yFWGk8ubEE\nvB0gKw+uzLHz5jR3/p1zC82z97vW/ALrxIn789WDtlwKEFOx5fm3GGN+AFYBPYCHRWRwZgHGGFPY\nGFMk5TvQFtgKzHcEmZRgMy/TaJUHvPsuPPusrQeYOxcCvWCEg4SkBB74/gEijkQws+tMmlVs5ukk\nKaVcILM6gorANOAz4C0gHnC2P2wIMNdRWZgfmC4ii4wxa4HvjDF9gQNAt+wkPLcQsT2E334bevaE\nr76C/F7QTU9E6PdjPxbtXsRn936m4/gr5cUy6lD2BZAPKATsFZFHjTGNgC+NMStF5J2MTiwie4G6\naaw/AbS+tmTnDinDRYwZA/36waRJeX846QMxB5i3cx6zts9ixcEVDG8xnH4N+nk6WUopF8ro2bWR\niNQFMMZsAF4WkXVAB2PMf9ySulxu2jQbBJ56yg4ZkS/dgrbcS0TYeHQj83bO44e/fmDTsU0A1Cxd\nkw/u+oAhTYd4OIVKKVfLKBAsNcb8hO1hPDP1BhGZ7dJU5RETJ0LNmjBuXN7sH3Dw1EEemvMQKw6u\nwGC47frbGHXXKDrf2JnQkqGeTp5Syk0y6lD2vGM4iCQROeXGNOUJ69fD2rV5NwjM3j6bfj/2IzE5\nkfF3j6f7zd0pU7iMp5OllPKAdAszjDF3i8jJjIKAMeZu1yQr9/v0U9sy6OGHPZ2Sy22P3M7mmM0k\nJiemuf1swlkGLhhI1++7EhocysYBG3mqyVMaBJTyYRkVDY0xxvyLHVwuPSOBRTmbpNwvNhamT4ce\nPaDEVV3hPGfW9lk8OPtBEpMTeX3n67Sr2o4OoR24J/QeShUqxdbjW+kxqwfbIrfxYrMXeavVW07P\nBayU8l4ZBYITwMeZHL83B9OSZ3zzDcTFwYABnk7JJV9v+pre83rTtEJT2hRpw8ECB/l518/M3DYT\ng6HxdY3ZfGwzxQoWY/FDi2lbta2nk6yUyiUyqiNo7s6E5BUitplovXpwyy2eTo316bpPefynx2lV\nuRXzesxj7aq1tGjRgmRJZv2R9fz0908s3L2QjtU78tE9HxES5HXDOymlroEXdH1yrzVrYNMmGwxy\nQyXxmD/GMGTJEDqEdmBWt1kE5L80ul0+k49G5RvRqHwjXm/xugdTqZTKzfJgy3fPmjQJgoLgv//1\ndEpgxG8jGLJkCF1rdmVO9zmXBQGllHKWvhFkQXQ0zJwJjzwCRYp4Jg2JyYmsO7yOqRunMiliEg/V\neYgvO39J/nz6p1RKZU+mdw9jzGpgCvCtiJx2fZJyr6+/hnPn3FtJLCLsPLGTX/b+wi97fyFsfxin\nL9g/wxONnmBC+wnkM/pip5TKPmceI3sBfYCNxphVwJcissy1ycp9UiqJb7kF6td3zzWPxR2j9bTW\nbIvcBkCVElXoUasHbaq0oWXllldNE6mUUtmRaSAQkb+Al4wxr2Ans59mjInHviVMEJEYF6cxV/j9\nd9ixA6ZMcc/1LiRe4L6Z97E3ei8ft/+YdtXaUaVEFfdcXCnlU5wqWDbG1MS+FdyLnT/gG6A58CuQ\nh6did96kSVCsGHTv7vpriQj9F/Tnj0N/8P0D39O1ZlfXX1Qp5bOcqSNYA5zFvgH8n4icc2xaaYy5\nzZWJyy2iouzk8wMGQCE3zML44R8fMm3TNN5s8aYGAaWUyznzRvCQiPyd1gYR8YnZSr78EuLj3VNJ\n/NPfP/Hi0hfpVqsbr93xmusvqJTyec40N3nYGJN6svkSxpg3XZimXCUpyQ433aIF1Krl2mttO76N\nB2c/SP1y9fmy85eY3NBjTSnl9ZwJBB1TVwiLSDS2rsAn/PgjHDgATz/t2uucOHuCTjM6UbhAYeb1\nmEchfzeUQSmlFM4VDfkZYwqISDyAMSYA8JkhK8ePhxtugHtdGPqOxR2j26xu/Hv6X5b3Xk6FohVc\ndzGllLqCM4FgBna2spSGk49iWw15vS1bICwMRo50zYT0FxIvMH71eN767S3OJZ5jWpdpNKnQJOcv\npJRSGXCmH8E7xpgtXJpwfqSI/OTaZOUOEybYyWf69s3Z84oI83fO57klz7Eneg8dq3fkw7YfUr1k\n9Zy9kFJKOcGp51wR+RH40cVpyVVOnID//Q8eegiCg3PuvFuPb2Xw4sH8svcXapauqXMDKKU8zpl+\nBI2BCUAN7ET2BrggIkVdnDaP+uILO67QU09d23lEhA1HN7Dg7wX8+PePrDu8jhIBJRh/93gGNhqI\nv59/ziRYKaWyyZk3go+Bh7B1BbcAvYEbXJgmj0tMtE1GW7aE2rWzfryIsGTPEubsmMOCXQs4HHsY\ng6FJhSaMaDWCAQ0HULJQyZxPuFJKZYMzgSCfiOw0xuQXkQTgM2PMBuBVF6fNY+bPh4MHYdy47B0/\nfct0Hpr7EEEFgmhXtR0dq3ekfWh7nSBeKZUrORMIzhhjCgCbjDHvAEcAP2cvYIzxA9YB/4pIR2NM\nZezbRUkgAng4pWlqbjFhQvabjCYmJ/Lm8jepV7Yef/b9k4L5C+Z8ApVSKgc506Gst2O/J4EkIBTI\nygA4zwA7Uv1+HxgjItWAaCCH2+Rcm82bITwcnnwS/JwOd5d8u+Vbdp3cxet3vq5BQCmVJ2QYCBxP\n82+IyHkRiRGR10Tk6fTGHkrj+ApAB+Bzx28DtAJmOXaZCnTJdupdIKXJ6KOPZv3YxORE3vrtLeqG\n1KXzjZ1zPnFKKeUCGRYNiUiSMaaKMcbfUT+QVWOBF4GUiR1LAjEikuj4fQi4LhvndYmUJqOPPJK9\nJqMzts5g18ldzOk2R8cJUkrlGc7UEewBVhhj5gFnUlaKyPiMDjLGdASOi0iEMaZFVhNmjOkP9AcI\nCQkhPDw8q6cAIC4uzuljv/uuAufPV+PWW9cSHn4m8wNSSZIkXln7ClULV6XY0WKEH3Pumq6QlTx7\nC82zb/C1PLsrv84EgoOOpZBjcdZtQCdjTHsgACgKjAOKO1ogJQIVgH/TOlhEJgOTARo1aiQtWrTI\nwqUvCQ8Px9lj33gD6tWDPn0aZ/k6/9v8P/459w9zus2hVY1WWT4+J2Ulz95C8+wbfC3P7sqvM0NM\nZGtQfBF5GXgZwPFG8LyI9DTGfI+tbJ6BnQ95XnbOn9NiY2HVKhgyJOvHptQN1AmpQ+ebtG5AKZW3\nONOzeCkgV64XkeyOi/ASMMMY8zawAfgim+fJUcuXQ0ICtM1GrmZsncHfJ/5mdrfZ5DPONMRSSqnc\nw5miodQdxwKA/wAXsnIREQkHwh3f92J7KOcqS5bYaShvy+Lkm0nJSRffBrrclKsaQCmllFOcKRpa\nfcWq5caYK9fleYsX21nICmax6b++DSil8rpM71zGmKKpluLGmNZACTekzW3274e//856sVBSchLD\nfxtO7TK19W1AKZVnOVM0tA1bR2CARGAf8JgrE+VuS5faz6wGgq83f83fJ/5m1gOz9G1AKZVnOVM0\nVNEdCfGkxYuhYkW46Sbnj4m9EMsry17hlutu4b4a97kucUop5WLOFA0NNMYUT/W7hKOzl1dITIRl\ny+zbQFY6A7/7+7sciTvCuLvH6duAUipPc+YONlBEYlJ+iEg08LjrkuRe69ZBTEzWioX2Ru/lwz8+\n5OE6D3NrhVtdlzillHIDZwLBZWNwGmPyAV4zrdaSJfZNoHXrzPdN8cLSF/DP58+7rd91XcKUUspN\nnKksXmqM+RaY5Pg9EPjFdUlyr8WLoXFjKOnkhGG/7vuVOTvmMKLVCK4rmmvGy1NKqWxz5o3gBWAl\nMNix/A4878pEuUtMDKxe7XyxUGJyIs8uepZKxSsxpGk2xqJQSqlcyJk3An/gYxH5CC4WDRXANiXN\n08LCICnJ+UDwWcRnbDm+hVkPzCIgf4BrE6eUUm7izBtBGFA41e/CwK+uSY57LVkCQUFwqxP1vdHn\nonkt7DVaVGrB/TXud33ilFLKTZwJBIEiEpvyw/E9K8NR50oitn6gVSvwd6Lq+83lbxJ9Ppqx7cbq\npDNKKa/iTCA4a4ypm/LDGFMPOO+6JLnHnj2wbx+0a5f5vjsid/DRmo/o36A/dcvWzfwApZTKQ5yp\nIxgMzDXGHMAOM1ER+K9LU+UGS5bYz8zqB5KSk3jsx8coWrAow1sOd33ClFLKzZwafdQYUwOo4Vi1\nHUhyaarcYMkSqFwZqlbNeL8Jayaw8p+VTOsyjdKFS7sncUop5UZOjY0gIhdEZCNQDJhAOtNL5hUJ\nCfDrr5kPK7H75G5eWfYKHUI78FCdh9yXQKWUciNnxhpqZIwZ7Sga+hlYA9zs8pS50J9/2qkpM6of\nSJZk+s7vSwG/Anza8VOtIFZKea10i4aMMcOB7sBR4FugEbBGRHLF1JLXYskS8PODli3T3+fjtR/z\n24HfmNJpivYgVkp5tYzqCAZh5yIYA/wsIvHGmKvmLs6Ldu6EatWgePG0t++N3svQX4bSrmo7etfr\n7da0KaWUu2VUNFQWGAk8AOw1xnwJBDp6FudpUVFQOp1632RJpt/8fuQz+fjs3s+0SEgp5fXSvamL\nSIKILBCRnkAosAhYDfxrjJnmrgS6QmQklCqV9rbJEZMJ2x/Gh20/pGIxr5+TRymlnG41dE5EZopI\nF2wz0nCXpsrF0nsjOBBzgBeWvkCbKm3o16Cf+xOmlFIe4EyHsss4JqmZ4oK0uIWIDQRXvhGcSzjH\nA98/AKBFQkopn5LlQJDXnT5tp6dMHQhEhEfnP8raw2uZ230ulYpX8lj6lFLK3ZzpR3BVsEhrXV4R\nGWk/UxcNjVgxghlbZ/BOq3foclMXzyRMKaU8xJk6gjVOrssToqLsZ8obwZwdc3gt7DUeqvMQQ5sP\n9VzClFLKQzLqUFYGKIdtMlobO+AcQFGcGIbaGBMA/AYUdFxnloi8boypDMwASgIRwMMiEn9NuciC\n1IFgw5ENPDzXTkCv9QJKKV+VURFPB+BRoAIwkUuBIBZ4zYlzXwBaiUicMcYf+N0YsxAYAowRkRnG\nmElAX+CT7GYgq1KKhiQwkk4zOlEysCRzu8/VGceUUj4r3UAgIl8CXxpjuonId1k9sYgIEOf46e9Y\nBGjFpWGspwJv4MZAkPJGMCi8GyfPnWTloyspG1TWXZdXSqlcx5lK3zLGmKIictrxBN8AeFlElmV2\noDHGD1v8Uw37VrEHiBGRlPmODwFpDuRjjOkP9AcICQkhPDzciaReLS4u7rJj16+vQr78ZVkXFc7w\nm4cT81cM4X9l79y51ZV59gWaZ9/ga3l2W35FJMMF2Oz4bAvMA+oCEZkdd8U5imPnPm4O7E61viKw\nNbPjGzZsKNkVFhZ22e8+fUQCgiOlzid1sn3O3O7KPPsCzbNv8LU8X2t+gXXixD3amVZDKQPNtQem\nicgmnOyRnCrYxDgCQVOgeKrmpxVw89wGUVGQr/AJQgqHuPOySimVazlzQ99kjPkZ6AgsNMYEcSk4\npMsYU9oYU9zxPRC4C9iBDQhdHbv1wr5luE1UFCQHRhISpIFAKaXAuTqCPkBDbJHOWWNMKWxLn8yU\nA6Y66gnyAd+JyAJjzHZghjHmbWAD4Nb5DaKihISAw/pGoJRSDs7MWZxkjKmCfaIfAQTixJuEiGwG\n6qexfi9wS9aTmjMiIyGp+jENBEop5eDMEBMfAS2BlEl7zwCTXJkoV0lIgJgYA4W0aEgppVI4U0fQ\nTEQGAOcBROQkUMClqXKRkycdXwpF6RuBUko5OBMIEhyzkgmAMaYkkOzSVLlISq9iCkVRpnAZj6ZF\nKaVyi3QDQaomnhOB2UBpY8ybwO/A+25IW45L6VVMYS0aUkqpFBlVFq8BGojINGNMBNAGO97QAyKy\n1S2py2EXA0GhKEoXSmfSYqWU8jEZBYKLQ3GKyDZgm+uT41opRUMlgpPx9/P3bGKUUiqXyCgQlDbG\nDElvo4iMdkF6XCrljaBsSJ6dV0cppXJcRndEPyCIVG8GeV1UFPgFxlK2aElPJ0UppXKNjALBEREZ\n7raUuIEdZ+ikVhQrpVQqGTUf9Zo3gRSRkZAUeJwyhbTpqFJKpcgoELR2Wyrc5HhkMsmBR/WNQCml\nUkk3EDh6EHuVyMhk7VWslFJXyNK8AnmZCJyIymcDgb4RKKXURT4TCM6ehQsX8tkB5/SNQCmlLvKZ\nQJC6V7G+ESil1CU+Ewh0wDmllEqbzwSClDeCQsXPEZA/wLOJUUqpXMTnAkHpUp5Nh1JK5TY+FwjK\n6ThDSil1GZ8JBJGRQL5EypcK8nRSlFIqV/GZQBAVBabQCcoW0YpipZRKzWcCwfHIZCRQZyZTSqkr\n+UwgOHIsQYeXUEqpNPhMIDgemQyFI7UPgVJKXcFnAsHJEzrOkFJKpcVlgcAYU9EYE2aM2W6M2WaM\necaxPtgYs9QYs8vxWcJVaUiRnAynY/y1aEgppdLgyjeCROA5EakJ3AoMMsbUBIYCy0QkFFjm+O1S\n0dEgyfpGoJRSaXFZIBCRIyKy3vE9FtgBXAd0BqY6dpsKdHFVGlKkdCYrUPQ0QQW0H4FSSqXmljoC\nY0wloD6wGggRkSOOTUcBlz+ipwSCEsFJrr6UUkrlOS4fb8EYEwTMBp4VkdPGXJoKWUTEGCPpHNcf\n6A8QEhJCeHh4tq4fFxfHxo1bgZspFHg22+fJS+Li4nwin6lpnn2Dr+XZXfl1aSAwxvhjg8A3IjLH\nsfqYMaaciBwxxpQDjqd1rIhMBiYDNGrUSFq0aJGtNISHh1O27M0AVKtajOyeJy8JDw/3iXympnn2\nDb6WZ3fl15WthgzwBbBDREan2jQf6OX43guY56o0pEgpGqoQEujqSymlVJ7jyjeC24CHgS3GmI2O\nda8A7wHfGWP6AgeAbi5MAwDHjieD/zmuC3Z5S1WllMpzXBYIROR3wKSzubWrrpuWw8cuaNNRpZRK\nh0/0LLbjDOmk9UoplRafCATHI0XfCJRSKh0+EQgujjOkbwRKKXUVnwgEp6ML6MijSimVDq8PBPHx\nhgtnC+JXOJriAcU9nRyllMp1vD4QnDrlD0CREvGk7tWslFLK8plAEFxSxxlSSqm0+EwgKFNa3waU\nUiotPhAICgBQLqSAh1OilFK5k9cHgpgY23m6QkiAh1OilFK5k9cHgshoAZK5vqxOSKOUUmnxjUAQ\nGE354tqHQCml0uL1geBEjPYqVkqpjHh9IIg5ld8OOKfjDCmlVJq8PhDEni6obwRKKZUBrw8EZ08V\nwhQ+QclCJT2dFKWUypW8OhCIwPm4IAoVO0s+49VZVUqpbPPqu2NsLEiSP0WD4z2dFKWUyrW8OhBE\nRtrP4OBkzyZEKaVyMa8OBFFR9rO0jjOklFLp8upAEBkpAJQP8fdwSpRSKvfy6kBw6Mh5ACqWDfRw\nSpRSKvfy6kCw/8gZACpfV8TDKVFKqdzLqwPBoaPnIV88N5QJ9nRSlFIq1/LqQHD0eCIUjqRsEe1V\nrJRS6XFZIDDGTDHGHDfGbE21LtgYs9QYs8vxWcJV1wdHZXGhKMoU1pFHlVIqPa58I/gKuPuKdUOB\nZSISCixz/HaZSk02Q/0vKF2otCsvo5RSeZrLAoGI/AacvGJ1Z2Cq4/tUoIurrg9w3Z2LKdp8Kv5+\n2nxUKaXS4+46ghAROeL4fhRwaeH9sTPHKFHApaVPSimV5+X31IVFRIwxkt52Y0x/oD9ASEgI4eHh\nWb5G8PlgGhdtnK1j87K4uDjNsw/QPHs/t+VXRFy2AJWAral+7wTKOb6XA3Y6c56GDRtKdoWFhWX7\n2LxK8+wbNM/e71rzC6wTJ+6x7i4amg/0cnzvBcxz8/WVUkpdwZXNR78F/gBuNMYcMsb0Bd4D7jLG\n7ALaOH4rpZTyIJfVEYjIg+lsau2qayqllMo6r+5ZrJRSKnMaCJRSysdpIFBKKR+ngUAppXycBgKl\nlPJxxvY5yN2MMZHAgWweXgqIysHk5AWaZ9+gefZ+15rfG0Qk01E380QguBbGmHUi0sjT6XAnzbNv\n0Dx7P3flV4uGlFLKx2kgUEopH+cLgWCypxPgAZpn36B59n5uya/X1xEopZTKmC+8ESillMqAVwcC\nY8zdxpidxpjdxhiXzo/sKcaYKcaY48aYranWBRtjlhpjdjk+vWaaNmNMRWNMmDFmuzFmmzHmGcd6\nb85zgDFmjTFmkyPPbzrWVzbGrHb8+55pjCng6bTmNGOMnzFmgzFmgeO3V+fZGLPfGLPFGLPRGLPO\nsc7l/7a9NhAYY/yAicA9QE3gQWNMTc+myiW+Au6+Yt1QYJmIhALLHL+9RSLwnIjUBG4FBjn+rt6c\n5wtAKxGpC9QD7jbG3Aq8D4wRkWpANNDXg2l0lWeAHal++0KeW4pIvVTNRl3+b9trAwFwC7BbRPaK\nSDwwA+js4TTlOBH5DTh5xerOwFTH96lAF7cmyoVE5IiIrHd8j8XeJK7Du/MsIhLn+OnvWARoBcxy\nrPeqPAMYYyoAHYDPHb8NXp7ndLj837Y3B4LrgH9S/T7kWOcLQkTkiOP7USDEk4lxFWNMJaA+sBov\nz7OjiGQjcBxYCuwBYkQk0bGLN/77Hgu8CCQ7fpfE+/MswBJjTIRj3nZww79tj01er9xDRMQY43VN\nw4wxQcBs4FkROW0fFi1vzLOIJAH1jDHFgbnATR5OkksZYzoCx0UkwhjTwtPpcaPmIvKvMaYMsNQY\n81fqja76t+3NbwT/AhVT/a7gWOcLjhljygE4Po97OD05yhjjjw0C34jIHMdqr85zChGJAcKApkBx\nY0zKw5y3/fu+DehkjNmPLdZtBYzDu/OMiPzr+DyODfi34IZ/294cCNYCoY5WBgWAHsB8D6fJXeYD\nvRzfewHzPJiWHOUoJ/4C2CEio1Nt8uY8l3a8CWCMCQTuwtaNhAFdHbt5VZ5F5GURqSAilbD/7/4q\nIj3x4jwbYwobY4qkfAfaAltxw79tr+5QZoxpjy1n9AOmiMgIDycpxxljvgVaYEcpPAa8DvwAfAdc\njx21tZuIXFmhnCcZY5oDK4AtXCo7fgVbT+Ctea6DrST0wz68fSciw40xVbBPy8HABuAhEbnguZS6\nhqNo6HkR6ejNeXbkba7jZ35guoiMMMaUxMX/tr06ECillMqcNxcNKaWUcoIGAqWU8nEaCJRSysdp\nIFBKKR+ngUAppXycBgLls4wxSY5RHlOWHBvMyxhTKfWIsErlZjrEhPJl50SknqcToZSn6RuBUldw\njAk/0jEu/BpjTDXH+krGmF+NMZuNMcuMMdc71ocYY+Y65gvYZIxp5jiVnzHmM8ccAkscvYIxxjzt\nmE9hszFmhoeyqdRFGgiULwu8omioe6ptp0SkNvARtnc6wARgqojUAb4BxjvWjweWO+YLaABsc6wP\nBSaKSC0gBviPY/1QoL7jPANdlTmlnKU9i5XPMsbEiUhQGuv3YyeC2esY4O6oiJQ0xkQB5UQkwbH+\niIiUMsZEAhVSD3XgGCJ7qWMyEYwxLwH+IvK2MWYREIcdCuSHVHMNKOUR+kagVNokne9ZkXoMnCQu\n1cl1wM6e1wBYm2o0TaU8QgOBUmnrnurzD8f3VdiRMAF6Yge/Azt94ONwcQKZYumd1BiTD6goImHA\nS0Ax4Kq3EqXcSZ9ElC8LdMz6lWKRiKQ0IS1hjNmMfap/0LHuKeBLY8wLQCTQx7H+GWCyMaYv9sn/\nceAIafMD/ucIFgYY75hjQCmP0ToCpa7gqCNoJCJRnk6LUu6gRUNKKeXj9I1AKaV8nL4RKKWUj9NA\noJRSPk4DgVJK+TgNBEop5eM0ECillI/TQKCUUj7u/wFvVWgw3wdDpgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNRxjrBUtmZf",
        "colab_type": "text"
      },
      "source": [
        "## Test Regularization functions on Quaternion Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tCIx3dJtm__",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "05cf0bc1-1453-42f0-f736-a7157bda4eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#@markdown ##Parameters and Hyper-parameters\n",
        "n_epochs = 50 #@param {type: 'number'}\n",
        "dataset = CIFAR10 #@param [\"CIFAR10\", \"MNIST\"] {type:\"raw\"}\n",
        "use_quaternion_variant = True #@param {type: 'boolean'}\n",
        "learning_rate = 0.002 #@param {type: 'number'}\n",
        "loss_criterion = F.cross_entropy #@param [\"F.cross_entropy\", \"F.nll_loss\"] {type:\"raw\"}\n",
        "batch_size_train = 400 #@param {type: 'number'}\n",
        "batch_size_test = 1000 #@param {type: 'number'}\n",
        "use_tensorboard = False #@param {type:\"boolean\"}\n",
        "\n",
        "log_interval = 10\n",
        "globaliter = 0\n",
        "\n",
        "#set_deterministic_environment()\n",
        "\n",
        "input_expansion = 'zero_vector' if dataset == MNIST else 'rgb_vector'\n",
        "\n",
        "train_counter, train_losses, test_counter, test_losses = [], [], [], []\n",
        "\n",
        "writer = run_tensorboard_server() if use_tensorboard else None\n",
        "\n",
        "regularization_experiments = {\n",
        "    #'None' : { 'reg_factor' : 0., 'latex_legend' : r'No reg.' },\n",
        "    #'L1'   : { 'reg_factor' : 0.003  if dataset == MNIST else 0.000002, 'latex_legend' : r'$L_1$' },\n",
        "    #'L2'   : { 'reg_factor' : 0.2    if dataset == MNIST else 0.00005, 'latex_legend' : r'$L_2$' },\n",
        "    #'RQ'   : { 'reg_factor' : 0.0075  if dataset == MNIST else 0.0000053, 'latex_legend' : r'$R_Q$' },\n",
        "    'RQL'   : { 'reg_factor' : 0.0027  if dataset == MNIST else 0.0000034, 'latex_legend' : r'$R_{QL}$' }\n",
        "    \n",
        "}\n",
        "\n",
        "experiments_results = {\n",
        "  'None' : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] },\n",
        "  'L1'   : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] },\n",
        "  'L2'   : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] },\n",
        "  'RQ'   : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] },\n",
        "  'RQL'  : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] },\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "plot1, ax1 = plt.subplots()\n",
        "plot2, ax2 = plt.subplots()\n",
        "plot3, ax3 = plt.subplots()\n",
        "plot4, ax4 = plt.subplots()\n",
        "for index, (experiment_name, experiment_params) in enumerate(regularization_experiments.items()):\n",
        "  \n",
        "  regularizer = experiment_name\n",
        "  regularization_factor = experiment_params['reg_factor']\n",
        "  \n",
        "  print('\\nRegularizer: {} Reg. factor: {} '.format(experiment_name, regularization_factor))\n",
        "  \n",
        "  if dataset == MNIST:\n",
        "    network = MNISTQConvNet() if use_quaternion_variant else MNISTConvNet()\n",
        "  else:\n",
        "    network = CIFARQConvNet() if use_quaternion_variant else CIFARConvNet()\n",
        "  \n",
        "  network = network.to(device)\n",
        "  optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "  \n",
        "  print('Device used: ' + device.type)\n",
        "  print('Network variant: ' + network.network_type())\n",
        "  print('Number of trainable parameters: {}\\n'.format(count_trainable_parameters()))\n",
        "  \n",
        "  test_counter = []\n",
        "  \n",
        "  train_set, test_set = get_dataset()\n",
        "  \n",
        "  train(experiment_name, writer)\n",
        "  \n",
        "  print(experiment_name + ' weight')\n",
        "  print(experiments_results[experiment_name]['weight'])\n",
        "  print(experiment_name + ' quaternion')\n",
        "  print(experiments_results[experiment_name]['quaternion'])\n",
        "  print(experiment_name + ' accuracy')\n",
        "  print(experiments_results[experiment_name]['accuracy'])\n",
        "        \n",
        "  \n",
        "  ax1.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['weight']), color=colors[index])\n",
        "  ax2.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['quaternion']), color=colors[index])\n",
        "  ax3.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['accuracy']), color=colors[index])\n",
        "  ax4.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['neuron']), color=colors[index])\n",
        "\n",
        "labels = [item['latex_legend'] for item in regularization_experiments.values()] \n",
        "  \n",
        "ax1.legend(labels, loc='best')\n",
        "ax2.legend(labels, loc='best')\n",
        "ax3.legend(labels, loc='best')\n",
        "ax4.legend(labels, loc='best')\n",
        "ax1.set(xlabel='Number of training examples seen', ylabel='Weight Sparsity [%]')\n",
        "ax2.set(xlabel='Number of training examples seen', ylabel='Quaternion Sparsity [%]')\n",
        "ax3.set(xlabel='Number of training examples seen', ylabel='Test Accuracy [%]')\n",
        "ax4.set(xlabel='Number of training examples seen', ylabel='Neuron Sparsity [%]')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d313eaaeb2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;31m#@param {type: 'number'}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m \u001b[0;31m#@param [\"CIFAR10\", \"MNIST\"] {type:\"raw\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0muse_quaternion_variant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m#@param {type: 'boolean'}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.002\u001b[0m \u001b[0;31m#@param {type: 'number'}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_criterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m \u001b[0;31m#@param [\"F.cross_entropy\", \"F.nll_loss\"] {type:\"raw\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CIFAR10' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erWTrF2Z4QdH",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@markdown ##Parameters and Hyper-parameters\n",
        "n_epochs = 1001 #@param {type: 'number'}\n",
        "learning_rate = 0.002 #@param {type: 'number'}\n",
        "use_quaternion_variant = True #@param {type: 'boolean'}\n",
        "generation_rate = 100  # One test picture will be generated every 'generation_rate'\n",
        "use_tensorboard = False #@param {type:\"boolean\"}\n",
        "\n",
        "globaliter = 0\n",
        "  \n",
        "writer = run_tensorboard_server() if use_tensorboard else None\n",
        "\n",
        "# MANAGING PICTURES #\n",
        "empty_directory('RES')\n",
        "\n",
        "train = rgb2gray(imageio.imread('KODAK/kodim05.png'), True)\n",
        "test = imageio.imread('KODAK/kodim23.png')\n",
        "\n",
        "# Normalizing\n",
        "train = train / 255\n",
        "test = test / 255\n",
        "\n",
        "if use_quaternion_variant:\n",
        "\n",
        "    # Add a 0 as real component if quaternions (padding)\n",
        "    npad = ((0, 0), (0, 0), (1, 0))\n",
        "    train = np.pad(train, pad_width=npad, mode='constant', constant_values=0)\n",
        "    test = np.pad(test, pad_width=npad, mode='constant', constant_values=0)\n",
        "\n",
        "# Channel first\n",
        "train = np.transpose(train, (2, 0, 1))\n",
        "test = np.transpose(test, (2, 0, 1))\n",
        "\n",
        "# Add batch_size dim\n",
        "train = np.reshape(train, (1, train.shape[0], train.shape[1], train.shape[2]))\n",
        "test = np.reshape(test, (1, test.shape[0], test.shape[1], test.shape[2]))\n",
        "\n",
        "train = torch.from_numpy(train).float().to(device)\n",
        "test = torch.from_numpy(test).float().to(device)\n",
        "\n",
        "regularization_experiments = {\n",
        "    'None' : { 'reg_factor' : 0.0,     'latex_legend' : r'No reg.'  },\n",
        "    'L1'   : { 'reg_factor' : 0.0001,  'latex_legend' : r'$L_1$'    },\n",
        "    'L2'   : { 'reg_factor' : 0.005,   'latex_legend' : r'$L_2$'    },\n",
        "    'RQ'   : { 'reg_factor' : 0.0003,  'latex_legend' : r'$R_Q$'    },\n",
        "    'RQL'  : { 'reg_factor' : 0.00015, 'latex_legend' : r'$R_{QL}$' }\n",
        "}\n",
        "\n",
        "experiments_results = {\n",
        "    'None' : { 'PSNR': [], 'SSIM': [], 'weight': [], 'quaternion': [] },\n",
        "    'L1'   : { 'PSNR': [], 'SSIM': [], 'weight': [], 'quaternion': [] },\n",
        "    'L2'   : { 'PSNR': [], 'SSIM': [], 'weight': [], 'quaternion': [] },\n",
        "    'RQ'   : { 'PSNR': [], 'SSIM': [], 'weight': [], 'quaternion': [] },\n",
        "    'RQL'  : { 'PSNR': [], 'SSIM': [], 'weight': [], 'quaternion': [] },\n",
        "}\n",
        "\n",
        "plot1, ax1 = plt.subplots()\n",
        "plot2, ax2 = plt.subplots()\n",
        "plot3, ax3 = plt.subplots()\n",
        "plot4, ax4 = plt.subplots()\n",
        "\n",
        "original = imageio.imread('KODAK/kodim23.png')\n",
        "\n",
        "for index, (experiment_name, experiment_params) in enumerate(regularization_experiments.items()):\n",
        "\n",
        "    regularizer = experiment_name\n",
        "    regularization_factor = experiment_params['reg_factor']\n",
        "    test_counter = []\n",
        "\n",
        "    print('Regularizer: {} Reg. factor: {} '.format(experiment_name, regularization_factor))\n",
        "\n",
        "    network = QCAE().to(device) if use_quaternion_variant else CAE().to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
        "\n",
        "    print('Device used: ' + device.type)\n",
        "    print('Network variant: ' + network.network_type())\n",
        "    print('Number of trainable parameters: {}\\n'.format(count_trainable_parameters()))\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        globaliter += 1\n",
        "        test_counter.append(epoch)\n",
        "        compute_and_print_sparsity(experiment_name=experiment_name)\n",
        "\n",
        "        output = network(train)\n",
        "        loss = criterion(output, train) + regularization_factor * regularization(regularizer)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # If generation rate, generate a test image\n",
        "        if (epoch % generation_rate) == 0:\n",
        "            print(\"It : \" + str(epoch + 1) + \" | loss_train \" + str(loss.to('cpu').detach().numpy()))\n",
        "            output = network(test)\n",
        "            out = output.to('cpu').detach().numpy()\n",
        "            \n",
        "            out = np.transpose(out, (0, 2, 3, 1))[:, :, :, 1:] if use_quaternion_variant else np.transpose(out, (0, 2, 3, 1))\n",
        "            out = np.reshape(out, (out.shape[1], out.shape[2], out.shape[3]))\n",
        "\n",
        "            imageio.imsave('RES/save_image' + str(epoch) + experiment_name + '.png', out)\n",
        "            string_test = 'RES/save_image' + str(epoch) + experiment_name + '.png'\n",
        "            contrast = imageio.imread(string_test)\n",
        "            psnr_value = psnr(original, contrast)\n",
        "            ssim_value = ssim(original, contrast, multichannel=True)\n",
        "            print('PSNR: {}\\nSSIM: {}'.format(psnr_value, ssim_value))\n",
        "            experiments_results[experiment_name]['PSNR'].append(psnr_value)\n",
        "            experiments_results[experiment_name]['SSIM'].append(ssim_value)\n",
        "\n",
        "    compute_and_print_sparsity(experiment_name=experiment_name)\n",
        "    test_counter.append(n_epochs)\n",
        "    print('Elapsed time: {:.2f} seconds\\n'.format(time.time() - start_time))\n",
        "    \n",
        "    print(experiment_name + ' weight')\n",
        "    print(experiments_results[experiment_name]['weight'])\n",
        "    print(experiment_name + ' quaternion')\n",
        "    print(experiments_results[experiment_name]['quaternion'])\n",
        "    print(experiment_name + ' PSNR')\n",
        "    print(experiments_results[experiment_name]['PSNR'])\n",
        "    print(experiment_name + ' SSIM')\n",
        "    print(experiments_results[experiment_name]['SSIM'])\n",
        "    \n",
        "    ax1.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['weight']), color=colors[index])\n",
        "    ax2.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['quaternion']), color=colors[index])\n",
        "    ax3.plot(test_counter[::generation_rate], smooth_data(test_counter[::generation_rate], experiments_results[experiment_name]['PSNR']), color=colors[index])\n",
        "    ax4.plot(test_counter[::generation_rate], smooth_data(test_counter[::generation_rate], experiments_results[experiment_name]['SSIM']), color=colors[index])\n",
        "\n",
        "labels = [item['latex_legend'] for item in regularization_experiments.values()] \n",
        "    \n",
        "ax1.legend(labels, loc='best')\n",
        "ax2.legend(labels, loc='best')\n",
        "ax3.legend(labels, loc='best')\n",
        "ax4.legend(labels, loc='best')\n",
        "ax1.set(xlabel='Number of training examples seen', ylabel='Weight Sparsity [%]')\n",
        "ax2.set(xlabel='Number of training examples seen', ylabel='Quaternion Sparsity [%]')\n",
        "ax3.set(xlabel='Number of training examples seen', ylabel='PSNR')\n",
        "ax4.set(xlabel='Number of training examples seen', ylabel='SSIM')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeRbg0s1r_C2",
        "colab_type": "text"
      },
      "source": [
        "## Test Quaternion Batch Normalization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qxxozkwr-CU",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@markdown ##Parameters and Hyper-parameters\n",
        "n_epochs = 3 #@param {type: 'number'}\n",
        "dataset = CIFAR10 #@param [\"CIFAR10\", \"MNIST\"] {type:\"raw\"}\n",
        "learning_rate = 0.001 #@param {type: 'number'}\n",
        "loss_criterion = F.cross_entropy #@param [\"F.cross_entropy\", \"F.nll_loss\"] {type:\"raw\"}\n",
        "batch_size_train = 400 #@param {type: 'number'}\n",
        "batch_size_test = 1000 #@param {type: 'number'}\n",
        "use_tensorboard = False #@param {type:\"boolean\"}\n",
        "regularization_factor = 0.000003 #@param {type: 'number'}\n",
        "regularizer = 'None' #@param ['None', 'L1', 'L2', 'RQ', 'RQL']\n",
        "\n",
        "use_quaternion_variant = True\n",
        "\n",
        "log_interval = 10\n",
        "globaliter = 0 \n",
        "\n",
        "input_expansion = 'zero_vector' if dataset == MNIST else 'rgb_vector'\n",
        "\n",
        "train_counter, train_losses, test_counter, test_losses = [], [], [], []\n",
        "\n",
        "writer = run_tensorboard_server() if use_tensorboard else None\n",
        "\n",
        "batch_norm_experiments = {\n",
        "  'QCNN'       : { 'network': MNISTQConvNet() if dataset == MNIST else CIFARQConvNet() },\n",
        "  'QCNN + BN'  : { 'network': MNISTQConvNetBN(False) if dataset == MNIST else CIFARQConvNetBN(False) },\n",
        "  'QCNN + QBN' : { 'network': MNISTQConvNetBN(True) if dataset == MNIST else CIFARQConvNetBN(True) }\n",
        "}\n",
        "\n",
        "experiments_results = {\n",
        "  'QCNN'       : { 'accuracy' : [], 'weight' : [], 'quaternion' : [] },\n",
        "  'QCNN + BN'  : { 'accuracy' : [], 'weight' : [], 'quaternion' : [] },\n",
        "  'QCNN + QBN' : { 'accuracy' : [], 'weight' : [], 'quaternion' : [] }\n",
        "}\n",
        "\n",
        "\n",
        "for index, (experiment_name, experiment_params) in enumerate(batch_norm_experiments.items()):\n",
        "\n",
        "  print('Experiment: ' + experiment_name)\n",
        "  \n",
        "  network = experiment_params['network']\n",
        "  \n",
        "  network = network.to(device)\n",
        "  optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "  \n",
        "  print('Device used: ' + device.type)\n",
        "  print('Network variant: ' + network.network_type())\n",
        "  print('Number of trainable parameters: {}\\n'.format(count_trainable_parameters()))\n",
        "  \n",
        "  test_counter = []\n",
        "  \n",
        "  train_set, test_set = get_dataset()\n",
        "  \n",
        "  train(experiment_name, writer)\n",
        "  \n",
        "  plt.plot(test_counter, experiments_results[experiment_name]['accuracy'], color=colors[index])  \n",
        "\n",
        "plt.legend(batch_norm_experiments.keys(), loc='best')\n",
        "plt.xlabel('Number of training examples seen')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA_LYs26438x",
        "colab_type": "text"
      },
      "source": [
        "####1.   L1(gamma) vs RQ\n",
        "####2.   L1(w+gamma) vs RQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_7y4jZq409G",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "cefd68a4-aece-446e-b7f4-8b2418af56e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def compute_and_print_sparsityy(decimals=3, experiment_name=None, writer=None):\n",
        "\n",
        "    network.eval()\n",
        "  \n",
        "    nonzero_weights, nonzero_quaternions, nonzero_neurons, number_of_quaternions, number_of_neurons= 0, 0, 0, 0, 0\n",
        "    gamma_mask_list, weight_mask_list, quat_mask_list, neurons_mask_list = [], [], [], []\n",
        "    \n",
        "    for module in network.modules():\n",
        "      \n",
        "      #for param in module.parameters(recurse=False):\n",
        "      #  weight_mask = torch.ge(torch.abs(param.detach()), 10**(-decimals)).float()\n",
        "      #  weight_mask_list.append(weight_mask)\n",
        "      \n",
        "      if isinstance(module, QuaternionBatchNorm2d):\n",
        "        nonzero_gamma = torch.ge(torch.abs(module.gamma), 10**(-decimals)).float()  # number of gamma that are > 0\n",
        "        gamma_mask_list.append(nonzero_gamma)\n",
        "\n",
        "      if isinstance(module, (QuaternionConv, QuaternionLinear, QuaternionTransposeConv)):\n",
        "\n",
        "        number_of_quaternions += module.r_weight.numel()\n",
        "        number_of_neurons += module.r_weight.shape[0]\n",
        "\n",
        "        quat_norm = torch.sqrt(module.r_weight**2 + module.i_weight**2 + module.j_weight**2 + module.k_weight**2)\n",
        "             \n",
        "        weight_mask = torch.ge(torch.abs(module.r_weight), 10**(-decimals)).float() + torch.ge(torch.abs(module.i_weight), 10**(-decimals)).float() + torch.ge(torch.abs(module.j_weight), 10**(-decimals)).float() + torch.ge(torch.abs(module.k_weight), 10**(-decimals)).float()               \n",
        "        quat_mask = torch.ge(torch.abs(quat_norm), 10**(-decimals)).float()\n",
        "        quat_neurons_mask = (torch.sum(quat_mask.reshape(quat_mask.shape[0], -1), dim=1) > 0).float()\n",
        "        \n",
        "        weight_mask_list.append(weight_mask)\n",
        "        quat_mask_list.append(quat_mask)\n",
        "        neurons_mask_list.append(quat_neurons_mask)\n",
        "        \n",
        "        \n",
        "    #if len(gamma_mask_list) > 0:\n",
        "    #  gamma_mask_list.pop(0)  \n",
        "    for index in range(len((neurons_mask_list))):\n",
        "   \n",
        "      gamma_mask = gamma_mask_list.pop(0).flatten() if len(gamma_mask_list) > 0 else torch.ones(neurons_mask_list[index].shape, device=device)\n",
        "\n",
        "      if torch.sum(gamma_mask).item() == 0.:\n",
        "        break\n",
        "      else:\n",
        "        weight_mask_list[index] = torch.sum(weight_mask_list[index].reshape(weight_mask_list[index].shape[0], -1), dim=1)\n",
        "        quat_mask_list[index] = torch.sum(quat_mask_list[index].reshape(quat_mask_list[index].shape[0], -1), dim=1)\n",
        "\n",
        "        nonzero_weights += torch.sum(weight_mask_list[index]*gamma_mask).item()\n",
        "        nonzero_quaternions += torch.sum(quat_mask_list[index]*gamma_mask).item()\n",
        "        nonzero_neurons += torch.sum(neurons_mask_list[index]*gamma_mask).item()\n",
        "        \n",
        "    \n",
        "    weight_sparsity = (1 - nonzero_weights / count_trainable_parameters()) * 100\n",
        "    quaternion_sparsity = (1 - nonzero_quaternions / number_of_quaternions) * 100 if use_quaternion_variant else 0\n",
        "    neuron_sparsity = (1 - nonzero_neurons / number_of_neurons) * 100 \n",
        "           \n",
        "    if experiment_name is not None:\n",
        "      experiments_results[experiment_name]['weight'].append(weight_sparsity)\n",
        "      experiments_results[experiment_name]['quaternion'].append(quaternion_sparsity)\n",
        "      experiments_results[experiment_name]['neuron'].append(neuron_sparsity)\n",
        "\n",
        "    print('Checking sparsity...\\nWeight sparsity: {:.2f}%'.format(weight_sparsity))\n",
        "    print('Quaternion sparsity: {:.2f}%\\n'.format(quaternion_sparsity)) if use_quaternion_variant else print()\n",
        "    print('Neuron sparsity: {:.2f}%\\n'.format(neuron_sparsity))\n",
        "    \n",
        "    if use_tensorboard and writer is not None:\n",
        "      writer.add_scalars('Sparsity', { 'Weight sparsity [%]' : weight_sparsity,\n",
        "                                       'Quaternion sparsity [%]' : quaternion_sparsity }, globaliter)\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ##Parameters and Hyper-parameters\n",
        "n_epochs = 20 #@param {type: 'number'}\n",
        "dataset = MNIST #@param [\"CIFAR10\", \"MNIST\"] {type:\"raw\"}\n",
        "learning_rate = 0.001 #@param {type: 'number'}\n",
        "loss_criterion = F.cross_entropy #@param [\"F.cross_entropy\", \"F.nll_loss\"] {type:\"raw\"}\n",
        "batch_size_train = 400 #@param {type: 'number'}\n",
        "batch_size_test = 1000 #@param {type: 'number'}\n",
        "use_tensorboard = False #@param {type:\"boolean\"}\n",
        "\n",
        "use_quaternion_variant = True\n",
        "\n",
        "log_interval = 10\n",
        "globaliter = 0 \n",
        "\n",
        "#set_deterministic_environment()\n",
        "use_beta=True   \n",
        "      \n",
        "class MNISTQConvNetBNA(nn.Module):  # Quaternion CNN for MNIST\n",
        "\n",
        "    def __init__(self, use_qbn=True, gamma=1.0):\n",
        "        super(MNISTQConvNetBNA, self).__init__()\n",
        "\n",
        "        self.act_fn = F.relu\n",
        "        self.use_qbn = use_qbn\n",
        "\n",
        "        self.conv2 = QuaternionConv(4, 16, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn2 = QuaternionBatchNorm2d(16, gamma_init=gamma, beta_param=use_beta) if self.use_qbn else nn.BatchNorm2d(16)\n",
        "        self.conv3 = QuaternionConv(16, 32, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn3 = QuaternionBatchNorm2d(32, gamma_init=gamma, beta_param=use_beta) if self.use_qbn else nn.BatchNorm2d(32)\n",
        "        self.fc1 = QuaternionLinear(800, 40)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act_fn(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = self.bn2(x)\n",
        "        x = self.act_fn(F.max_pool2d(self.conv3(x), 2))\n",
        "        x = self.bn3(x)\n",
        "        x = x.view(-1, 800)\n",
        "        x = self.act_fn(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = torch.reshape(x, (-1, 10, 4))\n",
        "        x = torch.sum(torch.abs(x), dim=2)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def network_type(self):\n",
        "        return type(self).__name__\n",
        "\n",
        "\n",
        "class CIFARQConvNetBNA(nn.Module):  # Quaternion CNN for CIFAR-10\n",
        "\n",
        "    def __init__(self, use_qbn=True, gamma=1.0):\n",
        "        super(CIFARQConvNetBNA, self).__init__()\n",
        "\n",
        "        self.act_fn = F.relu\n",
        "        self.use_qbn = use_qbn\n",
        "\n",
        "        self.conv1 = QuaternionConv(4, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = QuaternionBatchNorm2d(32, gamma_init=gamma, beta_param=use_beta) if self.use_qbn else nn.BatchNorm2d(32)\n",
        "        self.conv2 = QuaternionConv(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2_drop1 = nn.Dropout2d()\n",
        "        self.bn3 = QuaternionBatchNorm2d(64, gamma_init=gamma, beta_param=use_beta) if self.use_qbn else nn.BatchNorm2d(64)\n",
        "        self.conv3 = QuaternionConv(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = QuaternionBatchNorm2d(128, gamma_init=gamma, beta_param=use_beta) if self.use_qbn else nn.BatchNorm2d(128)\n",
        "        self.conv4 = QuaternionConv(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4_drop2 = nn.Dropout2d()\n",
        "        self.bn5 = QuaternionBatchNorm2d(256, gamma_init=gamma, beta_param=use_beta) if self.use_qbn else nn.BatchNorm2d(256)\n",
        "        self.conv5 = QuaternionConv(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = QuaternionBatchNorm2d(512, gamma_init=gamma, beta_param=use_beta) if self.use_qbn else nn.BatchNorm2d(512)\n",
        "\n",
        "        self.fc1 = QuaternionLinear(8192, 40)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act_fn(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = self.bn2(x)\n",
        "        x = self.act_fn(F.max_pool2d(self.conv2_drop1(self.conv2(x)), 2))\n",
        "        x = self.bn3(x)\n",
        "        x = self.act_fn(self.conv3(x))\n",
        "        x = self.bn4(x)\n",
        "        x = self.act_fn(F.max_pool2d(self.conv4_drop2(self.conv4(x)), 2))\n",
        "        x = self.bn5(x)\n",
        "        x = self.act_fn(self.conv5(x))\n",
        "        x = self.bn6(x)\n",
        "        x = x.view(-1, 8192)\n",
        "        x = self.act_fn(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = torch.reshape(x, (-1, 10, 4))\n",
        "        x = torch.sum(torch.abs(x), dim=2)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def network_type(self):\n",
        "        return type(self).__name__\n",
        "\n",
        "\n",
        "input_expansion = 'zero_vector' if dataset == MNIST else 'rgb_vector'\n",
        "\n",
        "train_counter, train_losses, test_counter, test_losses = [], [], [], []\n",
        "\n",
        "writer = run_tensorboard_server() if use_tensorboard else None\n",
        "\n",
        "batch_norm_experiments = {\n",
        "    \n",
        "  #'No reg' : {\n",
        "  #    'network': MNISTQConvNetBNA(True, 0.4) if dataset == MNIST else CIFARQConvNetBNA(True, 0.4),\n",
        "  #    'regularizer': 'None',\n",
        "  #    'reg_factor': 0.01 if dataset == MNIST else 0.0003,#0.00015\n",
        "  #    'latex_legend' : r'$No reg., \\gamma=0.4$'\n",
        "  #},\n",
        "    \n",
        "  'L1(gamma)' : {\n",
        "      'network': MNISTQConvNetBNA(True, 0.1) if dataset == MNIST else CIFARQConvNetBNA(True, 0.1),\n",
        "      'regularizer': 'L1_gamma',\n",
        "      'reg_factor': 2.5 if dataset == MNIST else  0.09,\n",
        "      'latex_legend' : r'$L_1(\\gamma)$'\n",
        "  },\n",
        "    \n",
        "  #'L1(w+gamma)' : {\n",
        "  #    'network': MNISTQConvNetBNA(True, 0.1) if dataset == MNIST else CIFARQConvNetBNA(True, 0.1),\n",
        "  #    'regularizer': 'L1',\n",
        "  #    'reg_factor': 0.015 if dataset == MNIST else 0.00045,\n",
        "  #    'latex_legend' : r'$L_1(w+\\gamma)$'\n",
        "  #},\n",
        "    \n",
        "  #'L1(w)' : {\n",
        "  #    'network': MNISTQConvNetBNA(True, 0.1) if dataset == MNIST else CIFARQConvNetBNA(True, 0.1),\n",
        "  #    'regularizer': 'L1_without_gamma',\n",
        "  #    'reg_factor': 0.0027 if dataset == MNIST else 0.0003,\n",
        "  #    'latex_legend' : r'$L_1(w), \\gamma=0.1$'\n",
        "  #},\n",
        "  \n",
        "  #'QBN+RQ' : {\n",
        "  #    'network': MNISTQConvNetBNA(True, 0.1) if dataset == MNIST else CIFARQConvNetBNA(True, 0.1),\n",
        "  #    'regularizer': 'RQ',\n",
        "  #    'reg_factor': 0.008 if dataset == MNIST else 0.0004,\n",
        "  #     'latex_legend' : r'$QBN+R_Q$'\n",
        "  #},\n",
        "    \n",
        "  #'RQ' : {\n",
        "  #    'network': MNISTQConvNet() if dataset == MNIST else CIFARQConvNet(),\n",
        "  #    'regularizer': 'RQ',\n",
        "  #    'reg_factor': 0.0075 if dataset == MNIST else 0.0000053,\n",
        "  #    'latex_legend' : r'$R_Q$'\n",
        "  #},\n",
        "    \n",
        "  #'QBN+RQL' : {\n",
        "  #    'network': MNISTQConvNetBNA(True, 0.1) if dataset == MNIST else CIFARQConvNetBNA(True, 0.1),\n",
        "  #    'regularizer': 'RQL_without_gamma',\n",
        "  #    'reg_factor': 0.006 if dataset == MNIST else 0.00025,\n",
        "  #    'latex_legend' : r'$QBN+R_{QL}$'\n",
        "  #},\n",
        "    \n",
        "  #'RQL' : {\n",
        "  #    'network': MNISTQConvNet() if dataset == MNIST else CIFARQConvNet(),\n",
        "  #    'regularizer': 'RQL',\n",
        "  #    'reg_factor': 0.0027 if dataset == MNIST else 0.0000034,\n",
        "  #    'latex_legend' : r'$R_{QL}$'\n",
        "  #},\n",
        "    \n",
        "}\n",
        "\n",
        "experiments_results = {\n",
        "  'No reg'      : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] }, \n",
        "  'L1(gamma)'   : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] }, \n",
        "  'L1(w+gamma)' : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] }, \n",
        "  'L1(w)'       : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] }, \n",
        "  'QBN+RQ'      : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] }, \n",
        "  'RQ'          : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] }, \n",
        "  'QBN+RQL'     : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] },  \n",
        "  'RQL'         : { 'accuracy' : [], 'weight' : [], 'quaternion' : [], 'neuron' : [] } \n",
        "}\n",
        "\n",
        "\n",
        "plot1, ax1 = plt.subplots()\n",
        "plot2, ax2 = plt.subplots()\n",
        "plot3, ax3 = plt.subplots()\n",
        "plot4, ax4 = plt.subplots()\n",
        "for index, (experiment_name, experiment_params) in enumerate(batch_norm_experiments.items()):\n",
        "  \n",
        "  network = experiment_params['network']\n",
        "  regularizer = experiment_params['regularizer']\n",
        "  regularization_factor = experiment_params['reg_factor']\n",
        "  \n",
        "  print('\\nRegularizer: {} Reg. factor: {} '.format(experiment_name, regularization_factor))\n",
        "   \n",
        "  network = network.to(device)\n",
        "  optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "  \n",
        "  print('Device used: ' + device.type)\n",
        "  print('Network variant: ' + network.network_type())\n",
        "  print('Number of trainable parameters: {}\\n'.format(count_trainable_parameters()))\n",
        "  \n",
        "  test_counter = []\n",
        "  \n",
        "  train_set, test_set = get_dataset()\n",
        "  \n",
        "  train(experiment_name, writer)\n",
        "  \n",
        "  print(experiment_name + ' weight')\n",
        "  print(experiments_results[experiment_name]['weight'])\n",
        "  print(experiment_name + ' quaternion')\n",
        "  print(experiments_results[experiment_name]['quaternion'])\n",
        "  print(experiment_name + ' neuron')\n",
        "  print(experiments_results[experiment_name]['neuron'])\n",
        "  print(experiment_name + ' accuracy')\n",
        "  print(experiments_results[experiment_name]['accuracy'])\n",
        "  \n",
        "  ax1.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['weight']), color=colors[index])\n",
        "  ax2.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['quaternion']), color=colors[index])\n",
        "  ax3.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['accuracy']), color=colors[index])\n",
        "  ax4.plot(test_counter, smooth_data(test_counter, experiments_results[experiment_name]['neuron']), color=colors[index])\n",
        "\n",
        "labels = [item['latex_legend'] for item in batch_norm_experiments.values()]\n",
        "\n",
        "ax1.legend(labels, loc='best')\n",
        "ax2.legend(labels, loc='best')\n",
        "ax3.legend(labels, loc='best')\n",
        "ax4.legend(labels, loc='best')\n",
        "\n",
        "ax1.set(xlabel='Number of training examples seen', ylabel='Weight Sparsity [%]')\n",
        "ax2.set(xlabel='Number of training examples seen', ylabel='Quaternion Sparsity [%]')\n",
        "ax3.set(xlabel='Number of training examples seen', ylabel='Test Accuracy [%]')\n",
        "ax4.set(xlabel='Number of training examples seen', ylabel='Neuron Sparsity [%]')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Regularizer: L1(gamma) Reg. factor: 2.5 \n",
            "Device used: cuda\n",
            "Network variant: MNISTQConvNetBNA\n",
            "Number of trainable parameters: 11748\n",
            "\n",
            "Retrieve MNIST dataset...\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 6630285.54it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 321513.43it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /files/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting /files/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5213121.95it/s]                           \n",
            "8192it [00:00, 129172.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "Start training from MNIST training set to generate the model...\n",
            "Epochs: 20\n",
            "Learning rate: 0.001\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 2.3008, Accuracy: 1328/10000 (13.28%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 18.58%\n",
            "Quaternion sparsity: 1.72%\n",
            "\n",
            "Neuron sparsity: 0.00%\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302015\n",
            "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.109522\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.891800\n",
            "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.773306\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.615102\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.568501\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.554435\n",
            "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.606589\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.504342\n",
            "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.533400\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.640430\n",
            "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 1.598245\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.493845\n",
            "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 1.353650\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.306643\n",
            "\n",
            "Test set: Avg. loss: 1.0166, Accuracy: 8828/10000 (88.28%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 9.09%\n",
            "Quaternion sparsity: 6.90%\n",
            "\n",
            "Neuron sparsity: 0.94%\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.229887\n",
            "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 1.165643\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 1.025324\n",
            "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 1.038290\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.993720\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.968980\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.895662\n",
            "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.747749\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.822503\n",
            "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.821553\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.812136\n",
            "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.720595\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.701761\n",
            "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.721376\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.731397\n",
            "\n",
            "Test set: Avg. loss: 0.3665, Accuracy: 9337/10000 (93.37%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.40%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.692548\n",
            "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.629332\n",
            "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.725801\n",
            "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.779650\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.675449\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.645346\n",
            "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.660779\n",
            "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.640319\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.656981\n",
            "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.607231\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.642418\n",
            "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.688286\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.598060\n",
            "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.606720\n",
            "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.589517\n",
            "\n",
            "Test set: Avg. loss: 0.3062, Accuracy: 9425/10000 (94.25%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.39%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.519240\n",
            "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.561167\n",
            "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.684477\n",
            "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.614072\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.590533\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.551365\n",
            "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.656694\n",
            "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.557136\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.594598\n",
            "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.570214\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.518244\n",
            "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.652063\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.670088\n",
            "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.523697\n",
            "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.650099\n",
            "\n",
            "Test set: Avg. loss: 0.2644, Accuracy: 9489/10000 (94.89%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.44%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.633325\n",
            "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.545036\n",
            "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.511403\n",
            "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.544624\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.596536\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.561491\n",
            "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.530509\n",
            "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.588022\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.560496\n",
            "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.533193\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.515976\n",
            "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.491518\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.569459\n",
            "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.676438\n",
            "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.507633\n",
            "\n",
            "Test set: Avg. loss: 0.2612, Accuracy: 9479/10000 (94.79%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.36%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.626915\n",
            "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.529627\n",
            "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.538310\n",
            "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.609706\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.585440\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.579947\n",
            "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.487261\n",
            "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.552317\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.476594\n",
            "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.545778\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.519901\n",
            "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.570100\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.577287\n",
            "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.544864\n",
            "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.534582\n",
            "\n",
            "Test set: Avg. loss: 0.2188, Accuracy: 9544/10000 (95.44%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.35%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.579658\n",
            "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.461776\n",
            "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.463877\n",
            "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.538156\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.524549\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.558333\n",
            "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.473223\n",
            "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.513187\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.461143\n",
            "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.474770\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.515510\n",
            "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.541346\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.564848\n",
            "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.533386\n",
            "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.556191\n",
            "\n",
            "Test set: Avg. loss: 0.2151, Accuracy: 9539/10000 (95.39%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.35%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.575908\n",
            "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.564425\n",
            "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.462671\n",
            "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.547597\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.554485\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.531814\n",
            "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.586804\n",
            "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.532065\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.472795\n",
            "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.560591\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.492235\n",
            "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.501799\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.470907\n",
            "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.517482\n",
            "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.510139\n",
            "\n",
            "Test set: Avg. loss: 0.2045, Accuracy: 9556/10000 (95.56%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.36%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.449739\n",
            "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.466222\n",
            "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.532184\n",
            "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.442766\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.497318\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.486931\n",
            "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.455635\n",
            "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.424011\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.504526\n",
            "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.434472\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.483729\n",
            "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.421027\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.499405\n",
            "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.444625\n",
            "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.569847\n",
            "\n",
            "Test set: Avg. loss: 0.1954, Accuracy: 9566/10000 (95.66%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.46%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.410701\n",
            "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 0.456054\n",
            "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 0.417906\n",
            "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 0.566048\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.547754\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.485587\n",
            "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 0.424987\n",
            "Train Epoch: 10 [28000/60000 (47%)]\tLoss: 0.424045\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.465901\n",
            "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 0.455385\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.458269\n",
            "Train Epoch: 10 [44000/60000 (73%)]\tLoss: 0.573135\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.510537\n",
            "Train Epoch: 10 [52000/60000 (87%)]\tLoss: 0.463002\n",
            "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 0.580930\n",
            "\n",
            "Test set: Avg. loss: 0.2024, Accuracy: 9554/10000 (95.54%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.40%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.476831\n",
            "Train Epoch: 11 [4000/60000 (7%)]\tLoss: 0.478417\n",
            "Train Epoch: 11 [8000/60000 (13%)]\tLoss: 0.462164\n",
            "Train Epoch: 11 [12000/60000 (20%)]\tLoss: 0.510932\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.463173\n",
            "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.559076\n",
            "Train Epoch: 11 [24000/60000 (40%)]\tLoss: 0.447286\n",
            "Train Epoch: 11 [28000/60000 (47%)]\tLoss: 0.462813\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.455502\n",
            "Train Epoch: 11 [36000/60000 (60%)]\tLoss: 0.484908\n",
            "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.531748\n",
            "Train Epoch: 11 [44000/60000 (73%)]\tLoss: 0.404698\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.547193\n",
            "Train Epoch: 11 [52000/60000 (87%)]\tLoss: 0.449010\n",
            "Train Epoch: 11 [56000/60000 (93%)]\tLoss: 0.443154\n",
            "\n",
            "Test set: Avg. loss: 0.1865, Accuracy: 9566/10000 (95.66%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.42%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.507010\n",
            "Train Epoch: 12 [4000/60000 (7%)]\tLoss: 0.562268\n",
            "Train Epoch: 12 [8000/60000 (13%)]\tLoss: 0.570252\n",
            "Train Epoch: 12 [12000/60000 (20%)]\tLoss: 0.535640\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.475806\n",
            "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.469532\n",
            "Train Epoch: 12 [24000/60000 (40%)]\tLoss: 0.526762\n",
            "Train Epoch: 12 [28000/60000 (47%)]\tLoss: 0.455836\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.566014\n",
            "Train Epoch: 12 [36000/60000 (60%)]\tLoss: 0.449749\n",
            "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.423765\n",
            "Train Epoch: 12 [44000/60000 (73%)]\tLoss: 0.409800\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.430709\n",
            "Train Epoch: 12 [52000/60000 (87%)]\tLoss: 0.369814\n",
            "Train Epoch: 12 [56000/60000 (93%)]\tLoss: 0.518302\n",
            "\n",
            "Test set: Avg. loss: 0.2115, Accuracy: 9567/10000 (95.67%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.46%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.489374\n",
            "Train Epoch: 13 [4000/60000 (7%)]\tLoss: 0.495846\n",
            "Train Epoch: 13 [8000/60000 (13%)]\tLoss: 0.490918\n",
            "Train Epoch: 13 [12000/60000 (20%)]\tLoss: 0.496969\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.457114\n",
            "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.460158\n",
            "Train Epoch: 13 [24000/60000 (40%)]\tLoss: 0.444087\n",
            "Train Epoch: 13 [28000/60000 (47%)]\tLoss: 0.444157\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.501391\n",
            "Train Epoch: 13 [36000/60000 (60%)]\tLoss: 0.483529\n",
            "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.484816\n",
            "Train Epoch: 13 [44000/60000 (73%)]\tLoss: 0.465713\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.460642\n",
            "Train Epoch: 13 [52000/60000 (87%)]\tLoss: 0.458232\n",
            "Train Epoch: 13 [56000/60000 (93%)]\tLoss: 0.391370\n",
            "\n",
            "Test set: Avg. loss: 0.1707, Accuracy: 9609/10000 (96.09%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.39%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.354093\n",
            "Train Epoch: 14 [4000/60000 (7%)]\tLoss: 0.446668\n",
            "Train Epoch: 14 [8000/60000 (13%)]\tLoss: 0.479570\n",
            "Train Epoch: 14 [12000/60000 (20%)]\tLoss: 0.495903\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.455803\n",
            "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.355986\n",
            "Train Epoch: 14 [24000/60000 (40%)]\tLoss: 0.471667\n",
            "Train Epoch: 14 [28000/60000 (47%)]\tLoss: 0.530735\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.446489\n",
            "Train Epoch: 14 [36000/60000 (60%)]\tLoss: 0.523039\n",
            "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.448171\n",
            "Train Epoch: 14 [44000/60000 (73%)]\tLoss: 0.418581\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.537896\n",
            "Train Epoch: 14 [52000/60000 (87%)]\tLoss: 0.362330\n",
            "Train Epoch: 14 [56000/60000 (93%)]\tLoss: 0.457453\n",
            "\n",
            "Test set: Avg. loss: 0.1873, Accuracy: 9592/10000 (95.92%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.32%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.447069\n",
            "Train Epoch: 15 [4000/60000 (7%)]\tLoss: 0.439649\n",
            "Train Epoch: 15 [8000/60000 (13%)]\tLoss: 0.451564\n",
            "Train Epoch: 15 [12000/60000 (20%)]\tLoss: 0.496935\n",
            "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 0.454155\n",
            "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.457524\n",
            "Train Epoch: 15 [24000/60000 (40%)]\tLoss: 0.470987\n",
            "Train Epoch: 15 [28000/60000 (47%)]\tLoss: 0.473368\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.447332\n",
            "Train Epoch: 15 [36000/60000 (60%)]\tLoss: 0.439373\n",
            "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.379881\n",
            "Train Epoch: 15 [44000/60000 (73%)]\tLoss: 0.442252\n",
            "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 0.457366\n",
            "Train Epoch: 15 [52000/60000 (87%)]\tLoss: 0.485482\n",
            "Train Epoch: 15 [56000/60000 (93%)]\tLoss: 0.411859\n",
            "\n",
            "Test set: Avg. loss: 0.1803, Accuracy: 9608/10000 (96.08%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.39%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.480215\n",
            "Train Epoch: 16 [4000/60000 (7%)]\tLoss: 0.491764\n",
            "Train Epoch: 16 [8000/60000 (13%)]\tLoss: 0.401125\n",
            "Train Epoch: 16 [12000/60000 (20%)]\tLoss: 0.567643\n",
            "Train Epoch: 16 [16000/60000 (27%)]\tLoss: 0.513598\n",
            "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.440078\n",
            "Train Epoch: 16 [24000/60000 (40%)]\tLoss: 0.461896\n",
            "Train Epoch: 16 [28000/60000 (47%)]\tLoss: 0.528128\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.460995\n",
            "Train Epoch: 16 [36000/60000 (60%)]\tLoss: 0.473257\n",
            "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.458706\n",
            "Train Epoch: 16 [44000/60000 (73%)]\tLoss: 0.429787\n",
            "Train Epoch: 16 [48000/60000 (80%)]\tLoss: 0.417023\n",
            "Train Epoch: 16 [52000/60000 (87%)]\tLoss: 0.449454\n",
            "Train Epoch: 16 [56000/60000 (93%)]\tLoss: 0.450515\n",
            "\n",
            "Test set: Avg. loss: 0.1901, Accuracy: 9550/10000 (95.50%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.46%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.381239\n",
            "Train Epoch: 17 [4000/60000 (7%)]\tLoss: 0.435971\n",
            "Train Epoch: 17 [8000/60000 (13%)]\tLoss: 0.442964\n",
            "Train Epoch: 17 [12000/60000 (20%)]\tLoss: 0.435629\n",
            "Train Epoch: 17 [16000/60000 (27%)]\tLoss: 0.481177\n",
            "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.448366\n",
            "Train Epoch: 17 [24000/60000 (40%)]\tLoss: 0.399038\n",
            "Train Epoch: 17 [28000/60000 (47%)]\tLoss: 0.444125\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.399092\n",
            "Train Epoch: 17 [36000/60000 (60%)]\tLoss: 0.512395\n",
            "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.445972\n",
            "Train Epoch: 17 [44000/60000 (73%)]\tLoss: 0.423107\n",
            "Train Epoch: 17 [48000/60000 (80%)]\tLoss: 0.498958\n",
            "Train Epoch: 17 [52000/60000 (87%)]\tLoss: 0.388925\n",
            "Train Epoch: 17 [56000/60000 (93%)]\tLoss: 0.434542\n",
            "\n",
            "Test set: Avg. loss: 0.1774, Accuracy: 9581/10000 (95.81%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.43%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.391055\n",
            "Train Epoch: 18 [4000/60000 (7%)]\tLoss: 0.465508\n",
            "Train Epoch: 18 [8000/60000 (13%)]\tLoss: 0.469399\n",
            "Train Epoch: 18 [12000/60000 (20%)]\tLoss: 0.421803\n",
            "Train Epoch: 18 [16000/60000 (27%)]\tLoss: 0.418113\n",
            "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.505764\n",
            "Train Epoch: 18 [24000/60000 (40%)]\tLoss: 0.405972\n",
            "Train Epoch: 18 [28000/60000 (47%)]\tLoss: 0.421616\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.407727\n",
            "Train Epoch: 18 [36000/60000 (60%)]\tLoss: 0.442995\n",
            "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.480733\n",
            "Train Epoch: 18 [44000/60000 (73%)]\tLoss: 0.499258\n",
            "Train Epoch: 18 [48000/60000 (80%)]\tLoss: 0.426581\n",
            "Train Epoch: 18 [52000/60000 (87%)]\tLoss: 0.419454\n",
            "Train Epoch: 18 [56000/60000 (93%)]\tLoss: 0.476318\n",
            "\n",
            "Test set: Avg. loss: 0.1716, Accuracy: 9611/10000 (96.11%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.39%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.490516\n",
            "Train Epoch: 19 [4000/60000 (7%)]\tLoss: 0.509138\n",
            "Train Epoch: 19 [8000/60000 (13%)]\tLoss: 0.438401\n",
            "Train Epoch: 19 [12000/60000 (20%)]\tLoss: 0.428778\n",
            "Train Epoch: 19 [16000/60000 (27%)]\tLoss: 0.484229\n",
            "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.428244\n",
            "Train Epoch: 19 [24000/60000 (40%)]\tLoss: 0.456656\n",
            "Train Epoch: 19 [28000/60000 (47%)]\tLoss: 0.493053\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.415339\n",
            "Train Epoch: 19 [36000/60000 (60%)]\tLoss: 0.398261\n",
            "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.435046\n",
            "Train Epoch: 19 [44000/60000 (73%)]\tLoss: 0.383325\n",
            "Train Epoch: 19 [48000/60000 (80%)]\tLoss: 0.425326\n",
            "Train Epoch: 19 [52000/60000 (87%)]\tLoss: 0.474475\n",
            "Train Epoch: 19 [56000/60000 (93%)]\tLoss: 0.346168\n",
            "\n",
            "Test set: Avg. loss: 0.1618, Accuracy: 9606/10000 (96.06%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.37%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.406427\n",
            "Train Epoch: 20 [4000/60000 (7%)]\tLoss: 0.443211\n",
            "Train Epoch: 20 [8000/60000 (13%)]\tLoss: 0.490473\n",
            "Train Epoch: 20 [12000/60000 (20%)]\tLoss: 0.476291\n",
            "Train Epoch: 20 [16000/60000 (27%)]\tLoss: 0.369778\n",
            "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 0.420691\n",
            "Train Epoch: 20 [24000/60000 (40%)]\tLoss: 0.439712\n",
            "Train Epoch: 20 [28000/60000 (47%)]\tLoss: 0.447420\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.390803\n",
            "Train Epoch: 20 [36000/60000 (60%)]\tLoss: 0.510742\n",
            "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 0.446289\n",
            "Train Epoch: 20 [44000/60000 (73%)]\tLoss: 0.465400\n",
            "Train Epoch: 20 [48000/60000 (80%)]\tLoss: 0.367244\n",
            "Train Epoch: 20 [52000/60000 (87%)]\tLoss: 0.397124\n",
            "Train Epoch: 20 [56000/60000 (93%)]\tLoss: 0.350356\n",
            "\n",
            "Test set: Avg. loss: 0.1665, Accuracy: 9624/10000 (96.24%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 26.40%\n",
            "Quaternion sparsity: 25.00%\n",
            "\n",
            "Neuron sparsity: 3.77%\n",
            "\n",
            "Elapsed time: 193.40 seconds\n",
            "\n",
            "L1(gamma) weight\n",
            "[18.58188627851549, 9.090909090909093, 26.3959822948587, 26.38747020769493, 26.43854273067756, 26.361933946203607, 26.353421859039837, 26.353421859039837, 26.361933946203607, 26.464078992168883, 26.3959822948587, 26.421518556350023, 26.464078992168883, 26.38747020769493, 26.319373510384747, 26.38747020769493, 26.464078992168883, 26.430030643513792, 26.38747020769493, 26.37044603336738, 26.3959822948587]\n",
            "L1(gamma) quaternion\n",
            "[1.7241379310344862, 6.896551724137934, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0]\n",
            "L1(gamma) neuron\n",
            "[0.0, 0.9433962264150941, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765, 3.7735849056603765]\n",
            "L1(gamma) accuracy\n",
            "[13.28, 88.28, 93.37, 94.25, 94.89, 94.79, 95.44, 95.39, 95.56, 95.66, 95.54, 95.66, 95.67, 96.09, 95.92, 96.08, 95.5, 95.81, 96.11, 96.06, 96.24]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npaQgIiJZSgoiIpKlpCAiIln/H8Q2yrSBvqcRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_g86MOmzr9a",
        "colab_type": "text"
      },
      "source": [
        "## Test Deep Compression pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygMFdp4Q5hRP",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "70fb3a62-fbc6-4a32-cefd-4f753a1b1225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@markdown ##Parameters and Hyper-parameters\n",
        "\n",
        "# PARAMETERS #\n",
        "\n",
        "dataset = MNIST #@param [\"CIFAR10\", \"MNIST\"] {type:\"raw\"}\n",
        "plot_curve = True #@param {type: 'boolean'}\n",
        "use_tensorboard = False #@param {type:\"boolean\"}\n",
        "log_interval = 10\n",
        "\n",
        "# HYPER PARAMETERS #\n",
        "n_epochs = 20 #@param {type: 'number'}\n",
        "learning_rate = 0.002 #@param {type: 'number'}\n",
        "loss_criterion = F.cross_entropy #@param [\"F.cross_entropy\", \"F.nll_loss\"] {type:\"raw\"}\n",
        "batch_size_train = 400 #@param {type: 'number'}\n",
        "batch_size_test = 1000 #@param {type: 'number'}\n",
        "regularization_factor = 0.0075 #@param {type: 'number'}\n",
        "regularizer = 'RQ' #@param ['None', 'L1', 'L2', 'RQ', 'RQL']\n",
        "\n",
        "use_quaternion_variant = True\n",
        "input_expansion = 'zero_vector' if dataset == MNIST else 'rgb_vector'\n",
        "\n",
        "codebook = {\n",
        "            'masks': [],\n",
        "            'centroids': [],\n",
        "            'labels': []\n",
        "           }\n",
        "\n",
        "\n",
        "def compress_network(use_quaternion_quantization=True, weight_sharing_bits=2, lr_tuning=0.0001):\n",
        "  \n",
        "  network.eval()\n",
        "  \n",
        "  torch.save(network.state_dict(), '../' + network.network_type() + '_1_after_training.pt')\n",
        "\n",
        "  prune()\n",
        "  test(n_epochs + 1)\n",
        "  torch.save(network.state_dict(), '../' + network.network_type() + '_2_after_pruning.pt')\n",
        "  \n",
        "  if use_quaternion_quantization == True:\n",
        "    apply_quaternion_weight_sharing(bits=weight_sharing_bits)\n",
        "  else:\n",
        "    apply_weight_sharing(bits=weight_sharing_bits)\n",
        "    \n",
        "  test(n_epochs + 2)\n",
        "  torch.save(network.state_dict(), '../' + network.network_type() + '_3_after_weight_sharing.pt')\n",
        "  \n",
        "  start = time.time()\n",
        "  fine_tuning_centroids(learning_rate_tuning=lr_tuning)\n",
        "  print('elapsed time: ' + str(time.time()-start) + ' seconds')\n",
        "  save_codebook()\n",
        "  \n",
        "  # huffman_encoding()\n",
        "  # torch.save(network.state_dict(), '../' + network.network_type() + '_4_after_huffman.pt')\n",
        "\n",
        "\n",
        "print('\\n*** Quaternion NNs Compression Experiments ***\\n')\n",
        "\n",
        "writer = run_tensorboard_server() if use_tensorboard else None\n",
        "\n",
        "if dataset == MNIST:\n",
        "  network = MNISTQConvNet()\n",
        "else:\n",
        "  network = CIFARQConvNet()\n",
        "        \n",
        "network = network.to(device)\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "\n",
        "print('Device used: ' + device.type)\n",
        "print('Network variant: ' + network.network_type())\n",
        "print('Number of trainable parameters: {}\\n'.format(count_trainable_parameters()))\n",
        "\n",
        "train_set, test_set = get_dataset()\n",
        "\n",
        "train_counter, train_losses, test_counter, test_losses = [], [], [], []\n",
        "globaliter = 0\n",
        "\n",
        "if use_tensorboard:\n",
        "  writer = run_tensorboard_server()\n",
        "  \n",
        "train(writer=writer)\n",
        "compress_network()\n",
        "\n",
        "sample_data, sample_targets = next(iter(test_set))\n",
        "print('Evaluation of a random sample: {}'.format(inference(sample_data[0])))\n",
        "show_image(sample_data[0], sample_targets[0])  # Show a random image from the test set\n",
        "\n",
        "#if plot_curve:\n",
        "#    plot_training_curve()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*** Quaternion NNs Compression Experiments ***\n",
            "\n",
            "Device used: cuda\n",
            "Network variant: MNISTQConvNet\n",
            "Number of trainable parameters: 11688\n",
            "\n",
            "Retrieve MNIST dataset...\n",
            "\n",
            "\n",
            "Start training from MNIST training set to generate the model...\n",
            "Epochs: 20\n",
            "Learning rate: 0.002\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 2.3021, Accuracy: 979/10000 (9.79%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 18.30%\n",
            "Quaternion sparsity: 1.72%\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302392\n",
            "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.226390\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.572095\n",
            "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.108619\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.975312\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.900868\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.815938\n",
            "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.777590\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.849524\n",
            "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.711237\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.829411\n",
            "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.562371\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.575570\n",
            "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.576297\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.628470\n",
            "\n",
            "Test set: Avg. loss: 0.2509, Accuracy: 9431/10000 (94.31%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 47.69%\n",
            "Quaternion sparsity: 36.59%\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.576562\n",
            "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.518172\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.618587\n",
            "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.567167\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.563811\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.563749\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.547062\n",
            "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.587415\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.401565\n",
            "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.544545\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.547417\n",
            "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.482063\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.523961\n",
            "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.552940\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.573291\n",
            "\n",
            "Test set: Avg. loss: 0.1797, Accuracy: 9591/10000 (95.91%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 58.61%\n",
            "Quaternion sparsity: 52.45%\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.465605\n",
            "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.528043\n",
            "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.518062\n",
            "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.451238\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.477453\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.463436\n",
            "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.458997\n",
            "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.472203\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.482687\n",
            "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.420997\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.458873\n",
            "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.464352\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.445141\n",
            "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.491734\n",
            "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.512419\n",
            "\n",
            "Test set: Avg. loss: 0.1728, Accuracy: 9591/10000 (95.91%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 59.92%\n",
            "Quaternion sparsity: 52.55%\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.491225\n",
            "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.341621\n",
            "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.406631\n",
            "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.387606\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.453695\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.441597\n",
            "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.441541\n",
            "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.432027\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.404857\n",
            "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.574479\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.465525\n",
            "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.501036\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.480906\n",
            "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.514001\n",
            "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.459258\n",
            "\n",
            "Test set: Avg. loss: 0.1638, Accuracy: 9602/10000 (96.02%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 60.39%\n",
            "Quaternion sparsity: 49.17%\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.431640\n",
            "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.499048\n",
            "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.355339\n",
            "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.457865\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.425111\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.526430\n",
            "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.492239\n",
            "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.453438\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.409220\n",
            "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.472713\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.512397\n",
            "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.520706\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.418365\n",
            "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.397040\n",
            "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.431031\n",
            "\n",
            "Test set: Avg. loss: 0.1780, Accuracy: 9578/10000 (95.78%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 62.07%\n",
            "Quaternion sparsity: 51.07%\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.387973\n",
            "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.478305\n",
            "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.398032\n",
            "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.471325\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.519291\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.402818\n",
            "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.429959\n",
            "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.477208\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.432687\n",
            "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.426383\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.463097\n",
            "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.440167\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.341618\n",
            "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.422068\n",
            "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.514061\n",
            "\n",
            "Test set: Avg. loss: 0.1529, Accuracy: 9628/10000 (96.28%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 49.30%\n",
            "Quaternion sparsity: 34.76%\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.391594\n",
            "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.346608\n",
            "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.439137\n",
            "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.410010\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.522344\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.426305\n",
            "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.484603\n",
            "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.445767\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.388921\n",
            "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.458484\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.449074\n",
            "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.463340\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.443461\n",
            "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.404765\n",
            "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.447111\n",
            "\n",
            "Test set: Avg. loss: 0.1579, Accuracy: 9622/10000 (96.22%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 66.14%\n",
            "Quaternion sparsity: 57.55%\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.376395\n",
            "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.526868\n",
            "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.431421\n",
            "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.539906\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.450200\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.417405\n",
            "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.449569\n",
            "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.378914\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.378267\n",
            "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.405687\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.478933\n",
            "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.332269\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.481053\n",
            "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.421069\n",
            "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.485034\n",
            "\n",
            "Test set: Avg. loss: 0.1558, Accuracy: 9625/10000 (96.25%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 67.56%\n",
            "Quaternion sparsity: 59.97%\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.465102\n",
            "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.505853\n",
            "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.505788\n",
            "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.505090\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.472223\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.441757\n",
            "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.393452\n",
            "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.556369\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.531885\n",
            "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.419726\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.430339\n",
            "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.402952\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.465760\n",
            "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.501577\n",
            "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.383971\n",
            "\n",
            "Test set: Avg. loss: 0.1475, Accuracy: 9618/10000 (96.18%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 65.74%\n",
            "Quaternion sparsity: 56.90%\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.409294\n",
            "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 0.437183\n",
            "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 0.496890\n",
            "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 0.364421\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.462462\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.486125\n",
            "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 0.441747\n",
            "Train Epoch: 10 [28000/60000 (47%)]\tLoss: 0.434556\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.488304\n",
            "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 0.433570\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.401277\n",
            "Train Epoch: 10 [44000/60000 (73%)]\tLoss: 0.417755\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.565120\n",
            "Train Epoch: 10 [52000/60000 (87%)]\tLoss: 0.477173\n",
            "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 0.406609\n",
            "\n",
            "Test set: Avg. loss: 0.1508, Accuracy: 9621/10000 (96.21%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 68.14%\n",
            "Quaternion sparsity: 59.90%\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.375172\n",
            "Train Epoch: 11 [4000/60000 (7%)]\tLoss: 0.507082\n",
            "Train Epoch: 11 [8000/60000 (13%)]\tLoss: 0.472110\n",
            "Train Epoch: 11 [12000/60000 (20%)]\tLoss: 0.504283\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.367017\n",
            "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.362305\n",
            "Train Epoch: 11 [24000/60000 (40%)]\tLoss: 0.468668\n",
            "Train Epoch: 11 [28000/60000 (47%)]\tLoss: 0.472934\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.525046\n",
            "Train Epoch: 11 [36000/60000 (60%)]\tLoss: 0.365031\n",
            "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.464729\n",
            "Train Epoch: 11 [44000/60000 (73%)]\tLoss: 0.432861\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.520849\n",
            "Train Epoch: 11 [52000/60000 (87%)]\tLoss: 0.390641\n",
            "Train Epoch: 11 [56000/60000 (93%)]\tLoss: 0.466751\n",
            "\n",
            "Test set: Avg. loss: 0.1541, Accuracy: 9611/10000 (96.11%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 69.28%\n",
            "Quaternion sparsity: 62.72%\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.452754\n",
            "Train Epoch: 12 [4000/60000 (7%)]\tLoss: 0.563085\n",
            "Train Epoch: 12 [8000/60000 (13%)]\tLoss: 0.340213\n",
            "Train Epoch: 12 [12000/60000 (20%)]\tLoss: 0.391862\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.478082\n",
            "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.404260\n",
            "Train Epoch: 12 [24000/60000 (40%)]\tLoss: 0.448993\n",
            "Train Epoch: 12 [28000/60000 (47%)]\tLoss: 0.445846\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.474902\n",
            "Train Epoch: 12 [36000/60000 (60%)]\tLoss: 0.346001\n",
            "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.383529\n",
            "Train Epoch: 12 [44000/60000 (73%)]\tLoss: 0.488258\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.457573\n",
            "Train Epoch: 12 [52000/60000 (87%)]\tLoss: 0.403557\n",
            "Train Epoch: 12 [56000/60000 (93%)]\tLoss: 0.389055\n",
            "\n",
            "Test set: Avg. loss: 0.1480, Accuracy: 9635/10000 (96.35%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 68.75%\n",
            "Quaternion sparsity: 55.03%\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.474507\n",
            "Train Epoch: 13 [4000/60000 (7%)]\tLoss: 0.418073\n",
            "Train Epoch: 13 [8000/60000 (13%)]\tLoss: 0.447843\n",
            "Train Epoch: 13 [12000/60000 (20%)]\tLoss: 0.500699\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.520586\n",
            "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.551540\n",
            "Train Epoch: 13 [24000/60000 (40%)]\tLoss: 0.358960\n",
            "Train Epoch: 13 [28000/60000 (47%)]\tLoss: 0.379486\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.453649\n",
            "Train Epoch: 13 [36000/60000 (60%)]\tLoss: 0.468329\n",
            "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.436271\n",
            "Train Epoch: 13 [44000/60000 (73%)]\tLoss: 0.398387\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.337980\n",
            "Train Epoch: 13 [52000/60000 (87%)]\tLoss: 0.353279\n",
            "Train Epoch: 13 [56000/60000 (93%)]\tLoss: 0.472399\n",
            "\n",
            "Test set: Avg. loss: 0.1643, Accuracy: 9552/10000 (95.52%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 70.13%\n",
            "Quaternion sparsity: 62.62%\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.492597\n",
            "Train Epoch: 14 [4000/60000 (7%)]\tLoss: 0.458225\n",
            "Train Epoch: 14 [8000/60000 (13%)]\tLoss: 0.462769\n",
            "Train Epoch: 14 [12000/60000 (20%)]\tLoss: 0.372914\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.437341\n",
            "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.342919\n",
            "Train Epoch: 14 [24000/60000 (40%)]\tLoss: 0.474838\n",
            "Train Epoch: 14 [28000/60000 (47%)]\tLoss: 0.559737\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.468994\n",
            "Train Epoch: 14 [36000/60000 (60%)]\tLoss: 0.476779\n",
            "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.399590\n",
            "Train Epoch: 14 [44000/60000 (73%)]\tLoss: 0.436444\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.443463\n",
            "Train Epoch: 14 [52000/60000 (87%)]\tLoss: 0.353505\n",
            "Train Epoch: 14 [56000/60000 (93%)]\tLoss: 0.365454\n",
            "\n",
            "Test set: Avg. loss: 0.1573, Accuracy: 9618/10000 (96.18%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 53.13%\n",
            "Quaternion sparsity: 42.79%\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.443675\n",
            "Train Epoch: 15 [4000/60000 (7%)]\tLoss: 0.335032\n",
            "Train Epoch: 15 [8000/60000 (13%)]\tLoss: 0.392244\n",
            "Train Epoch: 15 [12000/60000 (20%)]\tLoss: 0.429180\n",
            "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 0.349706\n",
            "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.400316\n",
            "Train Epoch: 15 [24000/60000 (40%)]\tLoss: 0.440529\n",
            "Train Epoch: 15 [28000/60000 (47%)]\tLoss: 0.496798\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.401532\n",
            "Train Epoch: 15 [36000/60000 (60%)]\tLoss: 0.458915\n",
            "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.492027\n",
            "Train Epoch: 15 [44000/60000 (73%)]\tLoss: 0.431509\n",
            "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 0.424453\n",
            "Train Epoch: 15 [52000/60000 (87%)]\tLoss: 0.449540\n",
            "Train Epoch: 15 [56000/60000 (93%)]\tLoss: 0.400203\n",
            "\n",
            "Test set: Avg. loss: 0.1442, Accuracy: 9657/10000 (96.57%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 67.45%\n",
            "Quaternion sparsity: 56.34%\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.446515\n",
            "Train Epoch: 16 [4000/60000 (7%)]\tLoss: 0.489417\n",
            "Train Epoch: 16 [8000/60000 (13%)]\tLoss: 0.336326\n",
            "Train Epoch: 16 [12000/60000 (20%)]\tLoss: 0.383215\n",
            "Train Epoch: 16 [16000/60000 (27%)]\tLoss: 0.492739\n",
            "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.395997\n",
            "Train Epoch: 16 [24000/60000 (40%)]\tLoss: 0.426146\n",
            "Train Epoch: 16 [28000/60000 (47%)]\tLoss: 0.360594\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.351697\n",
            "Train Epoch: 16 [36000/60000 (60%)]\tLoss: 0.364093\n",
            "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.350046\n",
            "Train Epoch: 16 [44000/60000 (73%)]\tLoss: 0.413612\n",
            "Train Epoch: 16 [48000/60000 (80%)]\tLoss: 0.483534\n",
            "Train Epoch: 16 [52000/60000 (87%)]\tLoss: 0.477145\n",
            "Train Epoch: 16 [56000/60000 (93%)]\tLoss: 0.319966\n",
            "\n",
            "Test set: Avg. loss: 0.1435, Accuracy: 9625/10000 (96.25%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 70.17%\n",
            "Quaternion sparsity: 63.28%\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.344825\n",
            "Train Epoch: 17 [4000/60000 (7%)]\tLoss: 0.397616\n",
            "Train Epoch: 17 [8000/60000 (13%)]\tLoss: 0.475479\n",
            "Train Epoch: 17 [12000/60000 (20%)]\tLoss: 0.444084\n",
            "Train Epoch: 17 [16000/60000 (27%)]\tLoss: 0.373622\n",
            "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.512974\n",
            "Train Epoch: 17 [24000/60000 (40%)]\tLoss: 0.437064\n",
            "Train Epoch: 17 [28000/60000 (47%)]\tLoss: 0.415941\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.387158\n",
            "Train Epoch: 17 [36000/60000 (60%)]\tLoss: 0.421814\n",
            "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.420631\n",
            "Train Epoch: 17 [44000/60000 (73%)]\tLoss: 0.419758\n",
            "Train Epoch: 17 [48000/60000 (80%)]\tLoss: 0.448228\n",
            "Train Epoch: 17 [52000/60000 (87%)]\tLoss: 0.490352\n",
            "Train Epoch: 17 [56000/60000 (93%)]\tLoss: 0.334100\n",
            "\n",
            "Test set: Avg. loss: 0.1446, Accuracy: 9657/10000 (96.57%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 67.55%\n",
            "Quaternion sparsity: 54.97%\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.393868\n",
            "Train Epoch: 18 [4000/60000 (7%)]\tLoss: 0.331085\n",
            "Train Epoch: 18 [8000/60000 (13%)]\tLoss: 0.408119\n",
            "Train Epoch: 18 [12000/60000 (20%)]\tLoss: 0.425804\n",
            "Train Epoch: 18 [16000/60000 (27%)]\tLoss: 0.382099\n",
            "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.437667\n",
            "Train Epoch: 18 [24000/60000 (40%)]\tLoss: 0.447920\n",
            "Train Epoch: 18 [28000/60000 (47%)]\tLoss: 0.480936\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.423389\n",
            "Train Epoch: 18 [36000/60000 (60%)]\tLoss: 0.436592\n",
            "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.352985\n",
            "Train Epoch: 18 [44000/60000 (73%)]\tLoss: 0.449040\n",
            "Train Epoch: 18 [48000/60000 (80%)]\tLoss: 0.418244\n",
            "Train Epoch: 18 [52000/60000 (87%)]\tLoss: 0.515467\n",
            "Train Epoch: 18 [56000/60000 (93%)]\tLoss: 0.414534\n",
            "\n",
            "Test set: Avg. loss: 0.1540, Accuracy: 9591/10000 (95.91%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 69.99%\n",
            "Quaternion sparsity: 60.97%\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.465745\n",
            "Train Epoch: 19 [4000/60000 (7%)]\tLoss: 0.437240\n",
            "Train Epoch: 19 [8000/60000 (13%)]\tLoss: 0.378582\n",
            "Train Epoch: 19 [12000/60000 (20%)]\tLoss: 0.435352\n",
            "Train Epoch: 19 [16000/60000 (27%)]\tLoss: 0.387096\n",
            "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.455662\n",
            "Train Epoch: 19 [24000/60000 (40%)]\tLoss: 0.478072\n",
            "Train Epoch: 19 [28000/60000 (47%)]\tLoss: 0.466393\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.476110\n",
            "Train Epoch: 19 [36000/60000 (60%)]\tLoss: 0.430944\n",
            "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.377305\n",
            "Train Epoch: 19 [44000/60000 (73%)]\tLoss: 0.385762\n",
            "Train Epoch: 19 [48000/60000 (80%)]\tLoss: 0.359434\n",
            "Train Epoch: 19 [52000/60000 (87%)]\tLoss: 0.400309\n",
            "Train Epoch: 19 [56000/60000 (93%)]\tLoss: 0.334773\n",
            "\n",
            "Test set: Avg. loss: 0.1342, Accuracy: 9663/10000 (96.63%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 67.80%\n",
            "Quaternion sparsity: 57.28%\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.403882\n",
            "Train Epoch: 20 [4000/60000 (7%)]\tLoss: 0.403316\n",
            "Train Epoch: 20 [8000/60000 (13%)]\tLoss: 0.418790\n",
            "Train Epoch: 20 [12000/60000 (20%)]\tLoss: 0.371301\n",
            "Train Epoch: 20 [16000/60000 (27%)]\tLoss: 0.421160\n",
            "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 0.411611\n",
            "Train Epoch: 20 [24000/60000 (40%)]\tLoss: 0.463417\n",
            "Train Epoch: 20 [28000/60000 (47%)]\tLoss: 0.489177\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.433416\n",
            "Train Epoch: 20 [36000/60000 (60%)]\tLoss: 0.446168\n",
            "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 0.435982\n",
            "Train Epoch: 20 [44000/60000 (73%)]\tLoss: 0.388896\n",
            "Train Epoch: 20 [48000/60000 (80%)]\tLoss: 0.426355\n",
            "Train Epoch: 20 [52000/60000 (87%)]\tLoss: 0.404218\n",
            "Train Epoch: 20 [56000/60000 (93%)]\tLoss: 0.440749\n",
            "\n",
            "Test set: Avg. loss: 0.1419, Accuracy: 9652/10000 (96.52%)\n",
            "\n",
            "Checking sparsity...\n",
            "Weight sparsity: 69.64%\n",
            "Quaternion sparsity: 59.00%\n",
            "\n",
            "Elapsed time: 170.13 seconds\n",
            "\n",
            "Pruning the network...\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.1421, Accuracy: 9651/10000 (96.51%)\n",
            "\n",
            "Applying quaternion weight sharing [2 bits]...\n",
            "\n",
            "Test set: Avg. loss: 0.5618, Accuracy: 8711/10000 (87.11%)\n",
            "\n",
            "Fine tuning centroids...\n",
            "\n",
            "iteration [1/30]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ee241dbb77b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mcompress_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0msample_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ee241dbb77b2>\u001b[0m in \u001b[0;36mcompress_network\u001b[0;34m(use_quaternion_quantization, weight_sharing_bits, lr_tuning)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   \u001b[0mfine_tuning_centroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate_tuning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_tuning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'elapsed time: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0msave_codebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b7394dfd85eb>\u001b[0m in \u001b[0;36mfine_tuning_centroids\u001b[0;34m(n_iter, learning_rate_tuning)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodebook\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mcodebook\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0msum_of_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_of_gradients\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mcodebook\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'centroids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate_tuning\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum_of_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 117 is out of bounds for dimension 0 with size 100"
          ]
        }
      ]
    }
  ]
}